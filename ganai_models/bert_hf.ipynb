{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2d14eea",
   "metadata": {
    "id": "b2d14eea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/navdeep-singh/Documents/Data-Science/practice/algorithms-from-scratch/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertModel,\n",
    "    BertForMaskedLM,\n",
    "    BertForSequenceClassification,\n",
    "    AdamWeightDecay,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set random seeds for reproucibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca8f8543",
   "metadata": {
    "id": "ca8f8543"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TASK 1: SIMPLE BERT MODEL - Feature Extraction\n",
      "================================================================================\n",
      "\n",
      "STEP 1.1: Loading tokenizer...\n",
      "Vocabulary size: 30522\n",
      "Special tokens: ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TASK 1: SIMPLE BERT MODEL (Feature Extraction)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TASK 1: SIMPLE BERT MODEL - Feature Extraction\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model_checkpoint = \"bert-base-uncased\"\n",
    "\n",
    "# Step 1.1: Load pre-trained tokenizer\n",
    "# Tokenizer converts text to token IDs that bert understands\n",
    "print(\"\\nSTEP 1.1: Loading tokenizer...\")\n",
    "tokenizer: BertTokenizer = BertTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_checkpoint,\n",
    "    cache_dir=\"./bert_load_hf\"\n",
    ")\n",
    "\n",
    "# 'bert-base-uncased': 12-layer, 768-hidden, 12-heads, 110M parameters\n",
    "# 'uncased': all text converted to lowercase\n",
    "\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")  # 30522 tokens\n",
    "print(f\"Special tokens: {tokenizer.all_special_tokens}\")  # [CLS], [SEP], [PAD], [MASK], [UNK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd850597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 1.2: Preparing sample text...\n",
      "Sample texts: ['The quick brown fox jumps over the lazy dog.', 'BERT is a powerful language model developed by Google.', 'Natural language processing is fascinating!']\n",
      "\n",
      "STEP 1.3: Tokenizing text...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 1.2: Prepare sample text data\n",
    "print(\"\\nSTEP 1.2: Preparing sample text...\")\n",
    "sample_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"BERT is a powerful language model developed by Google.\",\n",
    "    \"Natural language processing is fascinating!\"\n",
    "]\n",
    "\n",
    "print(f\"Sample texts: {sample_texts}\")\n",
    "\n",
    "# STEP 1.3: Tokenize the text\n",
    "print(\"\\nSTEP 1.3: Tokenizing text...\")\n",
    "\n",
    "encoded_inputs = tokenizer(\n",
    "    text=sample_texts,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=32,\n",
    "    return_tensors=\"pt\",\n",
    "    return_attention_mask=True\n",
    ")\n",
    "dict(encoded_inputs).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7a583c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input IDs shape: torch.Size([3, 12])\n",
      "Attention mask shape: torch.Size([3, 12])\n",
      "\n",
      "First sequence tokens: ['[CLS]', 'the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', '[SEP]']\n",
      "First sequence IDs: tensor([  101,  1996,  4248,  2829,  4419, 14523,  2058,  1996, 13971,  3899,\n",
      "         1012,   102])\n",
      "First sequence attention mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Display tokenization results\n",
    "print(f\"\\nInput IDs shape: {encoded_inputs['input_ids'].shape}\")  # (3, 32)\n",
    "print(f\"Attention mask shape: {encoded_inputs['attention_mask'].shape}\")  # (3, 32)\n",
    "\n",
    "print(f\"\\nFirst sequence tokens: {tokenizer.convert_ids_to_tokens(encoded_inputs['input_ids'][0])}\")\n",
    "print(f\"First sequence IDs: {encoded_inputs['input_ids'][0]}\")\n",
    "print(f\"First sequence attention mask: {encoded_inputs['attention_mask'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c2be5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 1.4: Loading pre-trained BERT model...\n",
      "Model loaded successfully. Total parameters: 109,482,240\n"
     ]
    }
   ],
   "source": [
    "# STEP 1.4: Load pre-trained BERT model\n",
    "print(\"\\nSTEP 1.4: Loading pre-trained BERT model...\")\n",
    "model: BertModel = BertModel.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_checkpoint,\n",
    "    cache_dir=\"./bert_load_hf\"\n",
    ")\n",
    "# Put model in evaluation mode (disables dropout)\n",
    "model.eval()\n",
    "print(f\"Model loaded successfully. Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76685d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 1.5: Running forward pass...\n",
      "\n",
      "STEP 1.6: Extracting outputs...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 1.5: Forward pass through BERT\n",
    "print(\"\\nSTEP 1.5: Running forward pass...\")\n",
    "with torch.no_grad():  # Don't compute gradients (inference only)\n",
    "    outputs = model(\n",
    "        input_ids=encoded_inputs[\"input_ids\"],\n",
    "        attention_mask=encoded_inputs[\"attention_mask\"]\n",
    "    )\n",
    "    \n",
    "# STEP 1.6: Extract outputs\n",
    "print(\"\\nSTEP 1.6: Extracting outputs...\")\n",
    "dict(outputs).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fedee5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last hidden state shape: torch.Size([3, 12, 768])\n",
      "This contains contextual embeddings for every token in the sequence\n",
      "Pooler output shape: torch.Size([3, 768])\n",
      "This is the [CLS] token representation, useful for sentence-level tasks\n"
     ]
    }
   ],
   "source": [
    "# Output 1: Last hidden state - embeddings for ALL tokens\n",
    "last_hidden_state = outputs.last_hidden_state  # (batch_size=3, seq_len=32, hidden_size=768)\n",
    "print(f\"Last hidden state shape: {last_hidden_state.shape}\")\n",
    "print(f\"This contains contextual embeddings for every token in the sequence\")\n",
    "\n",
    "# Output 2: Pooler output - embedding for [CLS] token (sentence representation)\n",
    "pooler_output = outputs.pooler_output  # (batch_size=3, hidden_size=768)\n",
    "print(f\"Pooler output shape: {pooler_output.shape}\")\n",
    "print(f\"This is the [CLS] token representation, useful for sentence-level tasks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61981206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 1.7: Using embeddings...\n",
      "Embedding for token 5 in first sequence: shape torch.Size([768])\n",
      "Token 5 is: 'jumps'\n"
     ]
    }
   ],
   "source": [
    "# STEP 1.7: Use embeddings for various purposes\n",
    "print(\"\\nSTEP 1.7: Using embeddings...\")\n",
    "\n",
    "# Example 1: Get embedding for a specific token (e.g., token at position 5)\n",
    "token_5_embedding = last_hidden_state[0, 5, :] # (768,)\n",
    "print(f\"Embedding for token 5 in first sequence: shape {token_5_embedding.shape}\")\n",
    "print(f\"Token 5 is: '{tokenizer.convert_ids_to_tokens(encoded_inputs['input_ids'][0][5].item())}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5273336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CLS] token (manual extraction): torch.Size([3, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3608,  0.2271, -0.3030,  ..., -0.4224,  0.6949,  0.6213],\n",
       "        [-0.0789, -0.5309,  0.2158,  ...,  0.0842, -0.2137,  0.6038],\n",
       "        [-0.0030,  0.0996, -0.2846,  ..., -0.1934,  0.0864,  0.5263]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 2: Get [CLS] token manually (should match pooler_output)\n",
    "cls_token_manual = last_hidden_state[:, 0, :]  # (3, 768)\n",
    "print(f\"\\n[CLS] token (manual extraction): {cls_token_manual.shape}\")\n",
    "cls_token_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5f2e4ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cosine similarity between sentence 1 and 2: -0.3177\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Compute similarity between sentences using [CLS] embeddings\n",
    "sentence_similarity = F.cosine_similarity(pooler_output[0:1], pooler_output[1:2])\n",
    "print(f\"\\nCosine similarity between sentence 1 and 2: {sentence_similarity.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "db6293db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 768])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aad78edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean pooled output shape: torch.Size([3, 768])\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Mean pooling (alternative to [CLS] token)\n",
    "# Average all token embeddings (excluding padding)\n",
    "def mean_pooling(token_embeddings, attention_mask):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        token_embeddings: (batch_size, seq_len, hidden_size)\n",
    "        attention_mask: (batch_size, seq_len)\n",
    "    Returns:\n",
    "        mean_pooled: (batch_size, hidden_size)\n",
    "    \"\"\"\n",
    "    # Expand attention mask to match embeddings dimensions\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    # (3, 32, 1) -> (3, 32, 768)\n",
    "    \n",
    "    # Sum embeddings (only for non-padded tokens)\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, dim=1) # (3, 768)\n",
    "    \n",
    "    # Sum mask values to get count of non-padded tokens\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9) # (3, 768)\n",
    "    \n",
    "    # Divide to get mean\n",
    "    return sum_embeddings / sum_mask  # (3, 768)\n",
    "\n",
    "mean_pooled_output = mean_pooling(last_hidden_state, encoded_inputs['attention_mask'])\n",
    "print(f\"\\nMean pooled output shape: {mean_pooled_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "759e611a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_inputs[\"attention_mask\"].unsqueeze(-1).expand(last_hidden_state.size()).float()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd71a02c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
