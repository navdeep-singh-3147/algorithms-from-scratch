{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcbf97ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "@dataclass\n",
    "class LlamaConfig:\n",
    "    vocab_size: int = 1000\n",
    "    dim: int = 512           # Embedding dimension\n",
    "    n_layers: int = 4        # Number of transformer blocks\n",
    "    n_heads: int = 8         # Number of query heads\n",
    "    n_kv_heads: int = 4      # Number of key/value heads (Grouped Query Attention)\n",
    "    multiple_of: int = 32    # MLP hidden layer multiple\n",
    "    norm_eps: float = 1e-5\n",
    "    max_seq_len: int = 128   # Max context window\n",
    "    head_dim: int = dim // n_heads\n",
    "    \n",
    "    # RoPE Config\n",
    "    rope_theta: float = 10000.0\n",
    "\n",
    "config = LlamaConfig()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "548d9389",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.weight * self._norm(x.float()).type_as(x)\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        \"\"\"Make parameters visible in model summary\"\"\"\n",
    "        return f\"dim={self.weight.shape[0]}, eps={self.eps}\"\n",
    "\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    \"\"\"\n",
    "    Precompute the frequency tensor for complex exponentials (cis) with polar form.\n",
    "    \"\"\"\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device)\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    return freqs_cis\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)\n",
    "\n",
    "def apply_rotary_emb(xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor):\n",
    "    # Reshape xq and xk to match complex representation\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    \n",
    "    # Apply rotation\n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f0f2b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaAttention(nn.Module):\n",
    "    def __init__(self, cfg: LlamaConfig):\n",
    "        super().__init__()\n",
    "        self.n_heads = cfg.n_heads\n",
    "        self.n_kv_heads = cfg.n_kv_heads\n",
    "        self.head_dim = cfg.head_dim\n",
    "        \n",
    "        # Projections\n",
    "        self.wq = nn.Linear(cfg.dim, self.n_heads * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(cfg.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(cfg.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wo = nn.Linear(self.n_heads * self.head_dim, cfg.dim, bias=False)\n",
    "        \n",
    "        self.cache_k = None\n",
    "        self.cache_v = None\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor, \n",
    "        freqs_cis: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None,\n",
    "        use_cache: bool = False\n",
    "    ):\n",
    "        bsz, seqlen, _ = x.shape\n",
    "        print(f\"Attention forward: bsz={bsz}, seqlen={seqlen}\")\n",
    "        \n",
    "        # 1. QKV Projections\n",
    "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "        print(f\"QKV projections: xq.shape={xq.shape}, xk.shape={xk.shape}, xv.shape={xv.shape}\")\n",
    "\n",
    "        # 2. Reshape for heads\n",
    "        xq = xq.view(bsz, seqlen, self.n_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)\n",
    "        print(f\"After reshape: xq.shape={xq.shape}, xk.shape={xk.shape}, xv.shape={xv.shape}\")\n",
    "        \n",
    "        # 3. Apply RoPE (Only to Query and Key)\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis)\n",
    "        print(\"RoPE applied\")\n",
    "        \n",
    "        # 4. KV Cache Management\n",
    "        if use_cache:\n",
    "            if self.cache_k is None or self.cache_k.shape[1] == 0:\n",
    "                self.cache_k = xk\n",
    "                self.cache_v = xv\n",
    "                print(\"KV cache initialized\")\n",
    "            else:\n",
    "                self.cache_k = torch.cat((self.cache_k, xk), dim=1)\n",
    "                self.cache_v = torch.cat((self.cache_v, xv), dim=1)\n",
    "            keys, values = self.cache_k, self.cache_v\n",
    "            print(f\"KV cache updated: cache_k.shape={self.cache_k.shape}\")\n",
    "        else:\n",
    "            self.cache_k, self.cache_v = None, None\n",
    "            keys, values = xk, xv\n",
    "            print(\"No KV cache used\")\n",
    "\n",
    "        # 5. Grouped Query Attention (GQA) Logic\n",
    "        # We repeat keys/values to match n_heads\n",
    "        # keys shape: (B, Seq, n_kv_heads, D) -> (B, Seq, n_heads, D)\n",
    "        keys = torch.repeat_interleave(keys, repeats=self.n_heads // self.n_kv_heads, dim=2)\n",
    "        values = torch.repeat_interleave(values, repeats=self.n_heads // self.n_kv_heads, dim=2)\n",
    "        print(f\"After GQA repeat: keys.shape={keys.shape}, values.shape={values.shape}\")\n",
    "\n",
    "        # 6. Transpose for Attention: (B, H, Seq, D)\n",
    "        xq = xq.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        print(f\"After transpose: xq.shape={xq.shape}, keys.shape={keys.shape}\")\n",
    "\n",
    "        # 7. Attention Calculation\n",
    "        # scores: (B, H, Seq_Q, Seq_K)\n",
    "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        print(f\"Scores computed: scores.shape={scores.shape}\")\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores + mask\n",
    "            print(\"Mask applied\")\n",
    "        \n",
    "        probs = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "        print(f\"Probs shape: {probs.shape}\")\n",
    "        \n",
    "        # output: (B, H, Seq_Q, D)\n",
    "        output = torch.matmul(probs, values)\n",
    "        print(f\"Attention output shape: {output.shape}\")\n",
    "\n",
    "        # 8. Restore shape\n",
    "        # Flatten heads: (B, Seq, H * D)\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)        \n",
    "        return self.wo(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9823779",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, cfg: LlamaConfig):\n",
    "        super().__init__()\n",
    "        hidden_dim = 4 * cfg.dim\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        hidden_dim = cfg.multiple_of * ((hidden_dim + cfg.multiple_of - 1) // cfg.multiple_of)\n",
    "\n",
    "        self.w1 = nn.Linear(cfg.dim, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, cfg.dim, bias=False)\n",
    "        self.w3 = nn.Linear(cfg.dim, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # SwiGLU: w2(F.silu(w1(x)) * w3(x))\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
    "\n",
    "class LlamaBlock(nn.Module):\n",
    "    def __init__(self, cfg: LlamaConfig):\n",
    "        super().__init__()\n",
    "        self.attention = LlamaAttention(cfg)\n",
    "        self.feed_forward = LlamaMLP(cfg)\n",
    "        self.attention_norm = RMSNorm(cfg.dim, eps=cfg.norm_eps)\n",
    "        self.ffn_norm = RMSNorm(cfg.dim, eps=cfg.norm_eps)\n",
    "\n",
    "    def forward(self, x, freqs_cis, mask, use_cache):\n",
    "        h = x + self.attention(self.attention_norm(x), freqs_cis, mask, use_cache)\n",
    "        out = h + self.feed_forward(self.ffn_norm(h))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0218ce45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llama2(nn.Module):\n",
    "    def __init__(self, cfg: LlamaConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.tok_embeddings = nn.Embedding(cfg.vocab_size, cfg.dim)\n",
    "        self.layers = nn.ModuleList([LlamaBlock(cfg) for _ in range(cfg.n_layers)])\n",
    "        self.norm = RMSNorm(cfg.dim, eps=cfg.norm_eps)\n",
    "        self.output = nn.Linear(cfg.dim, cfg.vocab_size, bias=False)\n",
    "        \n",
    "        # Precompute RoPE frequencies\n",
    "        self.freqs_cis = precompute_freqs_cis(\n",
    "            cfg.dim // cfg.n_heads, cfg.max_seq_len * 2, cfg.rope_theta\n",
    "        ).to(device)\n",
    "        print(f\"Llama2 initialized: vocab_size={cfg.vocab_size}, dim={cfg.dim}, n_layers={cfg.n_layers}\")\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        tokens: torch.Tensor, \n",
    "        start_pos: int = 0, \n",
    "        mask: Optional[torch.Tensor] = None,\n",
    "        use_cache: bool = False\n",
    "    ):\n",
    "        bsz, seqlen = tokens.shape\n",
    "        print(f\"Forward pass start: batch_size={bsz}, seq_len={seqlen}, start_pos={start_pos}, use_cache={use_cache}\")\n",
    "        h = self.tok_embeddings(tokens)\n",
    "        print(f\"After embeddings: h.shape={h.shape}\")\n",
    "                \n",
    "        # Fetch appropriate RoPE frequencies\n",
    "        # During inference (use_cache=True), we only need freqs for the current position\n",
    "        # During training (use_cache=False), we need freqs for 0..seqlen\n",
    "        self.freqs_cis = self.freqs_cis.to(h.device)\n",
    "        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]\n",
    "        print(f\"RoPE freqs_cis shape: {freqs_cis.shape}\")\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print(f\"Layer {i} forward start\")\n",
    "            h = layer(h, freqs_cis, mask, use_cache)\n",
    "            print(f\"Layer {i} output shape: {h.shape}\")\n",
    "        h = self.norm(h)\n",
    "        print(f\"After norm: h.shape={h.shape}\")\n",
    "        logits = self.output(h)\n",
    "        print(f\"Logits shape: {logits.shape}\")\n",
    "        return logits\n",
    "\n",
    "    def clear_kv_cache(self):\n",
    "        for layer in self.layers:\n",
    "            layer.attention.cache_k = None\n",
    "            layer.attention.cache_v = None\n",
    "        print(\"KV cache cleared\")\n",
    "        \n",
    "    # Add this method to the Llama2 class\n",
    "\n",
    "    def get_kv_cache_stats(self):\n",
    "        \"\"\"Print statistics about KV cache in all layers\"\"\"\n",
    "        print(\"\\n--- KV Cache Statistics ---\")\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            cache_k = layer.attention.cache_k\n",
    "            cache_v = layer.attention.cache_v\n",
    "            \n",
    "            if cache_k is not None:\n",
    "                print(f\"Layer {i}:\")\n",
    "                print(f\"  cache_k shape: {cache_k.shape}\")\n",
    "                print(f\"  cache_v shape: {cache_v.shape}\")\n",
    "                print(f\"  cache_k memory: {cache_k.element_size() * cache_k.nelement() / 1024:.2f} KB\")\n",
    "            else:\n",
    "                print(f\"Layer {i}: No cache\")\n",
    "        print(\"--- End KV Cache Stats ---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59a77890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: Simple BPE-like Tokenizer (Character-level + Word-level)\n",
    "# ============================================================================\n",
    "\n",
    "class SimpleTokenizer:\n",
    "    \"\"\"\n",
    "    Simple tokenizer for training - splits on whitespace + punctuation,\n",
    "    builds vocabulary from training corpus.\n",
    "    \"\"\"\n",
    "    def __init__(self, texts: List[str | int], min_freq: int = 0):        \n",
    "        self.unk_token = \"<UNK>\"\n",
    "        self.pad_token = \"<PAD>\"\n",
    "        self.bos_token = \"<BOS>\" # Beginning of Sequence\n",
    "        self.eos_token = \"<EOS>\" # End of Sequence\n",
    "        \n",
    "        # Pre-assign indicers for special tokens\n",
    "        self.specials = [self.unk_token, self.pad_token, self.bos_token, self.eos_token]\n",
    "        \n",
    "        # Add special tokens to vocab\n",
    "        self.vocab = {token: i for i, token in enumerate(self.specials)}\n",
    "        self.inverse_vocab = {i: token for i, token in enumerate(self.specials)}\n",
    "        if texts is not None:\n",
    "            self._build_vocab(texts=texts, min_freq=min_freq)\n",
    "        \n",
    "    @property\n",
    "    def pad_id(self):\n",
    "        return self.vocab[self.pad_token]\n",
    "    \n",
    "    @property\n",
    "    def bos_id(self):\n",
    "        return self.vocab[self.bos_token]\n",
    "    \n",
    "    @property\n",
    "    def eos_id(self):\n",
    "        return self.vocab[self.eos_token]\n",
    "    \n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Splits text into words and punctuation. \n",
    "        Does NOT remove punctuation.\n",
    "        \"\"\"\n",
    "        tokens = re.findall(r\"\\w+|[^\\w\\s]\", text.lower(), re.UNICODE)\n",
    "        return tokens\n",
    "\n",
    "    def _build_vocab(self, texts: List[str], min_freq: int = 0):\n",
    "        print(\"Building vocabulary...\")\n",
    "        all_tokens = []\n",
    "        for text in texts:\n",
    "            # Filter out punctuation when building vocab\n",
    "            tokens = self._tokenize(text=text)\n",
    "            all_tokens.extend(tokens)\n",
    "        \n",
    "        counter = Counter(all_tokens)\n",
    "\n",
    "        # Add non-special tokens satisfying min_freq\n",
    "        idx = len(self.vocab)\n",
    "        for token, freq in counter.items():\n",
    "            if freq >= min_freq:\n",
    "                self.vocab[token] = idx\n",
    "                self.inverse_vocab[idx] = token\n",
    "                idx += 1\n",
    "        print(f\"Vocab size: {len(self.vocab)}\")\n",
    "    \n",
    "    def encode_raw(self, text: str) -> List[int]:\n",
    "        \"\"\"\n",
    "        Returns raw list of IDs with BOS/EOS but NO padding/truncation.\n",
    "        Used for packing.\n",
    "        \"\"\"\n",
    "        tokens = self._tokenize(text=text)\n",
    "        ids = [self.vocab.get(token, self.vocab[self.unk_token]) for token in tokens]\n",
    "        return [self.bos_id] + ids + [self.eos_id]\n",
    "    \n",
    "    def encode_inference(self, text: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encodes text for INFERENCE (No padding, BOS only).\n",
    "        Expects batch_size=1 logic usually.\n",
    "        \"\"\"\n",
    "        tokens = self._tokenize(text=text)\n",
    "        ids = [self.vocab.get(token, self.vocab[self.unk_token]) for token in tokens]\n",
    "        \n",
    "        # Inference Prompt: Add BOS, do NOT add EOS (model must generate it), do NOT Pad.\n",
    "        ids = [self.bos_id] + ids\n",
    "        return torch.tensor(ids, dtype=torch.long)        \n",
    "    \n",
    "    def decode(self, ids: list) -> str:\n",
    "        \"\"\"\n",
    "        Convert token IDs to text\n",
    "        \"\"\"\n",
    "        if isinstance(ids, torch.Tensor):\n",
    "            ids = ids.tolist()\n",
    "        tokens = [self.inverse_vocab.get(id, self.unk_token) for id in ids]\n",
    "        \n",
    "        # Specialized decoding: filter out special tokens\n",
    "        filtered = [token for token in tokens if token not in self.specials]\n",
    "        \n",
    "        # Simple heuristic to join punctuation nicely\n",
    "        # (For a real BPE tokenizer, this is handled by the subword merge logic)\n",
    "        out_str = \" \".join(filtered)\n",
    "        # Cleanup spaces before punctuation (simple hack for readability)\n",
    "        out_str = re.sub(r'\\s+([?.!,:;])', r'\\1', out_str)       \n",
    "        return out_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fec6b62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PackedDataset(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        tokenizer: SimpleTokenizer,\n",
    "        max_seq_len: int = 128\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # Pack sequences once during initialization\n",
    "        self.input_ids, self.position_ids, self.seq_ids = self.pack_all_sequences()\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Return number of packed sequences\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Direct lookup from pre-packed tensors\n",
    "        input_ids = self.input_ids[index]\n",
    "        position_ids = self.position_ids[index]\n",
    "        seq_ids = self.seq_ids[index]\n",
    "        \n",
    "        # Labels: shift input_ids by 1 for next-token prediction\n",
    "        # The last token's label is pad_id (will be ignored in loss)\n",
    "        labels = torch.cat([input_ids[1:], torch.tensor([self.tokenizer.pad_id])])\n",
    "        \n",
    "        # Generate combined mask: causal + sequence packing\n",
    "        # Causal: prevent attending to future tokens\n",
    "        # Sequence: prevent attending across different sequences\n",
    "        combined_mask = self._generate_combined_mask(seq_ids)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"position_ids\": position_ids,\n",
    "            \"seq_ids\": seq_ids,\n",
    "            \"labels\": labels,\n",
    "            \"combined_mask\": combined_mask\n",
    "        }\n",
    "    \n",
    "    def pack_all_sequences(self):\n",
    "        sequences = self.encode_text(texts=self.texts)\n",
    "        stream_input_ids = []\n",
    "        stream_position_ids = []\n",
    "        stream_seq_ids = []\n",
    "        \n",
    "        current_global_seq_id = 0\n",
    "        \n",
    "        # ...existing code...\n",
    "        for seq in sequences:\n",
    "            full_seq = seq + [self.tokenizer.eos_id]\n",
    "            stream_input_ids.extend(full_seq)\n",
    "            \n",
    "            pos_ids = list(range(len(full_seq)))\n",
    "            stream_position_ids.extend(pos_ids)\n",
    "            \n",
    "            stream_seq_ids.extend([current_global_seq_id] * len(full_seq))\n",
    "            current_global_seq_id += 1    \n",
    "        \n",
    "        # ...existing code...\n",
    "        packed_input_batches = []\n",
    "        packed_pos_batches = []\n",
    "        packed_seq_batches = []\n",
    "\n",
    "        total_tokens = len(stream_input_ids)\n",
    "\n",
    "        for i in range(0, total_tokens, self.max_seq_len):\n",
    "            end_idx = i + self.max_seq_len\n",
    "            chunk_input = stream_input_ids[i:end_idx]\n",
    "            chunk_pos = stream_position_ids[i:end_idx]\n",
    "            chunk_seq = stream_seq_ids[i:end_idx]\n",
    "            \n",
    "            if len(chunk_input) < self.max_seq_len:\n",
    "                pad_len = self.max_seq_len - len(chunk_input)\n",
    "                chunk_input.extend([self.tokenizer.pad_id] * pad_len)\n",
    "                chunk_pos.extend([self.tokenizer.pad_id] * pad_len)\n",
    "                chunk_seq.extend([self.tokenizer.pad_id] * pad_len)\n",
    "                \n",
    "            packed_input_batches.append(chunk_input)\n",
    "            packed_pos_batches.append(chunk_pos)\n",
    "            packed_seq_batches.append(chunk_seq)\n",
    "        \n",
    "        input_ids = torch.tensor(packed_input_batches, dtype=torch.long)\n",
    "        position_ids = torch.tensor(packed_pos_batches, dtype=torch.long)\n",
    "        seq_ids = torch.tensor(packed_seq_batches, dtype=torch.long)\n",
    "        \n",
    "        return input_ids, position_ids, seq_ids\n",
    "\n",
    "    def encode_text(self, texts):\n",
    "        all_token_ids = []\n",
    "        for text in texts:\n",
    "            ids = self.tokenizer.encode_raw(text=text)\n",
    "            all_token_ids.append(ids)\n",
    "        return all_token_ids\n",
    "    \n",
    "    def _generate_combined_mask(self, seq_ids: torch.Tensor, dtype=torch.float32):\n",
    "        seq_len = seq_ids.shape[0]\n",
    "        seq_ids_row = seq_ids.unsqueeze(-1)\n",
    "        seq_ids_col = seq_ids.unsqueeze(-2)\n",
    "        \n",
    "        same_seq_mask = (seq_ids_row == seq_ids_col) & (seq_ids_col != self.tokenizer.pad_id)\n",
    "        causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=seq_ids.device)).bool()\n",
    "        \n",
    "        combined_mask = same_seq_mask & causal_mask\n",
    "        \n",
    "        final_mask = torch.zeros((seq_len, seq_len), dtype=dtype, device=seq_ids.device)\n",
    "        final_mask = final_mask.masked_fill(~combined_mask, value=-1e9)\n",
    "        return final_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57611299",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_texts = [\n",
    "        # Shakespeare-style\n",
    "        \"\"\"To be, or not to be: that is the question. Whether 'tis nobler in the mind to suffer\n",
    "        The slings and arrows of outrageous fortune, Or to take arms against a sea of troubles,\n",
    "        And by opposing end them? To die: to sleep; No more;\"\"\",\n",
    "        \n",
    "        # Technical / Math\n",
    "        \"\"\"Machine learning models like transformers have revolutionized natural language processing.\n",
    "        The attention mechanism computes a weighted sum of value vectors:\n",
    "        Attention(Q, K, V) = softmax(QK^T / sqrt(d_k))V.\"\"\",\n",
    "        \n",
    "        \"\"\"Self-supervised learning on large corpora enables emergent capabilities.\"\"\",\n",
    "        \n",
    "        \"\"\"This is an advanced optimization technique. Training Large Language Models (LLMs)\n",
    "        like Llama 2 is expensive; Sequence Packing (also known as Sample Packing or Multipacking)\n",
    "        is crucial because it eliminates the wasted computation caused by padding tokens.\"\"\",\n",
    "        \n",
    "        # Code\n",
    "        \"\"\"def train_step(model, batch, optimizer):\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                loss = model(batch)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            return loss.item()\"\"\",\n",
    "            \n",
    "        # General Prose\n",
    "        \"\"\"The quick brown fox jumps over the lazy dog. \n",
    "        Artificial intelligence is rapidly evolving, impacting sectors from healthcare to finance.\n",
    "        Robust data pipelines are essential for stable model training.\"\"\",\n",
    "        \n",
    "        \"\"\"I can outline an end-to-end approach and provide a cohesive code scaffold for training \n",
    "        Llama2-like KV-cache aware PyTorch model with optional KV cache during inference and sequence\n",
    "        packing during training.\"\"\"\n",
    "    ]\n",
    "\n",
    "# Simple 90/10 split\n",
    "split_idx = int(len(raw_texts) * 0.8)\n",
    "train_texts = raw_texts[:split_idx]\n",
    "val_texts = raw_texts[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f5a5d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocabulary...\n",
      "Vocab size: 127\n",
      "Packed Batches - Train: 3 | Val: 1\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizer(texts=train_texts)\n",
    "max_seq_len = 15\n",
    "batch_size =6\n",
    "\n",
    "train_dataset = PackedDataset(texts=train_texts, tokenizer=tokenizer, max_seq_len=max_seq_len)\n",
    "val_dataset = PackedDataset(texts=val_texts, tokenizer=tokenizer, max_seq_len=max_seq_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Packed Batches - Train: {len(train_loader)} | Val: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08c0659f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama2 initialized: vocab_size=127, dim=256, n_layers=4\n"
     ]
    }
   ],
   "source": [
    "config = LlamaConfig(\n",
    "    vocab_size=len(tokenizer.vocab),\n",
    "    dim=256,\n",
    "    n_layers=4,\n",
    "    n_heads=4,\n",
    "    n_kv_heads=2,\n",
    "    multiple_of=2,\n",
    "    max_seq_len=max_seq_len,\n",
    ")\n",
    "\n",
    "model = Llama2(cfg=config).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77b92d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama2(\n",
      "  (tok_embeddings): Embedding(127, 256)\n",
      "  (layers): ModuleList(\n",
      "    (0-3): 4 x LlamaBlock(\n",
      "      (attention): LlamaAttention(\n",
      "        (wq): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (wk): Linear(in_features=256, out_features=128, bias=False)\n",
      "        (wv): Linear(in_features=256, out_features=128, bias=False)\n",
      "        (wo): Linear(in_features=256, out_features=256, bias=False)\n",
      "      )\n",
      "      (feed_forward): LlamaMLP(\n",
      "        (w1): Linear(in_features=256, out_features=682, bias=False)\n",
      "        (w2): Linear(in_features=682, out_features=256, bias=False)\n",
      "        (w3): Linear(in_features=256, out_features=682, bias=False)\n",
      "      )\n",
      "      (attention_norm): RMSNorm(dim=256, eps=1e-05)\n",
      "      (ffn_norm): RMSNorm(dim=256, eps=1e-05)\n",
      "    )\n",
      "  )\n",
      "  (norm): RMSNorm(dim=256, eps=1e-05)\n",
      "  (output): Linear(in_features=256, out_features=127, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec68585e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training (with Packed Sequences) ---\n",
      "Epoch 0 start\n",
      "Batch 0: input_ids.shape=torch.Size([6, 15]), labels.shape=torch.Size([6, 15]), mask.shape=torch.Size([6, 1, 15, 15])\n",
      "Forward pass start: batch_size=6, seq_len=15, start_pos=0, use_cache=False\n",
      "After embeddings: h.shape=torch.Size([6, 15, 256])\n",
      "RoPE freqs_cis shape: torch.Size([15, 32])\n",
      "Layer 0 forward start\n",
      "Attention forward: bsz=6, seqlen=15\n",
      "QKV projections: xq.shape=torch.Size([6, 15, 256]), xk.shape=torch.Size([6, 15, 128]), xv.shape=torch.Size([6, 15, 128])\n",
      "After reshape: xq.shape=torch.Size([6, 15, 4, 64]), xk.shape=torch.Size([6, 15, 2, 64]), xv.shape=torch.Size([6, 15, 2, 64])\n",
      "RoPE applied\n",
      "No KV cache used\n",
      "After GQA repeat: keys.shape=torch.Size([6, 15, 4, 64]), values.shape=torch.Size([6, 15, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([6, 4, 15, 64]), keys.shape=torch.Size([6, 4, 15, 64])\n",
      "Scores computed: scores.shape=torch.Size([6, 4, 15, 15])\n",
      "Mask applied\n",
      "Probs shape: torch.Size([6, 4, 15, 15])\n",
      "Attention output shape: torch.Size([6, 4, 15, 64])\n",
      "Layer 0 output shape: torch.Size([6, 15, 256])\n",
      "Layer 1 forward start\n",
      "Attention forward: bsz=6, seqlen=15\n",
      "QKV projections: xq.shape=torch.Size([6, 15, 256]), xk.shape=torch.Size([6, 15, 128]), xv.shape=torch.Size([6, 15, 128])\n",
      "After reshape: xq.shape=torch.Size([6, 15, 4, 64]), xk.shape=torch.Size([6, 15, 2, 64]), xv.shape=torch.Size([6, 15, 2, 64])\n",
      "RoPE applied\n",
      "No KV cache used\n",
      "After GQA repeat: keys.shape=torch.Size([6, 15, 4, 64]), values.shape=torch.Size([6, 15, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([6, 4, 15, 64]), keys.shape=torch.Size([6, 4, 15, 64])\n",
      "Scores computed: scores.shape=torch.Size([6, 4, 15, 15])\n",
      "Mask applied\n",
      "Probs shape: torch.Size([6, 4, 15, 15])\n",
      "Attention output shape: torch.Size([6, 4, 15, 64])\n",
      "Layer 1 output shape: torch.Size([6, 15, 256])\n",
      "Layer 2 forward start\n",
      "Attention forward: bsz=6, seqlen=15\n",
      "QKV projections: xq.shape=torch.Size([6, 15, 256]), xk.shape=torch.Size([6, 15, 128]), xv.shape=torch.Size([6, 15, 128])\n",
      "After reshape: xq.shape=torch.Size([6, 15, 4, 64]), xk.shape=torch.Size([6, 15, 2, 64]), xv.shape=torch.Size([6, 15, 2, 64])\n",
      "RoPE applied\n",
      "No KV cache used\n",
      "After GQA repeat: keys.shape=torch.Size([6, 15, 4, 64]), values.shape=torch.Size([6, 15, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([6, 4, 15, 64]), keys.shape=torch.Size([6, 4, 15, 64])\n",
      "Scores computed: scores.shape=torch.Size([6, 4, 15, 15])\n",
      "Mask applied\n",
      "Probs shape: torch.Size([6, 4, 15, 15])\n",
      "Attention output shape: torch.Size([6, 4, 15, 64])\n",
      "Layer 2 output shape: torch.Size([6, 15, 256])\n",
      "Layer 3 forward start\n",
      "Attention forward: bsz=6, seqlen=15\n",
      "QKV projections: xq.shape=torch.Size([6, 15, 256]), xk.shape=torch.Size([6, 15, 128]), xv.shape=torch.Size([6, 15, 128])\n",
      "After reshape: xq.shape=torch.Size([6, 15, 4, 64]), xk.shape=torch.Size([6, 15, 2, 64]), xv.shape=torch.Size([6, 15, 2, 64])\n",
      "RoPE applied\n",
      "No KV cache used\n",
      "After GQA repeat: keys.shape=torch.Size([6, 15, 4, 64]), values.shape=torch.Size([6, 15, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([6, 4, 15, 64]), keys.shape=torch.Size([6, 4, 15, 64])\n",
      "Scores computed: scores.shape=torch.Size([6, 4, 15, 15])\n",
      "Mask applied\n",
      "Probs shape: torch.Size([6, 4, 15, 15])\n",
      "Attention output shape: torch.Size([6, 4, 15, 64])\n",
      "Layer 3 output shape: torch.Size([6, 15, 256])\n",
      "After norm: h.shape=torch.Size([6, 15, 256])\n",
      "Logits shape: torch.Size([6, 15, 127])\n",
      "Logits from model: shape=torch.Size([6, 15, 127])\n",
      "Loss computed: 5.0618\n",
      "Epoch 0 | Batch 0 | Loss: 5.0618\n",
      "Batch 1: input_ids.shape=torch.Size([6, 15]), labels.shape=torch.Size([6, 15]), mask.shape=torch.Size([6, 1, 15, 15])\n",
      "Forward pass start: batch_size=6, seq_len=15, start_pos=0, use_cache=False\n",
      "After embeddings: h.shape=torch.Size([6, 15, 256])\n",
      "RoPE freqs_cis shape: torch.Size([15, 32])\n",
      "Layer 0 forward start\n",
      "Attention forward: bsz=6, seqlen=15\n",
      "QKV projections: xq.shape=torch.Size([6, 15, 256]), xk.shape=torch.Size([6, 15, 128]), xv.shape=torch.Size([6, 15, 128])\n",
      "After reshape: xq.shape=torch.Size([6, 15, 4, 64]), xk.shape=torch.Size([6, 15, 2, 64]), xv.shape=torch.Size([6, 15, 2, 64])\n",
      "RoPE applied\n",
      "No KV cache used\n",
      "After GQA repeat: keys.shape=torch.Size([6, 15, 4, 64]), values.shape=torch.Size([6, 15, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([6, 4, 15, 64]), keys.shape=torch.Size([6, 4, 15, 64])\n",
      "Scores computed: scores.shape=torch.Size([6, 4, 15, 15])\n",
      "Mask applied\n",
      "Probs shape: torch.Size([6, 4, 15, 15])\n",
      "Attention output shape: torch.Size([6, 4, 15, 64])\n",
      "Layer 0 output shape: torch.Size([6, 15, 256])\n",
      "Layer 1 forward start\n",
      "Attention forward: bsz=6, seqlen=15\n",
      "QKV projections: xq.shape=torch.Size([6, 15, 256]), xk.shape=torch.Size([6, 15, 128]), xv.shape=torch.Size([6, 15, 128])\n",
      "After reshape: xq.shape=torch.Size([6, 15, 4, 64]), xk.shape=torch.Size([6, 15, 2, 64]), xv.shape=torch.Size([6, 15, 2, 64])\n",
      "RoPE applied\n",
      "No KV cache used\n",
      "After GQA repeat: keys.shape=torch.Size([6, 15, 4, 64]), values.shape=torch.Size([6, 15, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([6, 4, 15, 64]), keys.shape=torch.Size([6, 4, 15, 64])\n",
      "Scores computed: scores.shape=torch.Size([6, 4, 15, 15])\n",
      "Mask applied\n",
      "Probs shape: torch.Size([6, 4, 15, 15])\n",
      "Attention output shape: torch.Size([6, 4, 15, 64])\n",
      "Layer 1 output shape: torch.Size([6, 15, 256])\n",
      "Layer 2 forward start\n",
      "Attention forward: bsz=6, seqlen=15\n",
      "QKV projections: xq.shape=torch.Size([6, 15, 256]), xk.shape=torch.Size([6, 15, 128]), xv.shape=torch.Size([6, 15, 128])\n",
      "After reshape: xq.shape=torch.Size([6, 15, 4, 64]), xk.shape=torch.Size([6, 15, 2, 64]), xv.shape=torch.Size([6, 15, 2, 64])\n",
      "RoPE applied\n",
      "No KV cache used\n",
      "After GQA repeat: keys.shape=torch.Size([6, 15, 4, 64]), values.shape=torch.Size([6, 15, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([6, 4, 15, 64]), keys.shape=torch.Size([6, 4, 15, 64])\n",
      "Scores computed: scores.shape=torch.Size([6, 4, 15, 15])\n",
      "Mask applied\n",
      "Probs shape: torch.Size([6, 4, 15, 15])\n",
      "Attention output shape: torch.Size([6, 4, 15, 64])\n",
      "Layer 2 output shape: torch.Size([6, 15, 256])\n",
      "Layer 3 forward start\n",
      "Attention forward: bsz=6, seqlen=15\n",
      "QKV projections: xq.shape=torch.Size([6, 15, 256]), xk.shape=torch.Size([6, 15, 128]), xv.shape=torch.Size([6, 15, 128])\n",
      "After reshape: xq.shape=torch.Size([6, 15, 4, 64]), xk.shape=torch.Size([6, 15, 2, 64]), xv.shape=torch.Size([6, 15, 2, 64])\n",
      "RoPE applied\n",
      "No KV cache used\n",
      "After GQA repeat: keys.shape=torch.Size([6, 15, 4, 64]), values.shape=torch.Size([6, 15, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([6, 4, 15, 64]), keys.shape=torch.Size([6, 4, 15, 64])\n",
      "Scores computed: scores.shape=torch.Size([6, 4, 15, 15])\n",
      "Mask applied\n",
      "Probs shape: torch.Size([6, 4, 15, 15])\n",
      "Attention output shape: torch.Size([6, 4, 15, 64])\n",
      "Layer 3 output shape: torch.Size([6, 15, 256])\n",
      "After norm: h.shape=torch.Size([6, 15, 256])\n",
      "Logits shape: torch.Size([6, 15, 127])\n",
      "Logits from model: shape=torch.Size([6, 15, 127])\n",
      "Loss computed: 4.9028\n",
      "Batch 2: input_ids.shape=torch.Size([3, 15]), labels.shape=torch.Size([3, 15]), mask.shape=torch.Size([3, 1, 15, 15])\n",
      "Forward pass start: batch_size=3, seq_len=15, start_pos=0, use_cache=False\n",
      "After embeddings: h.shape=torch.Size([3, 15, 256])\n",
      "RoPE freqs_cis shape: torch.Size([15, 32])\n",
      "Layer 0 forward start\n",
      "Attention forward: bsz=3, seqlen=15\n",
      "QKV projections: xq.shape=torch.Size([3, 15, 256]), xk.shape=torch.Size([3, 15, 128]), xv.shape=torch.Size([3, 15, 128])\n",
      "After reshape: xq.shape=torch.Size([3, 15, 4, 64]), xk.shape=torch.Size([3, 15, 2, 64]), xv.shape=torch.Size([3, 15, 2, 64])\n",
      "RoPE applied\n",
      "No KV cache used\n",
      "After GQA repeat: keys.shape=torch.Size([3, 15, 4, 64]), values.shape=torch.Size([3, 15, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([3, 4, 15, 64]), keys.shape=torch.Size([3, 4, 15, 64])\n",
      "Scores computed: scores.shape=torch.Size([3, 4, 15, 15])\n",
      "Mask applied\n",
      "Probs shape: torch.Size([3, 4, 15, 15])\n",
      "Attention output shape: torch.Size([3, 4, 15, 64])\n",
      "Layer 0 output shape: torch.Size([3, 15, 256])\n",
      "Layer 1 forward start\n",
      "Attention forward: bsz=3, seqlen=15\n",
      "QKV projections: xq.shape=torch.Size([3, 15, 256]), xk.shape=torch.Size([3, 15, 128]), xv.shape=torch.Size([3, 15, 128])\n",
      "After reshape: xq.shape=torch.Size([3, 15, 4, 64]), xk.shape=torch.Size([3, 15, 2, 64]), xv.shape=torch.Size([3, 15, 2, 64])\n",
      "RoPE applied\n",
      "No KV cache used\n",
      "After GQA repeat: keys.shape=torch.Size([3, 15, 4, 64]), values.shape=torch.Size([3, 15, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([3, 4, 15, 64]), keys.shape=torch.Size([3, 4, 15, 64])\n",
      "Scores computed: scores.shape=torch.Size([3, 4, 15, 15])\n",
      "Mask applied\n",
      "Probs shape: torch.Size([3, 4, 15, 15])\n",
      "Attention output shape: torch.Size([3, 4, 15, 64])\n",
      "Layer 1 output shape: torch.Size([3, 15, 256])\n",
      "Layer 2 forward start\n",
      "Attention forward: bsz=3, seqlen=15\n",
      "QKV projections: xq.shape=torch.Size([3, 15, 256]), xk.shape=torch.Size([3, 15, 128]), xv.shape=torch.Size([3, 15, 128])\n",
      "After reshape: xq.shape=torch.Size([3, 15, 4, 64]), xk.shape=torch.Size([3, 15, 2, 64]), xv.shape=torch.Size([3, 15, 2, 64])\n",
      "RoPE applied\n",
      "No KV cache used\n",
      "After GQA repeat: keys.shape=torch.Size([3, 15, 4, 64]), values.shape=torch.Size([3, 15, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([3, 4, 15, 64]), keys.shape=torch.Size([3, 4, 15, 64])\n",
      "Scores computed: scores.shape=torch.Size([3, 4, 15, 15])\n",
      "Mask applied\n",
      "Probs shape: torch.Size([3, 4, 15, 15])\n",
      "Attention output shape: torch.Size([3, 4, 15, 64])\n",
      "Layer 2 output shape: torch.Size([3, 15, 256])\n",
      "Layer 3 forward start\n",
      "Attention forward: bsz=3, seqlen=15\n",
      "QKV projections: xq.shape=torch.Size([3, 15, 256]), xk.shape=torch.Size([3, 15, 128]), xv.shape=torch.Size([3, 15, 128])\n",
      "After reshape: xq.shape=torch.Size([3, 15, 4, 64]), xk.shape=torch.Size([3, 15, 2, 64]), xv.shape=torch.Size([3, 15, 2, 64])\n",
      "RoPE applied\n",
      "No KV cache used\n",
      "After GQA repeat: keys.shape=torch.Size([3, 15, 4, 64]), values.shape=torch.Size([3, 15, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([3, 4, 15, 64]), keys.shape=torch.Size([3, 4, 15, 64])\n",
      "Scores computed: scores.shape=torch.Size([3, 4, 15, 15])\n",
      "Mask applied\n",
      "Probs shape: torch.Size([3, 4, 15, 15])\n",
      "Attention output shape: torch.Size([3, 4, 15, 64])\n",
      "Layer 3 output shape: torch.Size([3, 15, 256])\n",
      "After norm: h.shape=torch.Size([3, 15, 256])\n",
      "Logits shape: torch.Size([3, 15, 127])\n",
      "Logits from model: shape=torch.Size([3, 15, 127])\n",
      "Loss computed: 4.9815\n",
      "Epoch 1 start\n",
      "Batch 0: input_ids.shape=torch.Size([6, 15]), labels.shape=torch.Size([6, 15]), mask.shape=torch.Size([6, 1, 15, 15])\n",
      "Forward pass start: batch_size=6, seq_len=15, start_pos=0, use_cache=False\n",
      "After embeddings: h.shape=torch.Size([6, 15, 256])\n",
      "RoPE freqs_cis shape: torch.Size([15, 32])\n",
      "Layer 0 forward start\n",
      "Attention forward: bsz=6, seqlen=15\n",
      "QKV projections: xq.shape=torch.Size([6, 15, 256]), xk.shape=torch.Size([6, 15, 128]), xv.shape=torch.Size([6, 15, 128])\n",
      "After reshape: xq.shape=torch.Size([6, 15, 4, 64]), xk.shape=torch.Size([6, 15, 2, 64]), xv.shape=torch.Size([6, 15, 2, 64])\n",
      "RoPE applied\n",
      "No KV cache used\n",
      "After GQA repeat: keys.shape=torch.Size([6, 15, 4, 64]), values.shape=torch.Size([6, 15, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([6, 4, 15, 64]), keys.shape=torch.Size([6, 4, 15, 64])\n",
      "Scores computed: scores.shape=torch.Size([6, 4, 15, 15])\n",
      "Mask applied\n",
      "Probs shape: torch.Size([6, 4, 15, 15])\n",
      "Attention output shape: torch.Size([6, 4, 15, 64])\n",
      "Layer 0 output shape: torch.Size([6, 15, 256])\n",
      "Layer 1 forward start\n",
      "Attention forward: bsz=6, seqlen=15\n",
      "QKV projections: xq.shape=torch.Size([6, 15, 256]), xk.shape=torch.Size([6, 15, 128]), xv.shape=torch.Size([6, 15, 128])\n",
      "After reshape: xq.shape=torch.Size([6, 15, 4, 64]), xk.shape=torch.Size([6, 15, 2, 64]), xv.shape=torch.Size([6, 15, 2, 64])\n",
      "RoPE applied\n",
      "No KV cache used\n",
      "After GQA repeat: keys.shape=torch.Size([6, 15, 4, 64]), values.shape=torch.Size([6, 15, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([6, 4, 15, 64]), keys.shape=torch.Size([6, 4, 15, 64])\n",
      "Scores computed: scores.shape=torch.Size([6, 4, 15, 15])\n",
      "Mask applied\n",
      "Probs shape: torch.Size([6, 4, 15, 15])\n",
      "Attention output shape: torch.Size([6, 4, 15, 64])\n",
      "Layer 1 output shape: torch.Size([6, 15, 256])\n",
      "Layer 2 forward start\n",
      "Attention forward: bsz=6, seqlen=15\n",
      "QKV projections: xq.shape=torch.Size([6, 15, 256]), xk.shape=torch.Size([6, 15, 128]), xv.shape=torch.Size([6, 15, 128])\n",
      "After reshape: xq.shape=torch.Size([6, 15, 4, 64]), xk.shape=torch.Size([6, 15, 2, 64]), xv.shape=torch.Size([6, 15, 2, 64])\n",
      "RoPE applied\n",
      "No KV cache used\n",
      "After GQA repeat: keys.shape=torch.Size([6, 15, 4, 64]), values.shape=torch.Size([6, 15, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([6, 4, 15, 64]), keys.shape=torch.Size([6, 4, 15, 64])\n",
      "Scores computed: scores.shape=torch.Size([6, 4, 15, 15])\n",
      "Mask applied\n",
      "Probs shape: torch.Size([6, 4, 15, 15])\n",
      "Attention output shape: torch.Size([6, 4, 15, 64])\n",
      "Layer 2 output shape: torch.Size([6, 15, 256])\n",
      "Layer 3 forward start\n",
      "Attention forward: bsz=6, seqlen=15\n",
      "QKV projections: xq.shape=torch.Size([6, 15, 256]), xk.shape=torch.Size([6, 15, 128]), xv.shape=torch.Size([6, 15, 128])\n",
      "After reshape: xq.shape=torch.Size([6, 15, 4, 64]), xk.shape=torch.Size([6, 15, 2, 64]), xv.shape=torch.Size([6, 15, 2, 64])\n",
      "RoPE applied\n",
      "No KV cache used\n",
      "After GQA repeat: keys.shape=torch.Size([6, 15, 4, 64]), values.shape=torch.Size([6, 15, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([6, 4, 15, 64]), keys.shape=torch.Size([6, 4, 15, 64])\n",
      "Scores computed: scores.shape=torch.Size([6, 4, 15, 15])\n",
      "Mask applied\n",
      "Probs shape: torch.Size([6, 4, 15, 15])\n",
      "Attention output shape: torch.Size([6, 4, 15, 64])\n",
      "Layer 3 output shape: torch.Size([6, 15, 256])\n",
      "After norm: h.shape=torch.Size([6, 15, 256])\n",
      "Logits shape: torch.Size([6, 15, 127])\n",
      "Logits from model: shape=torch.Size([6, 15, 127])\n",
      "Loss computed: 4.5059\n",
      "Epoch 1 | Batch 0 | Loss: 4.5059\n",
      "Batch 1: input_ids.shape=torch.Size([6, 15]), labels.shape=torch.Size([6, 15]), mask.shape=torch.Size([6, 1, 15, 15])\n",
      "Forward pass start: batch_size=6, seq_len=15, start_pos=0, use_cache=False\n",
      "After embeddings: h.shape=torch.Size([6, 15, 256])\n",
      "RoPE freqs_cis shape: torch.Size([15, 32])\n",
      "Layer 0 forward start\n",
      "Attention forward: bsz=6, seqlen=15\n",
      "QKV projections: xq.shape=torch.Size([6, 15, 256]), xk.shape=torch.Size([6, 15, 128]), xv.shape=torch.Size([6, 15, 128])\n",
      "After reshape: xq.shape=torch.Size([6, 15, 4, 64]), xk.shape=torch.Size([6, 15, 2, 64]), xv.shape=torch.Size([6, 15, 2, 64])\n",
      "RoPE applied\n",
      "No KV cache used\n",
      "After GQA repeat: keys.shape=torch.Size([6, 15, 4, 64]), values.shape=torch.Size([6, 15, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([6, 4, 15, 64]), keys.shape=torch.Size([6, 4, 15, 64])\n",
      "Scores computed: scores.shape=torch.Size([6, 4, 15, 15])\n",
      "Mask applied\n",
      "Probs shape: torch.Size([6, 4, 15, 15])\n",
      "Attention output shape: torch.Size([6, 4, 15, 64])\n",
      "Layer 0 output shape: torch.Size([6, 15, 256])\n",
      "Layer 1 forward start\n",
      "Attention forward: bsz=6, seqlen=15\n",
      "QKV projections: xq.shape=torch.Size([6, 15, 256]), xk.shape=torch.Size([6, 15, 128]), xv.shape=torch.Size([6, 15, 128])\n",
      "After reshape: xq.shape=torch.Size([6, 15, 4, 64]), xk.shape=torch.Size([6, 15, 2, 64]), xv.shape=torch.Size([6, 15, 2, 64])\n",
      "RoPE applied\n",
      "No KV cache used\n",
      "After GQA repeat: keys.shape=torch.Size([6, 15, 4, 64]), values.shape=torch.Size([6, 15, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([6, 4, 15, 64]), keys.shape=torch.Size([6, 4, 15, 64])\n",
      "Scores computed: scores.shape=torch.Size([6, 4, 15, 15])\n",
      "Mask applied\n",
      "Probs shape: torch.Size([6, 4, 15, 15])\n",
      "Attention output shape: torch.Size([6, 4, 15, 64])\n",
      "Layer 1 output shape: torch.Size([6, 15, 256])\n",
      "Layer 2 forward start\n",
      "Attention forward: bsz=6, seqlen=15\n",
      "QKV projections: xq.shape=torch.Size([6, 15, 256]), xk.shape=torch.Size([6, 15, 128]), xv.shape=torch.Size([6, 15, 128])\n",
      "After reshape: xq.shape=torch.Size([6, 15, 4, 64]), xk.shape=torch.Size([6, 15, 2, 64]), xv.shape=torch.Size([6, 15, 2, 64])\n",
      "RoPE applied\n",
      "No KV cache used\n",
      "After GQA repeat: keys.shape=torch.Size([6, 15, 4, 64]), values.shape=torch.Size([6, 15, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([6, 4, 15, 64]), keys.shape=torch.Size([6, 4, 15, 64])\n",
      "Scores computed: scores.shape=torch.Size([6, 4, 15, 15])\n",
      "Mask applied\n",
      "Probs shape: torch.Size([6, 4, 15, 15])\n",
      "Attention output shape: torch.Size([6, 4, 15, 64])\n",
      "Layer 2 output shape: torch.Size([6, 15, 256])\n",
      "Layer 3 forward start\n",
      "Attention forward: bsz=6, seqlen=15\n",
      "QKV projections: xq.shape=torch.Size([6, 15, 256]), xk.shape=torch.Size([6, 15, 128]), xv.shape=torch.Size([6, 15, 128])\n",
      "After reshape: xq.shape=torch.Size([6, 15, 4, 64]), xk.shape=torch.Size([6, 15, 2, 64]), xv.shape=torch.Size([6, 15, 2, 64])\n",
      "RoPE applied\n",
      "No KV cache used\n",
      "After GQA repeat: keys.shape=torch.Size([6, 15, 4, 64]), values.shape=torch.Size([6, 15, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([6, 4, 15, 64]), keys.shape=torch.Size([6, 4, 15, 64])\n",
      "Scores computed: scores.shape=torch.Size([6, 4, 15, 15])\n",
      "Mask applied\n",
      "Probs shape: torch.Size([6, 4, 15, 15])\n",
      "Attention output shape: torch.Size([6, 4, 15, 64])\n",
      "Layer 3 output shape: torch.Size([6, 15, 256])\n",
      "After norm: h.shape=torch.Size([6, 15, 256])\n",
      "Logits shape: torch.Size([6, 15, 127])\n",
      "Logits from model: shape=torch.Size([6, 15, 127])\n",
      "Loss computed: 4.2059\n",
      "Batch 2: input_ids.shape=torch.Size([3, 15]), labels.shape=torch.Size([3, 15]), mask.shape=torch.Size([3, 1, 15, 15])\n",
      "Forward pass start: batch_size=3, seq_len=15, start_pos=0, use_cache=False\n",
      "After embeddings: h.shape=torch.Size([3, 15, 256])\n",
      "RoPE freqs_cis shape: torch.Size([15, 32])\n",
      "Layer 0 forward start\n",
      "Attention forward: bsz=3, seqlen=15\n",
      "QKV projections: xq.shape=torch.Size([3, 15, 256]), xk.shape=torch.Size([3, 15, 128]), xv.shape=torch.Size([3, 15, 128])\n",
      "After reshape: xq.shape=torch.Size([3, 15, 4, 64]), xk.shape=torch.Size([3, 15, 2, 64]), xv.shape=torch.Size([3, 15, 2, 64])\n",
      "RoPE applied\n",
      "No KV cache used\n",
      "After GQA repeat: keys.shape=torch.Size([3, 15, 4, 64]), values.shape=torch.Size([3, 15, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([3, 4, 15, 64]), keys.shape=torch.Size([3, 4, 15, 64])\n",
      "Scores computed: scores.shape=torch.Size([3, 4, 15, 15])\n",
      "Mask applied\n",
      "Probs shape: torch.Size([3, 4, 15, 15])\n",
      "Attention output shape: torch.Size([3, 4, 15, 64])\n",
      "Layer 0 output shape: torch.Size([3, 15, 256])\n",
      "Layer 1 forward start\n",
      "Attention forward: bsz=3, seqlen=15\n",
      "QKV projections: xq.shape=torch.Size([3, 15, 256]), xk.shape=torch.Size([3, 15, 128]), xv.shape=torch.Size([3, 15, 128])\n",
      "After reshape: xq.shape=torch.Size([3, 15, 4, 64]), xk.shape=torch.Size([3, 15, 2, 64]), xv.shape=torch.Size([3, 15, 2, 64])\n",
      "RoPE applied\n",
      "No KV cache used\n",
      "After GQA repeat: keys.shape=torch.Size([3, 15, 4, 64]), values.shape=torch.Size([3, 15, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([3, 4, 15, 64]), keys.shape=torch.Size([3, 4, 15, 64])\n",
      "Scores computed: scores.shape=torch.Size([3, 4, 15, 15])\n",
      "Mask applied\n",
      "Probs shape: torch.Size([3, 4, 15, 15])\n",
      "Attention output shape: torch.Size([3, 4, 15, 64])\n",
      "Layer 1 output shape: torch.Size([3, 15, 256])\n",
      "Layer 2 forward start\n",
      "Attention forward: bsz=3, seqlen=15\n",
      "QKV projections: xq.shape=torch.Size([3, 15, 256]), xk.shape=torch.Size([3, 15, 128]), xv.shape=torch.Size([3, 15, 128])\n",
      "After reshape: xq.shape=torch.Size([3, 15, 4, 64]), xk.shape=torch.Size([3, 15, 2, 64]), xv.shape=torch.Size([3, 15, 2, 64])\n",
      "RoPE applied\n",
      "No KV cache used\n",
      "After GQA repeat: keys.shape=torch.Size([3, 15, 4, 64]), values.shape=torch.Size([3, 15, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([3, 4, 15, 64]), keys.shape=torch.Size([3, 4, 15, 64])\n",
      "Scores computed: scores.shape=torch.Size([3, 4, 15, 15])\n",
      "Mask applied\n",
      "Probs shape: torch.Size([3, 4, 15, 15])\n",
      "Attention output shape: torch.Size([3, 4, 15, 64])\n",
      "Layer 2 output shape: torch.Size([3, 15, 256])\n",
      "Layer 3 forward start\n",
      "Attention forward: bsz=3, seqlen=15\n",
      "QKV projections: xq.shape=torch.Size([3, 15, 256]), xk.shape=torch.Size([3, 15, 128]), xv.shape=torch.Size([3, 15, 128])\n",
      "After reshape: xq.shape=torch.Size([3, 15, 4, 64]), xk.shape=torch.Size([3, 15, 2, 64]), xv.shape=torch.Size([3, 15, 2, 64])\n",
      "RoPE applied\n",
      "No KV cache used\n",
      "After GQA repeat: keys.shape=torch.Size([3, 15, 4, 64]), values.shape=torch.Size([3, 15, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([3, 4, 15, 64]), keys.shape=torch.Size([3, 4, 15, 64])\n",
      "Scores computed: scores.shape=torch.Size([3, 4, 15, 15])\n",
      "Mask applied\n",
      "Probs shape: torch.Size([3, 4, 15, 15])\n",
      "Attention output shape: torch.Size([3, 4, 15, 64])\n",
      "Layer 3 output shape: torch.Size([3, 15, 256])\n",
      "After norm: h.shape=torch.Size([3, 15, 256])\n",
      "Logits shape: torch.Size([3, 15, 127])\n",
      "Logits from model: shape=torch.Size([3, 15, 127])\n",
      "Loss computed: 3.9187\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Training Loop ---\n",
    "print(\"\\n--- Starting Training (with Packed Sequences) ---\")\n",
    "model.train()\n",
    "\n",
    "for epoch in range(2):\n",
    "    print(f\"Epoch {epoch} start\")\n",
    "    total_loss = 0\n",
    "    for i, input_data in enumerate(train_loader):\n",
    "        input_ids = input_data[\"input_ids\"].to(device)\n",
    "        labels = input_data[\"labels\"].to(device)\n",
    "        combined_mask = input_data[\"combined_mask\"].unsqueeze(1).to(device)\n",
    "        print(f\"Batch {i}: input_ids.shape={input_ids.shape}, labels.shape={labels.shape}, mask.shape={combined_mask.shape}\")\n",
    "        \n",
    "        # Forward pass with combined mask\n",
    "        logits = model(input_ids, mask=combined_mask, use_cache=False)\n",
    "        print(f\"Logits from model: shape={logits.shape}\")\n",
    "        \n",
    "        # Reshape for loss\n",
    "        loss = loss_fn(logits.view(-1, config.vocab_size), labels.view(-1))\n",
    "        print(f\"Loss computed: {loss.item():.4f}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if i % 5 == 0:\n",
    "            print(f\"Epoch {epoch} | Batch {i} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0cbec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def generate(model, prompt, max_new_tokens):\n",
    "    model.eval()\n",
    "    model.clear_kv_cache() # Reset cache fro new generation\n",
    "    print(\"Generation start: clearing KV cache\")\n",
    "    \n",
    "    # Encode the prompt text using the tokenizer\n",
    "    tokens = tokenizer.encode_inference(prompt).unsqueeze(0).to(device=device)  # Shape: (1, seq_len)\n",
    "    print(f\"Encoded prompt tokens: {tokens}\")\n",
    "    \n",
    "    # 1. Prefill Phase\n",
    "    # We pass the whole prompt to fill the KV cache\n",
    "    # Causal mask for the prompt: lower triangle (including diagonal) = 0, upper = -inf\n",
    "    seq_len = tokens.shape[1]\n",
    "    mask = torch.full((seq_len, seq_len), float(\"-inf\"), device=device)\n",
    "    mask = torch.triu(mask, diagonal=1)\n",
    "    print(f\"Prefill mask shape: {mask.shape}\")\n",
    "    \n",
    "    logits = model(tokens, start_pos=0, mask=mask, use_cache=True)\n",
    "    next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "    \n",
    "    generated = [next_token.item()]\n",
    "    print(f\"Initial generated sequence: {generated}\")\n",
    "    \n",
    "    # 2. Generation Phase (Token by Token)\n",
    "    cur_pos = seq_len\n",
    "    input_token = next_token\n",
    "    \n",
    "    for i in range(max_new_tokens-1):\n",
    "        print(f\"Generating token {i+1}: cur_pos={cur_pos}\")\n",
    "        # Pass only the single new token, no mask needed for autoregressive generation\n",
    "        logits = model(input_token, start_pos=cur_pos, mask=None, use_cache=True)\n",
    "        next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "        \n",
    "        input_token = next_token\n",
    "        cur_pos += 1\n",
    "        generated.append(next_token.item())\n",
    "        print(f\"Token {i+1}: {next_token.item()}\")\n",
    "        print(f\"generated: {generated}\")\n",
    "        \n",
    "    # Decode the generated tokens to text\n",
    "    decoded_text = tokenizer.decode(generated)\n",
    "    print(f\"Generated text: {decoded_text}\")\n",
    "    return generated, decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58961437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded prompt tokens: tensor([[ 2, 84, 36, 75,  4, 75, 36,  0]])\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.encode_inference(\"an end-to-end approach\").unsqueeze(0).to(device=device)  # Shape: (1, seq_len)\n",
    "print(f\"Encoded prompt tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8e71cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KV cache cleared\n",
      "Generation start: clearing KV cache\n",
      "Encoded prompt tokens: tensor([[ 2,  0,  6,  0,  0,  0, 38]])\n",
      "Prefill mask shape: torch.Size([7, 7])\n",
      "Forward pass start: batch_size=1, seq_len=7, start_pos=0, use_cache=True\n",
      "After embeddings: h.shape=torch.Size([1, 7, 256])\n",
      "RoPE freqs_cis shape: torch.Size([7, 32])\n",
      "Layer 0 forward start\n",
      "Attention forward: bsz=1, seqlen=7\n",
      "QKV projections: xq.shape=torch.Size([1, 7, 256]), xk.shape=torch.Size([1, 7, 128]), xv.shape=torch.Size([1, 7, 128])\n",
      "After reshape: xq.shape=torch.Size([1, 7, 4, 64]), xk.shape=torch.Size([1, 7, 2, 64]), xv.shape=torch.Size([1, 7, 2, 64])\n",
      "RoPE applied\n",
      "KV cache initialized\n",
      "KV cache updated: cache_k.shape=torch.Size([1, 7, 2, 64])\n",
      "After GQA repeat: keys.shape=torch.Size([1, 7, 4, 64]), values.shape=torch.Size([1, 7, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([1, 4, 7, 64]), keys.shape=torch.Size([1, 4, 7, 64])\n",
      "Scores computed: scores.shape=torch.Size([1, 4, 7, 7])\n",
      "Mask applied\n",
      "Probs shape: torch.Size([1, 4, 7, 7])\n",
      "Attention output shape: torch.Size([1, 4, 7, 64])\n",
      "Layer 0 output shape: torch.Size([1, 7, 256])\n",
      "Layer 1 forward start\n",
      "Attention forward: bsz=1, seqlen=7\n",
      "QKV projections: xq.shape=torch.Size([1, 7, 256]), xk.shape=torch.Size([1, 7, 128]), xv.shape=torch.Size([1, 7, 128])\n",
      "After reshape: xq.shape=torch.Size([1, 7, 4, 64]), xk.shape=torch.Size([1, 7, 2, 64]), xv.shape=torch.Size([1, 7, 2, 64])\n",
      "RoPE applied\n",
      "KV cache initialized\n",
      "KV cache updated: cache_k.shape=torch.Size([1, 7, 2, 64])\n",
      "After GQA repeat: keys.shape=torch.Size([1, 7, 4, 64]), values.shape=torch.Size([1, 7, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([1, 4, 7, 64]), keys.shape=torch.Size([1, 4, 7, 64])\n",
      "Scores computed: scores.shape=torch.Size([1, 4, 7, 7])\n",
      "Mask applied\n",
      "Probs shape: torch.Size([1, 4, 7, 7])\n",
      "Attention output shape: torch.Size([1, 4, 7, 64])\n",
      "Layer 1 output shape: torch.Size([1, 7, 256])\n",
      "Layer 2 forward start\n",
      "Attention forward: bsz=1, seqlen=7\n",
      "QKV projections: xq.shape=torch.Size([1, 7, 256]), xk.shape=torch.Size([1, 7, 128]), xv.shape=torch.Size([1, 7, 128])\n",
      "After reshape: xq.shape=torch.Size([1, 7, 4, 64]), xk.shape=torch.Size([1, 7, 2, 64]), xv.shape=torch.Size([1, 7, 2, 64])\n",
      "RoPE applied\n",
      "KV cache initialized\n",
      "KV cache updated: cache_k.shape=torch.Size([1, 7, 2, 64])\n",
      "After GQA repeat: keys.shape=torch.Size([1, 7, 4, 64]), values.shape=torch.Size([1, 7, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([1, 4, 7, 64]), keys.shape=torch.Size([1, 4, 7, 64])\n",
      "Scores computed: scores.shape=torch.Size([1, 4, 7, 7])\n",
      "Mask applied\n",
      "Probs shape: torch.Size([1, 4, 7, 7])\n",
      "Attention output shape: torch.Size([1, 4, 7, 64])\n",
      "Layer 2 output shape: torch.Size([1, 7, 256])\n",
      "Layer 3 forward start\n",
      "Attention forward: bsz=1, seqlen=7\n",
      "QKV projections: xq.shape=torch.Size([1, 7, 256]), xk.shape=torch.Size([1, 7, 128]), xv.shape=torch.Size([1, 7, 128])\n",
      "After reshape: xq.shape=torch.Size([1, 7, 4, 64]), xk.shape=torch.Size([1, 7, 2, 64]), xv.shape=torch.Size([1, 7, 2, 64])\n",
      "RoPE applied\n",
      "KV cache initialized\n",
      "KV cache updated: cache_k.shape=torch.Size([1, 7, 2, 64])\n",
      "After GQA repeat: keys.shape=torch.Size([1, 7, 4, 64]), values.shape=torch.Size([1, 7, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([1, 4, 7, 64]), keys.shape=torch.Size([1, 4, 7, 64])\n",
      "Scores computed: scores.shape=torch.Size([1, 4, 7, 7])\n",
      "Mask applied\n",
      "Probs shape: torch.Size([1, 4, 7, 7])\n",
      "Attention output shape: torch.Size([1, 4, 7, 64])\n",
      "Layer 3 output shape: torch.Size([1, 7, 256])\n",
      "After norm: h.shape=torch.Size([1, 7, 256])\n",
      "Logits shape: torch.Size([1, 7, 127])\n",
      "Initial generated sequence: [0]\n",
      "Generating token 1: cur_pos=7\n",
      "Forward pass start: batch_size=1, seq_len=1, start_pos=7, use_cache=True\n",
      "After embeddings: h.shape=torch.Size([1, 1, 256])\n",
      "RoPE freqs_cis shape: torch.Size([1, 32])\n",
      "Layer 0 forward start\n",
      "Attention forward: bsz=1, seqlen=1\n",
      "QKV projections: xq.shape=torch.Size([1, 1, 256]), xk.shape=torch.Size([1, 1, 128]), xv.shape=torch.Size([1, 1, 128])\n",
      "After reshape: xq.shape=torch.Size([1, 1, 4, 64]), xk.shape=torch.Size([1, 1, 2, 64]), xv.shape=torch.Size([1, 1, 2, 64])\n",
      "RoPE applied\n",
      "KV cache updated: cache_k.shape=torch.Size([1, 8, 2, 64])\n",
      "After GQA repeat: keys.shape=torch.Size([1, 8, 4, 64]), values.shape=torch.Size([1, 8, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([1, 4, 1, 64]), keys.shape=torch.Size([1, 4, 8, 64])\n",
      "Scores computed: scores.shape=torch.Size([1, 4, 1, 8])\n",
      "Probs shape: torch.Size([1, 4, 1, 8])\n",
      "Attention output shape: torch.Size([1, 4, 1, 64])\n",
      "Layer 0 output shape: torch.Size([1, 1, 256])\n",
      "Layer 1 forward start\n",
      "Attention forward: bsz=1, seqlen=1\n",
      "QKV projections: xq.shape=torch.Size([1, 1, 256]), xk.shape=torch.Size([1, 1, 128]), xv.shape=torch.Size([1, 1, 128])\n",
      "After reshape: xq.shape=torch.Size([1, 1, 4, 64]), xk.shape=torch.Size([1, 1, 2, 64]), xv.shape=torch.Size([1, 1, 2, 64])\n",
      "RoPE applied\n",
      "KV cache updated: cache_k.shape=torch.Size([1, 8, 2, 64])\n",
      "After GQA repeat: keys.shape=torch.Size([1, 8, 4, 64]), values.shape=torch.Size([1, 8, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([1, 4, 1, 64]), keys.shape=torch.Size([1, 4, 8, 64])\n",
      "Scores computed: scores.shape=torch.Size([1, 4, 1, 8])\n",
      "Probs shape: torch.Size([1, 4, 1, 8])\n",
      "Attention output shape: torch.Size([1, 4, 1, 64])\n",
      "Layer 1 output shape: torch.Size([1, 1, 256])\n",
      "Layer 2 forward start\n",
      "Attention forward: bsz=1, seqlen=1\n",
      "QKV projections: xq.shape=torch.Size([1, 1, 256]), xk.shape=torch.Size([1, 1, 128]), xv.shape=torch.Size([1, 1, 128])\n",
      "After reshape: xq.shape=torch.Size([1, 1, 4, 64]), xk.shape=torch.Size([1, 1, 2, 64]), xv.shape=torch.Size([1, 1, 2, 64])\n",
      "RoPE applied\n",
      "KV cache updated: cache_k.shape=torch.Size([1, 8, 2, 64])\n",
      "After GQA repeat: keys.shape=torch.Size([1, 8, 4, 64]), values.shape=torch.Size([1, 8, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([1, 4, 1, 64]), keys.shape=torch.Size([1, 4, 8, 64])\n",
      "Scores computed: scores.shape=torch.Size([1, 4, 1, 8])\n",
      "Probs shape: torch.Size([1, 4, 1, 8])\n",
      "Attention output shape: torch.Size([1, 4, 1, 64])\n",
      "Layer 2 output shape: torch.Size([1, 1, 256])\n",
      "Layer 3 forward start\n",
      "Attention forward: bsz=1, seqlen=1\n",
      "QKV projections: xq.shape=torch.Size([1, 1, 256]), xk.shape=torch.Size([1, 1, 128]), xv.shape=torch.Size([1, 1, 128])\n",
      "After reshape: xq.shape=torch.Size([1, 1, 4, 64]), xk.shape=torch.Size([1, 1, 2, 64]), xv.shape=torch.Size([1, 1, 2, 64])\n",
      "RoPE applied\n",
      "KV cache updated: cache_k.shape=torch.Size([1, 8, 2, 64])\n",
      "After GQA repeat: keys.shape=torch.Size([1, 8, 4, 64]), values.shape=torch.Size([1, 8, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([1, 4, 1, 64]), keys.shape=torch.Size([1, 4, 8, 64])\n",
      "Scores computed: scores.shape=torch.Size([1, 4, 1, 8])\n",
      "Probs shape: torch.Size([1, 4, 1, 8])\n",
      "Attention output shape: torch.Size([1, 4, 1, 64])\n",
      "Layer 3 output shape: torch.Size([1, 1, 256])\n",
      "After norm: h.shape=torch.Size([1, 1, 256])\n",
      "Logits shape: torch.Size([1, 1, 127])\n",
      "Token 1: 9\n",
      "generated: [0, 9]\n",
      "Generating token 2: cur_pos=8\n",
      "Forward pass start: batch_size=1, seq_len=1, start_pos=8, use_cache=True\n",
      "After embeddings: h.shape=torch.Size([1, 1, 256])\n",
      "RoPE freqs_cis shape: torch.Size([1, 32])\n",
      "Layer 0 forward start\n",
      "Attention forward: bsz=1, seqlen=1\n",
      "QKV projections: xq.shape=torch.Size([1, 1, 256]), xk.shape=torch.Size([1, 1, 128]), xv.shape=torch.Size([1, 1, 128])\n",
      "After reshape: xq.shape=torch.Size([1, 1, 4, 64]), xk.shape=torch.Size([1, 1, 2, 64]), xv.shape=torch.Size([1, 1, 2, 64])\n",
      "RoPE applied\n",
      "KV cache updated: cache_k.shape=torch.Size([1, 9, 2, 64])\n",
      "After GQA repeat: keys.shape=torch.Size([1, 9, 4, 64]), values.shape=torch.Size([1, 9, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([1, 4, 1, 64]), keys.shape=torch.Size([1, 4, 9, 64])\n",
      "Scores computed: scores.shape=torch.Size([1, 4, 1, 9])\n",
      "Probs shape: torch.Size([1, 4, 1, 9])\n",
      "Attention output shape: torch.Size([1, 4, 1, 64])\n",
      "Layer 0 output shape: torch.Size([1, 1, 256])\n",
      "Layer 1 forward start\n",
      "Attention forward: bsz=1, seqlen=1\n",
      "QKV projections: xq.shape=torch.Size([1, 1, 256]), xk.shape=torch.Size([1, 1, 128]), xv.shape=torch.Size([1, 1, 128])\n",
      "After reshape: xq.shape=torch.Size([1, 1, 4, 64]), xk.shape=torch.Size([1, 1, 2, 64]), xv.shape=torch.Size([1, 1, 2, 64])\n",
      "RoPE applied\n",
      "KV cache updated: cache_k.shape=torch.Size([1, 9, 2, 64])\n",
      "After GQA repeat: keys.shape=torch.Size([1, 9, 4, 64]), values.shape=torch.Size([1, 9, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([1, 4, 1, 64]), keys.shape=torch.Size([1, 4, 9, 64])\n",
      "Scores computed: scores.shape=torch.Size([1, 4, 1, 9])\n",
      "Probs shape: torch.Size([1, 4, 1, 9])\n",
      "Attention output shape: torch.Size([1, 4, 1, 64])\n",
      "Layer 1 output shape: torch.Size([1, 1, 256])\n",
      "Layer 2 forward start\n",
      "Attention forward: bsz=1, seqlen=1\n",
      "QKV projections: xq.shape=torch.Size([1, 1, 256]), xk.shape=torch.Size([1, 1, 128]), xv.shape=torch.Size([1, 1, 128])\n",
      "After reshape: xq.shape=torch.Size([1, 1, 4, 64]), xk.shape=torch.Size([1, 1, 2, 64]), xv.shape=torch.Size([1, 1, 2, 64])\n",
      "RoPE applied\n",
      "KV cache updated: cache_k.shape=torch.Size([1, 9, 2, 64])\n",
      "After GQA repeat: keys.shape=torch.Size([1, 9, 4, 64]), values.shape=torch.Size([1, 9, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([1, 4, 1, 64]), keys.shape=torch.Size([1, 4, 9, 64])\n",
      "Scores computed: scores.shape=torch.Size([1, 4, 1, 9])\n",
      "Probs shape: torch.Size([1, 4, 1, 9])\n",
      "Attention output shape: torch.Size([1, 4, 1, 64])\n",
      "Layer 2 output shape: torch.Size([1, 1, 256])\n",
      "Layer 3 forward start\n",
      "Attention forward: bsz=1, seqlen=1\n",
      "QKV projections: xq.shape=torch.Size([1, 1, 256]), xk.shape=torch.Size([1, 1, 128]), xv.shape=torch.Size([1, 1, 128])\n",
      "After reshape: xq.shape=torch.Size([1, 1, 4, 64]), xk.shape=torch.Size([1, 1, 2, 64]), xv.shape=torch.Size([1, 1, 2, 64])\n",
      "RoPE applied\n",
      "KV cache updated: cache_k.shape=torch.Size([1, 9, 2, 64])\n",
      "After GQA repeat: keys.shape=torch.Size([1, 9, 4, 64]), values.shape=torch.Size([1, 9, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([1, 4, 1, 64]), keys.shape=torch.Size([1, 4, 9, 64])\n",
      "Scores computed: scores.shape=torch.Size([1, 4, 1, 9])\n",
      "Probs shape: torch.Size([1, 4, 1, 9])\n",
      "Attention output shape: torch.Size([1, 4, 1, 64])\n",
      "Layer 3 output shape: torch.Size([1, 1, 256])\n",
      "After norm: h.shape=torch.Size([1, 1, 256])\n",
      "Logits shape: torch.Size([1, 1, 127])\n",
      "Token 2: 74\n",
      "generated: [0, 9, 74]\n",
      "Generating token 3: cur_pos=9\n",
      "Forward pass start: batch_size=1, seq_len=1, start_pos=9, use_cache=True\n",
      "After embeddings: h.shape=torch.Size([1, 1, 256])\n",
      "RoPE freqs_cis shape: torch.Size([1, 32])\n",
      "Layer 0 forward start\n",
      "Attention forward: bsz=1, seqlen=1\n",
      "QKV projections: xq.shape=torch.Size([1, 1, 256]), xk.shape=torch.Size([1, 1, 128]), xv.shape=torch.Size([1, 1, 128])\n",
      "After reshape: xq.shape=torch.Size([1, 1, 4, 64]), xk.shape=torch.Size([1, 1, 2, 64]), xv.shape=torch.Size([1, 1, 2, 64])\n",
      "RoPE applied\n",
      "KV cache updated: cache_k.shape=torch.Size([1, 10, 2, 64])\n",
      "After GQA repeat: keys.shape=torch.Size([1, 10, 4, 64]), values.shape=torch.Size([1, 10, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([1, 4, 1, 64]), keys.shape=torch.Size([1, 4, 10, 64])\n",
      "Scores computed: scores.shape=torch.Size([1, 4, 1, 10])\n",
      "Probs shape: torch.Size([1, 4, 1, 10])\n",
      "Attention output shape: torch.Size([1, 4, 1, 64])\n",
      "Layer 0 output shape: torch.Size([1, 1, 256])\n",
      "Layer 1 forward start\n",
      "Attention forward: bsz=1, seqlen=1\n",
      "QKV projections: xq.shape=torch.Size([1, 1, 256]), xk.shape=torch.Size([1, 1, 128]), xv.shape=torch.Size([1, 1, 128])\n",
      "After reshape: xq.shape=torch.Size([1, 1, 4, 64]), xk.shape=torch.Size([1, 1, 2, 64]), xv.shape=torch.Size([1, 1, 2, 64])\n",
      "RoPE applied\n",
      "KV cache updated: cache_k.shape=torch.Size([1, 10, 2, 64])\n",
      "After GQA repeat: keys.shape=torch.Size([1, 10, 4, 64]), values.shape=torch.Size([1, 10, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([1, 4, 1, 64]), keys.shape=torch.Size([1, 4, 10, 64])\n",
      "Scores computed: scores.shape=torch.Size([1, 4, 1, 10])\n",
      "Probs shape: torch.Size([1, 4, 1, 10])\n",
      "Attention output shape: torch.Size([1, 4, 1, 64])\n",
      "Layer 1 output shape: torch.Size([1, 1, 256])\n",
      "Layer 2 forward start\n",
      "Attention forward: bsz=1, seqlen=1\n",
      "QKV projections: xq.shape=torch.Size([1, 1, 256]), xk.shape=torch.Size([1, 1, 128]), xv.shape=torch.Size([1, 1, 128])\n",
      "After reshape: xq.shape=torch.Size([1, 1, 4, 64]), xk.shape=torch.Size([1, 1, 2, 64]), xv.shape=torch.Size([1, 1, 2, 64])\n",
      "RoPE applied\n",
      "KV cache updated: cache_k.shape=torch.Size([1, 10, 2, 64])\n",
      "After GQA repeat: keys.shape=torch.Size([1, 10, 4, 64]), values.shape=torch.Size([1, 10, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([1, 4, 1, 64]), keys.shape=torch.Size([1, 4, 10, 64])\n",
      "Scores computed: scores.shape=torch.Size([1, 4, 1, 10])\n",
      "Probs shape: torch.Size([1, 4, 1, 10])\n",
      "Attention output shape: torch.Size([1, 4, 1, 64])\n",
      "Layer 2 output shape: torch.Size([1, 1, 256])\n",
      "Layer 3 forward start\n",
      "Attention forward: bsz=1, seqlen=1\n",
      "QKV projections: xq.shape=torch.Size([1, 1, 256]), xk.shape=torch.Size([1, 1, 128]), xv.shape=torch.Size([1, 1, 128])\n",
      "After reshape: xq.shape=torch.Size([1, 1, 4, 64]), xk.shape=torch.Size([1, 1, 2, 64]), xv.shape=torch.Size([1, 1, 2, 64])\n",
      "RoPE applied\n",
      "KV cache updated: cache_k.shape=torch.Size([1, 10, 2, 64])\n",
      "After GQA repeat: keys.shape=torch.Size([1, 10, 4, 64]), values.shape=torch.Size([1, 10, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([1, 4, 1, 64]), keys.shape=torch.Size([1, 4, 10, 64])\n",
      "Scores computed: scores.shape=torch.Size([1, 4, 1, 10])\n",
      "Probs shape: torch.Size([1, 4, 1, 10])\n",
      "Attention output shape: torch.Size([1, 4, 1, 64])\n",
      "Layer 3 output shape: torch.Size([1, 1, 256])\n",
      "After norm: h.shape=torch.Size([1, 1, 256])\n",
      "Logits shape: torch.Size([1, 1, 127])\n",
      "Token 3: 16\n",
      "generated: [0, 9, 74, 16]\n",
      "Generating token 4: cur_pos=10\n",
      "Forward pass start: batch_size=1, seq_len=1, start_pos=10, use_cache=True\n",
      "After embeddings: h.shape=torch.Size([1, 1, 256])\n",
      "RoPE freqs_cis shape: torch.Size([1, 32])\n",
      "Layer 0 forward start\n",
      "Attention forward: bsz=1, seqlen=1\n",
      "QKV projections: xq.shape=torch.Size([1, 1, 256]), xk.shape=torch.Size([1, 1, 128]), xv.shape=torch.Size([1, 1, 128])\n",
      "After reshape: xq.shape=torch.Size([1, 1, 4, 64]), xk.shape=torch.Size([1, 1, 2, 64]), xv.shape=torch.Size([1, 1, 2, 64])\n",
      "RoPE applied\n",
      "KV cache updated: cache_k.shape=torch.Size([1, 11, 2, 64])\n",
      "After GQA repeat: keys.shape=torch.Size([1, 11, 4, 64]), values.shape=torch.Size([1, 11, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([1, 4, 1, 64]), keys.shape=torch.Size([1, 4, 11, 64])\n",
      "Scores computed: scores.shape=torch.Size([1, 4, 1, 11])\n",
      "Probs shape: torch.Size([1, 4, 1, 11])\n",
      "Attention output shape: torch.Size([1, 4, 1, 64])\n",
      "Layer 0 output shape: torch.Size([1, 1, 256])\n",
      "Layer 1 forward start\n",
      "Attention forward: bsz=1, seqlen=1\n",
      "QKV projections: xq.shape=torch.Size([1, 1, 256]), xk.shape=torch.Size([1, 1, 128]), xv.shape=torch.Size([1, 1, 128])\n",
      "After reshape: xq.shape=torch.Size([1, 1, 4, 64]), xk.shape=torch.Size([1, 1, 2, 64]), xv.shape=torch.Size([1, 1, 2, 64])\n",
      "RoPE applied\n",
      "KV cache updated: cache_k.shape=torch.Size([1, 11, 2, 64])\n",
      "After GQA repeat: keys.shape=torch.Size([1, 11, 4, 64]), values.shape=torch.Size([1, 11, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([1, 4, 1, 64]), keys.shape=torch.Size([1, 4, 11, 64])\n",
      "Scores computed: scores.shape=torch.Size([1, 4, 1, 11])\n",
      "Probs shape: torch.Size([1, 4, 1, 11])\n",
      "Attention output shape: torch.Size([1, 4, 1, 64])\n",
      "Layer 1 output shape: torch.Size([1, 1, 256])\n",
      "Layer 2 forward start\n",
      "Attention forward: bsz=1, seqlen=1\n",
      "QKV projections: xq.shape=torch.Size([1, 1, 256]), xk.shape=torch.Size([1, 1, 128]), xv.shape=torch.Size([1, 1, 128])\n",
      "After reshape: xq.shape=torch.Size([1, 1, 4, 64]), xk.shape=torch.Size([1, 1, 2, 64]), xv.shape=torch.Size([1, 1, 2, 64])\n",
      "RoPE applied\n",
      "KV cache updated: cache_k.shape=torch.Size([1, 11, 2, 64])\n",
      "After GQA repeat: keys.shape=torch.Size([1, 11, 4, 64]), values.shape=torch.Size([1, 11, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([1, 4, 1, 64]), keys.shape=torch.Size([1, 4, 11, 64])\n",
      "Scores computed: scores.shape=torch.Size([1, 4, 1, 11])\n",
      "Probs shape: torch.Size([1, 4, 1, 11])\n",
      "Attention output shape: torch.Size([1, 4, 1, 64])\n",
      "Layer 2 output shape: torch.Size([1, 1, 256])\n",
      "Layer 3 forward start\n",
      "Attention forward: bsz=1, seqlen=1\n",
      "QKV projections: xq.shape=torch.Size([1, 1, 256]), xk.shape=torch.Size([1, 1, 128]), xv.shape=torch.Size([1, 1, 128])\n",
      "After reshape: xq.shape=torch.Size([1, 1, 4, 64]), xk.shape=torch.Size([1, 1, 2, 64]), xv.shape=torch.Size([1, 1, 2, 64])\n",
      "RoPE applied\n",
      "KV cache updated: cache_k.shape=torch.Size([1, 11, 2, 64])\n",
      "After GQA repeat: keys.shape=torch.Size([1, 11, 4, 64]), values.shape=torch.Size([1, 11, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([1, 4, 1, 64]), keys.shape=torch.Size([1, 4, 11, 64])\n",
      "Scores computed: scores.shape=torch.Size([1, 4, 1, 11])\n",
      "Probs shape: torch.Size([1, 4, 1, 11])\n",
      "Attention output shape: torch.Size([1, 4, 1, 64])\n",
      "Layer 3 output shape: torch.Size([1, 1, 256])\n",
      "After norm: h.shape=torch.Size([1, 1, 256])\n",
      "Logits shape: torch.Size([1, 1, 127])\n",
      "Token 4: 47\n",
      "generated: [0, 9, 74, 16, 47]\n",
      "Generating token 5: cur_pos=11\n",
      "Forward pass start: batch_size=1, seq_len=1, start_pos=11, use_cache=True\n",
      "After embeddings: h.shape=torch.Size([1, 1, 256])\n",
      "RoPE freqs_cis shape: torch.Size([1, 32])\n",
      "Layer 0 forward start\n",
      "Attention forward: bsz=1, seqlen=1\n",
      "QKV projections: xq.shape=torch.Size([1, 1, 256]), xk.shape=torch.Size([1, 1, 128]), xv.shape=torch.Size([1, 1, 128])\n",
      "After reshape: xq.shape=torch.Size([1, 1, 4, 64]), xk.shape=torch.Size([1, 1, 2, 64]), xv.shape=torch.Size([1, 1, 2, 64])\n",
      "RoPE applied\n",
      "KV cache updated: cache_k.shape=torch.Size([1, 12, 2, 64])\n",
      "After GQA repeat: keys.shape=torch.Size([1, 12, 4, 64]), values.shape=torch.Size([1, 12, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([1, 4, 1, 64]), keys.shape=torch.Size([1, 4, 12, 64])\n",
      "Scores computed: scores.shape=torch.Size([1, 4, 1, 12])\n",
      "Probs shape: torch.Size([1, 4, 1, 12])\n",
      "Attention output shape: torch.Size([1, 4, 1, 64])\n",
      "Layer 0 output shape: torch.Size([1, 1, 256])\n",
      "Layer 1 forward start\n",
      "Attention forward: bsz=1, seqlen=1\n",
      "QKV projections: xq.shape=torch.Size([1, 1, 256]), xk.shape=torch.Size([1, 1, 128]), xv.shape=torch.Size([1, 1, 128])\n",
      "After reshape: xq.shape=torch.Size([1, 1, 4, 64]), xk.shape=torch.Size([1, 1, 2, 64]), xv.shape=torch.Size([1, 1, 2, 64])\n",
      "RoPE applied\n",
      "KV cache updated: cache_k.shape=torch.Size([1, 12, 2, 64])\n",
      "After GQA repeat: keys.shape=torch.Size([1, 12, 4, 64]), values.shape=torch.Size([1, 12, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([1, 4, 1, 64]), keys.shape=torch.Size([1, 4, 12, 64])\n",
      "Scores computed: scores.shape=torch.Size([1, 4, 1, 12])\n",
      "Probs shape: torch.Size([1, 4, 1, 12])\n",
      "Attention output shape: torch.Size([1, 4, 1, 64])\n",
      "Layer 1 output shape: torch.Size([1, 1, 256])\n",
      "Layer 2 forward start\n",
      "Attention forward: bsz=1, seqlen=1\n",
      "QKV projections: xq.shape=torch.Size([1, 1, 256]), xk.shape=torch.Size([1, 1, 128]), xv.shape=torch.Size([1, 1, 128])\n",
      "After reshape: xq.shape=torch.Size([1, 1, 4, 64]), xk.shape=torch.Size([1, 1, 2, 64]), xv.shape=torch.Size([1, 1, 2, 64])\n",
      "RoPE applied\n",
      "KV cache updated: cache_k.shape=torch.Size([1, 12, 2, 64])\n",
      "After GQA repeat: keys.shape=torch.Size([1, 12, 4, 64]), values.shape=torch.Size([1, 12, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([1, 4, 1, 64]), keys.shape=torch.Size([1, 4, 12, 64])\n",
      "Scores computed: scores.shape=torch.Size([1, 4, 1, 12])\n",
      "Probs shape: torch.Size([1, 4, 1, 12])\n",
      "Attention output shape: torch.Size([1, 4, 1, 64])\n",
      "Layer 2 output shape: torch.Size([1, 1, 256])\n",
      "Layer 3 forward start\n",
      "Attention forward: bsz=1, seqlen=1\n",
      "QKV projections: xq.shape=torch.Size([1, 1, 256]), xk.shape=torch.Size([1, 1, 128]), xv.shape=torch.Size([1, 1, 128])\n",
      "After reshape: xq.shape=torch.Size([1, 1, 4, 64]), xk.shape=torch.Size([1, 1, 2, 64]), xv.shape=torch.Size([1, 1, 2, 64])\n",
      "RoPE applied\n",
      "KV cache updated: cache_k.shape=torch.Size([1, 12, 2, 64])\n",
      "After GQA repeat: keys.shape=torch.Size([1, 12, 4, 64]), values.shape=torch.Size([1, 12, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([1, 4, 1, 64]), keys.shape=torch.Size([1, 4, 12, 64])\n",
      "Scores computed: scores.shape=torch.Size([1, 4, 1, 12])\n",
      "Probs shape: torch.Size([1, 4, 1, 12])\n",
      "Attention output shape: torch.Size([1, 4, 1, 64])\n",
      "Layer 3 output shape: torch.Size([1, 1, 256])\n",
      "After norm: h.shape=torch.Size([1, 1, 256])\n",
      "Logits shape: torch.Size([1, 1, 127])\n",
      "Token 5: 48\n",
      "generated: [0, 9, 74, 16, 47, 48]\n",
      "Generating token 6: cur_pos=12\n",
      "Forward pass start: batch_size=1, seq_len=1, start_pos=12, use_cache=True\n",
      "After embeddings: h.shape=torch.Size([1, 1, 256])\n",
      "RoPE freqs_cis shape: torch.Size([1, 32])\n",
      "Layer 0 forward start\n",
      "Attention forward: bsz=1, seqlen=1\n",
      "QKV projections: xq.shape=torch.Size([1, 1, 256]), xk.shape=torch.Size([1, 1, 128]), xv.shape=torch.Size([1, 1, 128])\n",
      "After reshape: xq.shape=torch.Size([1, 1, 4, 64]), xk.shape=torch.Size([1, 1, 2, 64]), xv.shape=torch.Size([1, 1, 2, 64])\n",
      "RoPE applied\n",
      "KV cache updated: cache_k.shape=torch.Size([1, 13, 2, 64])\n",
      "After GQA repeat: keys.shape=torch.Size([1, 13, 4, 64]), values.shape=torch.Size([1, 13, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([1, 4, 1, 64]), keys.shape=torch.Size([1, 4, 13, 64])\n",
      "Scores computed: scores.shape=torch.Size([1, 4, 1, 13])\n",
      "Probs shape: torch.Size([1, 4, 1, 13])\n",
      "Attention output shape: torch.Size([1, 4, 1, 64])\n",
      "Layer 0 output shape: torch.Size([1, 1, 256])\n",
      "Layer 1 forward start\n",
      "Attention forward: bsz=1, seqlen=1\n",
      "QKV projections: xq.shape=torch.Size([1, 1, 256]), xk.shape=torch.Size([1, 1, 128]), xv.shape=torch.Size([1, 1, 128])\n",
      "After reshape: xq.shape=torch.Size([1, 1, 4, 64]), xk.shape=torch.Size([1, 1, 2, 64]), xv.shape=torch.Size([1, 1, 2, 64])\n",
      "RoPE applied\n",
      "KV cache updated: cache_k.shape=torch.Size([1, 13, 2, 64])\n",
      "After GQA repeat: keys.shape=torch.Size([1, 13, 4, 64]), values.shape=torch.Size([1, 13, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([1, 4, 1, 64]), keys.shape=torch.Size([1, 4, 13, 64])\n",
      "Scores computed: scores.shape=torch.Size([1, 4, 1, 13])\n",
      "Probs shape: torch.Size([1, 4, 1, 13])\n",
      "Attention output shape: torch.Size([1, 4, 1, 64])\n",
      "Layer 1 output shape: torch.Size([1, 1, 256])\n",
      "Layer 2 forward start\n",
      "Attention forward: bsz=1, seqlen=1\n",
      "QKV projections: xq.shape=torch.Size([1, 1, 256]), xk.shape=torch.Size([1, 1, 128]), xv.shape=torch.Size([1, 1, 128])\n",
      "After reshape: xq.shape=torch.Size([1, 1, 4, 64]), xk.shape=torch.Size([1, 1, 2, 64]), xv.shape=torch.Size([1, 1, 2, 64])\n",
      "RoPE applied\n",
      "KV cache updated: cache_k.shape=torch.Size([1, 13, 2, 64])\n",
      "After GQA repeat: keys.shape=torch.Size([1, 13, 4, 64]), values.shape=torch.Size([1, 13, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([1, 4, 1, 64]), keys.shape=torch.Size([1, 4, 13, 64])\n",
      "Scores computed: scores.shape=torch.Size([1, 4, 1, 13])\n",
      "Probs shape: torch.Size([1, 4, 1, 13])\n",
      "Attention output shape: torch.Size([1, 4, 1, 64])\n",
      "Layer 2 output shape: torch.Size([1, 1, 256])\n",
      "Layer 3 forward start\n",
      "Attention forward: bsz=1, seqlen=1\n",
      "QKV projections: xq.shape=torch.Size([1, 1, 256]), xk.shape=torch.Size([1, 1, 128]), xv.shape=torch.Size([1, 1, 128])\n",
      "After reshape: xq.shape=torch.Size([1, 1, 4, 64]), xk.shape=torch.Size([1, 1, 2, 64]), xv.shape=torch.Size([1, 1, 2, 64])\n",
      "RoPE applied\n",
      "KV cache updated: cache_k.shape=torch.Size([1, 13, 2, 64])\n",
      "After GQA repeat: keys.shape=torch.Size([1, 13, 4, 64]), values.shape=torch.Size([1, 13, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([1, 4, 1, 64]), keys.shape=torch.Size([1, 4, 13, 64])\n",
      "Scores computed: scores.shape=torch.Size([1, 4, 1, 13])\n",
      "Probs shape: torch.Size([1, 4, 1, 13])\n",
      "Attention output shape: torch.Size([1, 4, 1, 64])\n",
      "Layer 3 output shape: torch.Size([1, 1, 256])\n",
      "After norm: h.shape=torch.Size([1, 1, 256])\n",
      "Logits shape: torch.Size([1, 1, 127])\n",
      "Token 6: 74\n",
      "generated: [0, 9, 74, 16, 47, 48, 74]\n",
      "Generating token 7: cur_pos=13\n",
      "Forward pass start: batch_size=1, seq_len=1, start_pos=13, use_cache=True\n",
      "After embeddings: h.shape=torch.Size([1, 1, 256])\n",
      "RoPE freqs_cis shape: torch.Size([1, 32])\n",
      "Layer 0 forward start\n",
      "Attention forward: bsz=1, seqlen=1\n",
      "QKV projections: xq.shape=torch.Size([1, 1, 256]), xk.shape=torch.Size([1, 1, 128]), xv.shape=torch.Size([1, 1, 128])\n",
      "After reshape: xq.shape=torch.Size([1, 1, 4, 64]), xk.shape=torch.Size([1, 1, 2, 64]), xv.shape=torch.Size([1, 1, 2, 64])\n",
      "RoPE applied\n",
      "KV cache updated: cache_k.shape=torch.Size([1, 14, 2, 64])\n",
      "After GQA repeat: keys.shape=torch.Size([1, 14, 4, 64]), values.shape=torch.Size([1, 14, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([1, 4, 1, 64]), keys.shape=torch.Size([1, 4, 14, 64])\n",
      "Scores computed: scores.shape=torch.Size([1, 4, 1, 14])\n",
      "Probs shape: torch.Size([1, 4, 1, 14])\n",
      "Attention output shape: torch.Size([1, 4, 1, 64])\n",
      "Layer 0 output shape: torch.Size([1, 1, 256])\n",
      "Layer 1 forward start\n",
      "Attention forward: bsz=1, seqlen=1\n",
      "QKV projections: xq.shape=torch.Size([1, 1, 256]), xk.shape=torch.Size([1, 1, 128]), xv.shape=torch.Size([1, 1, 128])\n",
      "After reshape: xq.shape=torch.Size([1, 1, 4, 64]), xk.shape=torch.Size([1, 1, 2, 64]), xv.shape=torch.Size([1, 1, 2, 64])\n",
      "RoPE applied\n",
      "KV cache updated: cache_k.shape=torch.Size([1, 14, 2, 64])\n",
      "After GQA repeat: keys.shape=torch.Size([1, 14, 4, 64]), values.shape=torch.Size([1, 14, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([1, 4, 1, 64]), keys.shape=torch.Size([1, 4, 14, 64])\n",
      "Scores computed: scores.shape=torch.Size([1, 4, 1, 14])\n",
      "Probs shape: torch.Size([1, 4, 1, 14])\n",
      "Attention output shape: torch.Size([1, 4, 1, 64])\n",
      "Layer 1 output shape: torch.Size([1, 1, 256])\n",
      "Layer 2 forward start\n",
      "Attention forward: bsz=1, seqlen=1\n",
      "QKV projections: xq.shape=torch.Size([1, 1, 256]), xk.shape=torch.Size([1, 1, 128]), xv.shape=torch.Size([1, 1, 128])\n",
      "After reshape: xq.shape=torch.Size([1, 1, 4, 64]), xk.shape=torch.Size([1, 1, 2, 64]), xv.shape=torch.Size([1, 1, 2, 64])\n",
      "RoPE applied\n",
      "KV cache updated: cache_k.shape=torch.Size([1, 14, 2, 64])\n",
      "After GQA repeat: keys.shape=torch.Size([1, 14, 4, 64]), values.shape=torch.Size([1, 14, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([1, 4, 1, 64]), keys.shape=torch.Size([1, 4, 14, 64])\n",
      "Scores computed: scores.shape=torch.Size([1, 4, 1, 14])\n",
      "Probs shape: torch.Size([1, 4, 1, 14])\n",
      "Attention output shape: torch.Size([1, 4, 1, 64])\n",
      "Layer 2 output shape: torch.Size([1, 1, 256])\n",
      "Layer 3 forward start\n",
      "Attention forward: bsz=1, seqlen=1\n",
      "QKV projections: xq.shape=torch.Size([1, 1, 256]), xk.shape=torch.Size([1, 1, 128]), xv.shape=torch.Size([1, 1, 128])\n",
      "After reshape: xq.shape=torch.Size([1, 1, 4, 64]), xk.shape=torch.Size([1, 1, 2, 64]), xv.shape=torch.Size([1, 1, 2, 64])\n",
      "RoPE applied\n",
      "KV cache updated: cache_k.shape=torch.Size([1, 14, 2, 64])\n",
      "After GQA repeat: keys.shape=torch.Size([1, 14, 4, 64]), values.shape=torch.Size([1, 14, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([1, 4, 1, 64]), keys.shape=torch.Size([1, 4, 14, 64])\n",
      "Scores computed: scores.shape=torch.Size([1, 4, 1, 14])\n",
      "Probs shape: torch.Size([1, 4, 1, 14])\n",
      "Attention output shape: torch.Size([1, 4, 1, 64])\n",
      "Layer 3 output shape: torch.Size([1, 1, 256])\n",
      "After norm: h.shape=torch.Size([1, 1, 256])\n",
      "Logits shape: torch.Size([1, 1, 127])\n",
      "Token 7: 16\n",
      "generated: [0, 9, 74, 16, 47, 48, 74, 16]\n",
      "Generating token 8: cur_pos=14\n",
      "Forward pass start: batch_size=1, seq_len=1, start_pos=14, use_cache=True\n",
      "After embeddings: h.shape=torch.Size([1, 1, 256])\n",
      "RoPE freqs_cis shape: torch.Size([1, 32])\n",
      "Layer 0 forward start\n",
      "Attention forward: bsz=1, seqlen=1\n",
      "QKV projections: xq.shape=torch.Size([1, 1, 256]), xk.shape=torch.Size([1, 1, 128]), xv.shape=torch.Size([1, 1, 128])\n",
      "After reshape: xq.shape=torch.Size([1, 1, 4, 64]), xk.shape=torch.Size([1, 1, 2, 64]), xv.shape=torch.Size([1, 1, 2, 64])\n",
      "RoPE applied\n",
      "KV cache updated: cache_k.shape=torch.Size([1, 15, 2, 64])\n",
      "After GQA repeat: keys.shape=torch.Size([1, 15, 4, 64]), values.shape=torch.Size([1, 15, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([1, 4, 1, 64]), keys.shape=torch.Size([1, 4, 15, 64])\n",
      "Scores computed: scores.shape=torch.Size([1, 4, 1, 15])\n",
      "Probs shape: torch.Size([1, 4, 1, 15])\n",
      "Attention output shape: torch.Size([1, 4, 1, 64])\n",
      "Layer 0 output shape: torch.Size([1, 1, 256])\n",
      "Layer 1 forward start\n",
      "Attention forward: bsz=1, seqlen=1\n",
      "QKV projections: xq.shape=torch.Size([1, 1, 256]), xk.shape=torch.Size([1, 1, 128]), xv.shape=torch.Size([1, 1, 128])\n",
      "After reshape: xq.shape=torch.Size([1, 1, 4, 64]), xk.shape=torch.Size([1, 1, 2, 64]), xv.shape=torch.Size([1, 1, 2, 64])\n",
      "RoPE applied\n",
      "KV cache updated: cache_k.shape=torch.Size([1, 15, 2, 64])\n",
      "After GQA repeat: keys.shape=torch.Size([1, 15, 4, 64]), values.shape=torch.Size([1, 15, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([1, 4, 1, 64]), keys.shape=torch.Size([1, 4, 15, 64])\n",
      "Scores computed: scores.shape=torch.Size([1, 4, 1, 15])\n",
      "Probs shape: torch.Size([1, 4, 1, 15])\n",
      "Attention output shape: torch.Size([1, 4, 1, 64])\n",
      "Layer 1 output shape: torch.Size([1, 1, 256])\n",
      "Layer 2 forward start\n",
      "Attention forward: bsz=1, seqlen=1\n",
      "QKV projections: xq.shape=torch.Size([1, 1, 256]), xk.shape=torch.Size([1, 1, 128]), xv.shape=torch.Size([1, 1, 128])\n",
      "After reshape: xq.shape=torch.Size([1, 1, 4, 64]), xk.shape=torch.Size([1, 1, 2, 64]), xv.shape=torch.Size([1, 1, 2, 64])\n",
      "RoPE applied\n",
      "KV cache updated: cache_k.shape=torch.Size([1, 15, 2, 64])\n",
      "After GQA repeat: keys.shape=torch.Size([1, 15, 4, 64]), values.shape=torch.Size([1, 15, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([1, 4, 1, 64]), keys.shape=torch.Size([1, 4, 15, 64])\n",
      "Scores computed: scores.shape=torch.Size([1, 4, 1, 15])\n",
      "Probs shape: torch.Size([1, 4, 1, 15])\n",
      "Attention output shape: torch.Size([1, 4, 1, 64])\n",
      "Layer 2 output shape: torch.Size([1, 1, 256])\n",
      "Layer 3 forward start\n",
      "Attention forward: bsz=1, seqlen=1\n",
      "QKV projections: xq.shape=torch.Size([1, 1, 256]), xk.shape=torch.Size([1, 1, 128]), xv.shape=torch.Size([1, 1, 128])\n",
      "After reshape: xq.shape=torch.Size([1, 1, 4, 64]), xk.shape=torch.Size([1, 1, 2, 64]), xv.shape=torch.Size([1, 1, 2, 64])\n",
      "RoPE applied\n",
      "KV cache updated: cache_k.shape=torch.Size([1, 15, 2, 64])\n",
      "After GQA repeat: keys.shape=torch.Size([1, 15, 4, 64]), values.shape=torch.Size([1, 15, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([1, 4, 1, 64]), keys.shape=torch.Size([1, 4, 15, 64])\n",
      "Scores computed: scores.shape=torch.Size([1, 4, 1, 15])\n",
      "Probs shape: torch.Size([1, 4, 1, 15])\n",
      "Attention output shape: torch.Size([1, 4, 1, 64])\n",
      "Layer 3 output shape: torch.Size([1, 1, 256])\n",
      "After norm: h.shape=torch.Size([1, 1, 256])\n",
      "Logits shape: torch.Size([1, 1, 127])\n",
      "Token 8: 2\n",
      "generated: [0, 9, 74, 16, 47, 48, 74, 16, 2]\n",
      "Generating token 9: cur_pos=15\n",
      "Forward pass start: batch_size=1, seq_len=1, start_pos=15, use_cache=True\n",
      "After embeddings: h.shape=torch.Size([1, 1, 256])\n",
      "RoPE freqs_cis shape: torch.Size([1, 32])\n",
      "Layer 0 forward start\n",
      "Attention forward: bsz=1, seqlen=1\n",
      "QKV projections: xq.shape=torch.Size([1, 1, 256]), xk.shape=torch.Size([1, 1, 128]), xv.shape=torch.Size([1, 1, 128])\n",
      "After reshape: xq.shape=torch.Size([1, 1, 4, 64]), xk.shape=torch.Size([1, 1, 2, 64]), xv.shape=torch.Size([1, 1, 2, 64])\n",
      "RoPE applied\n",
      "KV cache updated: cache_k.shape=torch.Size([1, 16, 2, 64])\n",
      "After GQA repeat: keys.shape=torch.Size([1, 16, 4, 64]), values.shape=torch.Size([1, 16, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([1, 4, 1, 64]), keys.shape=torch.Size([1, 4, 16, 64])\n",
      "Scores computed: scores.shape=torch.Size([1, 4, 1, 16])\n",
      "Probs shape: torch.Size([1, 4, 1, 16])\n",
      "Attention output shape: torch.Size([1, 4, 1, 64])\n",
      "Layer 0 output shape: torch.Size([1, 1, 256])\n",
      "Layer 1 forward start\n",
      "Attention forward: bsz=1, seqlen=1\n",
      "QKV projections: xq.shape=torch.Size([1, 1, 256]), xk.shape=torch.Size([1, 1, 128]), xv.shape=torch.Size([1, 1, 128])\n",
      "After reshape: xq.shape=torch.Size([1, 1, 4, 64]), xk.shape=torch.Size([1, 1, 2, 64]), xv.shape=torch.Size([1, 1, 2, 64])\n",
      "RoPE applied\n",
      "KV cache updated: cache_k.shape=torch.Size([1, 16, 2, 64])\n",
      "After GQA repeat: keys.shape=torch.Size([1, 16, 4, 64]), values.shape=torch.Size([1, 16, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([1, 4, 1, 64]), keys.shape=torch.Size([1, 4, 16, 64])\n",
      "Scores computed: scores.shape=torch.Size([1, 4, 1, 16])\n",
      "Probs shape: torch.Size([1, 4, 1, 16])\n",
      "Attention output shape: torch.Size([1, 4, 1, 64])\n",
      "Layer 1 output shape: torch.Size([1, 1, 256])\n",
      "Layer 2 forward start\n",
      "Attention forward: bsz=1, seqlen=1\n",
      "QKV projections: xq.shape=torch.Size([1, 1, 256]), xk.shape=torch.Size([1, 1, 128]), xv.shape=torch.Size([1, 1, 128])\n",
      "After reshape: xq.shape=torch.Size([1, 1, 4, 64]), xk.shape=torch.Size([1, 1, 2, 64]), xv.shape=torch.Size([1, 1, 2, 64])\n",
      "RoPE applied\n",
      "KV cache updated: cache_k.shape=torch.Size([1, 16, 2, 64])\n",
      "After GQA repeat: keys.shape=torch.Size([1, 16, 4, 64]), values.shape=torch.Size([1, 16, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([1, 4, 1, 64]), keys.shape=torch.Size([1, 4, 16, 64])\n",
      "Scores computed: scores.shape=torch.Size([1, 4, 1, 16])\n",
      "Probs shape: torch.Size([1, 4, 1, 16])\n",
      "Attention output shape: torch.Size([1, 4, 1, 64])\n",
      "Layer 2 output shape: torch.Size([1, 1, 256])\n",
      "Layer 3 forward start\n",
      "Attention forward: bsz=1, seqlen=1\n",
      "QKV projections: xq.shape=torch.Size([1, 1, 256]), xk.shape=torch.Size([1, 1, 128]), xv.shape=torch.Size([1, 1, 128])\n",
      "After reshape: xq.shape=torch.Size([1, 1, 4, 64]), xk.shape=torch.Size([1, 1, 2, 64]), xv.shape=torch.Size([1, 1, 2, 64])\n",
      "RoPE applied\n",
      "KV cache updated: cache_k.shape=torch.Size([1, 16, 2, 64])\n",
      "After GQA repeat: keys.shape=torch.Size([1, 16, 4, 64]), values.shape=torch.Size([1, 16, 4, 64])\n",
      "After transpose: xq.shape=torch.Size([1, 4, 1, 64]), keys.shape=torch.Size([1, 4, 16, 64])\n",
      "Scores computed: scores.shape=torch.Size([1, 4, 1, 16])\n",
      "Probs shape: torch.Size([1, 4, 1, 16])\n",
      "Attention output shape: torch.Size([1, 4, 1, 64])\n",
      "Layer 3 output shape: torch.Size([1, 1, 256])\n",
      "After norm: h.shape=torch.Size([1, 1, 256])\n",
      "Logits shape: torch.Size([1, 1, 127])\n",
      "Token 9: 4\n",
      "generated: [0, 9, 74, 16, 47, 48, 74, 16, 2, 4]\n",
      "Generated text: : self ' like transformers self ' to\n",
      "\n",
      "Final generated tokens: [0, 9, 74, 16, 47, 48, 74, 16, 2, 4]\n",
      "Final generated text: : self ' like transformers self ' to\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TESTING THE GENERATE FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "# Example prompt for testing (a string input)\n",
    "test_prompt = \"Hello, how are you?\"\n",
    "\n",
    "# Call the generate function (after training the model)\n",
    "generated_tokens, generated_text = generate(model, test_prompt, max_new_tokens=10)\n",
    "\n",
    "# Print the results\n",
    "print(f\"\\nFinal generated tokens: {generated_tokens}\")\n",
    "print(f\"Final generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c7fe2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- KV Cache Statistics ---\n",
      "Layer 0:\n",
      "  cache_k shape: torch.Size([1, 16, 2, 64])\n",
      "  cache_v shape: torch.Size([1, 16, 2, 64])\n",
      "  cache_k memory: 8.00 KB\n",
      "Layer 1:\n",
      "  cache_k shape: torch.Size([1, 16, 2, 64])\n",
      "  cache_v shape: torch.Size([1, 16, 2, 64])\n",
      "  cache_k memory: 8.00 KB\n",
      "Layer 2:\n",
      "  cache_k shape: torch.Size([1, 16, 2, 64])\n",
      "  cache_v shape: torch.Size([1, 16, 2, 64])\n",
      "  cache_k memory: 8.00 KB\n",
      "Layer 3:\n",
      "  cache_k shape: torch.Size([1, 16, 2, 64])\n",
      "  cache_v shape: torch.Size([1, 16, 2, 64])\n",
      "  cache_k memory: 8.00 KB\n",
      "--- End KV Cache Stats ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.get_kv_cache_stats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
