{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "933b95b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3736a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: CONFIGURATION - Define all hyperparameters\n",
    "# ============================================================================\n",
    "\n",
    "class BertConfig:\n",
    "    \"\"\"\n",
    "    Configuration classs to store BERT hyperparameters.\n",
    "    These values are based on BERT-Base architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 30522                     # Size of vocabulary (WordPiece tokens)\n",
    "        self.max_position_embeddings = 512          # Maximum sequence length\n",
    "        self.hidden_size = 768                      # Dimension of embeddings (d_model)\n",
    "        self.num_hidden_layers = 12                 # Number of transformer encoder layers\n",
    "        self.num_attention_heads = 12               # Number of attention heads\n",
    "        self.intermediate_size = 3072               # Dimension of feed-forward layer (4 * hidden_size)\n",
    "        self.hidden_dropout_prob = 0.1              # Dropout probability\n",
    "        self.attention_probs_dropout_prob = 0.1     # Attention dropout\n",
    "        self.type_vocab_size = 2                    # Number of segment types (Sentence A/B)\n",
    "        self.layer_norm_eps = 1e-12                 # Layer normalization epsilon\n",
    "    \n",
    "config = BertConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82ffe7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: EMBEDDING LAYER - Convert tokens to embeddings\n",
    "# ============================================================================\n",
    "\n",
    "class BertEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT has 3 types of embeddings that are summed together:\n",
    "    1. Token Embeddings - represents the actual word/token.\n",
    "    2. Position Embeddings - represents position in sequence.\n",
    "    3. Segment Embeddings - differentiates between sentence A and B\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertEmbedding, self).__init__()\n",
    "\n",
    "        # Token Embedding: Maps token IDs to dense vectors\n",
    "        # Input: (batch_size, seq_len) with token IDs\n",
    "        # Output: (batch_size, seq_len, hidden_size)\n",
    "        self.word_embeddings = nn.Embedding(\n",
    "            num_embeddings=config.vocab_size,\n",
    "            embedding_dim=config.hidden_size,\n",
    "            padding_idx=0\n",
    "        )\n",
    "\n",
    "        # Position Embedding: Learned position encodings\n",
    "        # Input: (batch_size, seq_len) with position IDs [0, 1, 2,......, seq_len-1]\n",
    "        # Output: (batch_size, seq_len, hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(\n",
    "            num_embeddings=config.max_position_embeddings,\n",
    "            embedding_dim=config.hidden_size\n",
    "        )\n",
    "\n",
    "        # Segment/Token Type Embedding: Differentiates sentence A from B\n",
    "        # Input: (batch_size, seq_len) with segment IDs (0 or 1)\n",
    "        # Output: (batch_size, seq_len, hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(\n",
    "            num_embeddings=config.vocab_size,\n",
    "            embedding_dim=config.hidden_size\n",
    "        )\n",
    "\n",
    "        # Layer Normalization: Normalizes the summed embeddings\n",
    "        self.layer_norm = nn.LayerNorm(\n",
    "            normalized_shape=config.hidden_size,\n",
    "            eps=config.layer_norm_eps\n",
    "        )\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(p=config.hidden_dropout_prob)\n",
    "\n",
    "        # Register buffer for positions_ids to avoid recreating it every forward pass\n",
    "        # Shape: (1, max_positions_embeddings)\n",
    "        self.register_buffer(\n",
    "            \"position_ids\",\n",
    "            torch.arange(config.max_position_embeddings).expand((1, -1))\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids: Token IDs, shape: (batch_size, seq_len)\n",
    "            token_type_ids: Segment IDs, shape: (batch_size, seq_len), Optional\n",
    "        \n",
    "        Returns:\n",
    "            embeddings: Combined embeddings, shape: (batch_size, seq_len, hidden_size)\n",
    "        \"\"\"\n",
    "        # input_ids shape: (batch_size, seq_len)\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "\n",
    "        # Step 2.1: Get position IDs for the sequence\n",
    "        # Shape: (1, seq_len) -> will broadcast to (batch_size, seq_len)\n",
    "        position_ids = self.position_ids[:, :seq_len]\n",
    "\n",
    "        # Step 2.2: If token_type_ids not provided, assume all tokens are from sentence A.\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids == torch.zeros_like(input_ids) # Shape: (batch_size, seq_len)\n",
    "        \n",
    "        # Step 2.3: Get token embeddings\n",
    "        # Input shape: (batch_size, seq_len)\n",
    "        # Output shape: (batch_size, seq_len, hidden_size=768)\n",
    "        word_embeddings = self.word_embeddings(input_ids)\n",
    "\n",
    "        # Step 2.4: Get position embeddings\n",
    "        # Input shape: (1, seq_len) \n",
    "        # Output shape: (1, seq_len, hidden_size=768) -> broadcasts to (batch_size, seq_len, 768)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "\n",
    "        # Step 2.5: Get Segment/Token type embeddings\n",
    "        # Input shape: (batch_size, seq_len)\n",
    "        # Output shape: (batch_size, seq_len, hidden_size=768)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        # Step 2.7: Sum all three embeddings\n",
    "        # All inputs: (batch_size, seq_len, hidden_size=768)\n",
    "        # All outputs: (batch_size, seq_len, hidden_size=768)\n",
    "        embeddings = word_embeddings + position_embeddings + token_type_embeddings\n",
    "\n",
    "        # Step 2.7: Apply Layer Normalization\n",
    "        # Input shape: (batch_size, seq_len, hidden_size=768)\n",
    "        # Output shape: (batch_size, seq_len, hidden_size=768)\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "\n",
    "        # Step 2.8: Apply dropout\n",
    "        # Input shape: (batch_size, seq_len, hidden_size=768)\n",
    "        # Output shape: (batch_size, seq_len, hidden_size=768)\n",
    "        return self.dropout(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1aa3715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: MULTI-HEAD SELF-ATTENTION - Core attention mechanism\n",
    "# ============================================================================\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head self-attention mechanism. \n",
    "    \n",
    "    The attention mechanism allows the model to focus on different parts of the input.\n",
    "    Multi-head attention runs multiple attention mechanisms in parellel\n",
    "\n",
    "    Formula: Attention(Q, K, V) = softmax(Q*K^T / sqrt(d_k)) * V\n",
    "    \"\"\"\n",
    "    def __init__(self, config: BertConfig):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "\n",
    "        assert config.hidden_size % config.num_attention_heads == 0, \"hidden_size must be divisible by num_attention_heads\"\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = config.hidden_size // config.num_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        # Linear transformation for Query, Key and Value\n",
    "        # Each transforms (batch_size, seq_len, hidden_size) -> (batch_size, seq_len, hidden_size)\n",
    "        self.query = nn.Linear(in_features=config.hidden_size, out_features=self.all_head_size)\n",
    "        self.key = nn.Linear(in_features=config.hidden_size, out_features=self.all_head_size)\n",
    "        self.value = nn.Linear(in_features=config.hidden_size, out_features=self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=config.attention_probs_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        \"\"\"\n",
    "        Reshape tensor for multi-head attention computation.\n",
    "\n",
    "        Args:\n",
    "            x: shape (batch_size, seq_len, hidden_size=768)\n",
    "\n",
    "        Returns:\n",
    "            reshaped tensor: (batch_size, num_heads=12, seq_len, head_size=64)\n",
    "        \"\"\"\n",
    "        # Input shape: (batch_size, seq_len, hidden_size=768)\n",
    "        batch_size, seq_len, hidden_size = x.size()\n",
    "\n",
    "        # Step 3.1: Reshape to separate heads\n",
    "        # (batch_size, seq_len, 768) -> (batch_size, seq_len, 12, 64)\n",
    "        x = x.view(batch_size, seq_len, self.num_attention_heads, self.attention_head_size)\n",
    "\n",
    "        # Step 3.2: Transpose to bring heads dimension forward\n",
    "        # (batch_size, seq_len, 12, 64) -> (batch_size, 12, seq_len, 64)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "    \n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_state: shape (batch_size, seq_len, hidden_size=768)\n",
    "            attention_mask: shape (batch_size, 1, 1, seq_len) - mask padded tokens\n",
    "        \n",
    "        Returns:\n",
    "            context_layer: shape (batch_size, seq_len, hidden_size=768)\n",
    "        \"\"\"\n",
    "        # Input shape: (batch_size, seq_len, hidden_size)\n",
    "        batch_size, seq_len, hidden_size = hidden_states.size()\n",
    "\n",
    "        # Step 3.3: Apply linear transformation to get Q, K and V\n",
    "        # Each output shape: (batch_size, seq_len, hidden_size)\n",
    "        query_layer = self.query(hidden_states)     # (batch_size, seq_len, hidden_size)\n",
    "        key_layer = self.key(hidden_states)         # (batch_size, seq_len, hidden_size)\n",
    "        value_layer = self.value(hidden_states)     # (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        # Step 3.4: Reshape Q, K and V for multi-head attention\n",
    "        # Transform to (batch_size, num_heads, seq_len, attention_head_size)\n",
    "        query_layer = self.transpose_for_scores(query_layer)    # (batch_size, num_heads, seq_len, attention_hidden_size)\n",
    "        key_layer = self.transpose_for_scores(key_layer)        # (batch_size, num_heads, seq_len, attention_hidden_size)\n",
    "        value_layer = self.transpose_for_scores(value_layer)    # (batch_size, num_heads, seq_len, attention_hidden_size)\n",
    "\n",
    "        # Step 3.5: Calculate attention scores - Q*K^T\n",
    "        # query: (batch_size, num_heads, seq_len, attention_hidden_size)\n",
    "        # key transposed: (batch_size, num_heads, attention_hidden_size, seq_len)\n",
    "        # Output: (batch_size, num_heads, seq_len, seq_len)\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "\n",
    "        # Step 3.6: Scale by sqrt of head dimension\n",
    "        # This prevents dot product from growing too large\n",
    "        # Input/Output shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "\n",
    "        # Step 3.7: Apply attention mask (if provided)\n",
    "        # Mask padded tokens by setting there scores to very -ve value\n",
    "        if attention_mask is not None:\n",
    "            # attention_mask shape: (batch_size, 1, 1, seq_len)\n",
    "            # attention_scores shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Step 3.8: Apply softmax to get attention probabilities\n",
    "        # Input: (batch_size, num_heads, seq_len, seq_len)\n",
    "        # Output: (batch_size, num_heads, seq_len, seq_len) - probability sum to 1 over last dimension\n",
    "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "\n",
    "        # Step 3.9: Apply dropout to attention probabilities\n",
    "        # Shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # Step 3.10: Multiply attention probabilities with V to get context\n",
    "        # attention_probs: (batch_size, num_heads, seq_len, seq_len)\n",
    "        # value_layer: (batch_size, num_heads, seq_len, attention_head_size)\n",
    "        # Output: (batch_size, num_heads, seq_len, attention_head_size)\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        # Step 3.11: Transpose back to original format\n",
    "        # (batch_size, num_heads, seq_len, attention_head_size) -> (batch_size, seq_len, num_heads, attention_head_size)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "        # Step 3.12: Reshape to combine all heads\n",
    "        # (batch_size, seq_len, num_heads, attention_head_size) -> (batch_size, seq_len, hidden_size)\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size, )\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        return context_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3dfb80f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: SELF-ATTENTION OUTPUT - Project attention output back\n",
    "# ============================================================================\n",
    "\n",
    "class SelfAttentionOutput(nn.Module):\n",
    "    \"\"\"\n",
    "    Projects the attention output and applies residual connection + layer norm.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: BertConfig):\n",
    "        super(SelfAttentionOutput, self).__init__()\n",
    "\n",
    "        # Linear layer to project attention output\n",
    "        # Input/output: (batch_size, seq_len, hidden_size=768)\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states: Attention output, shape (batch_size, seq_len, hidden_size=768)\n",
    "            input_tensor: Original input to atttention, shape (batch_size, seq_len, hidden_size=768)\n",
    "        \n",
    "        Returns:\n",
    "            output: shape (batch_size, seq_len, hidden_size=768)\n",
    "        \"\"\"\n",
    "        # Step 4.1: Apply linear transformation\n",
    "        # Input: (batch_size, seq_len, hidden_size)\n",
    "        # Output: (batch_size, seq_len, hidden_size)\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "\n",
    "        # Step 4.2: Apply dropout\n",
    "        # shape: (batch_size, seq_len, hidden_size)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "\n",
    "        # Step 4.3: Add residual connection and apply layer norm\n",
    "        # Both inputs: (batch_size, seq_len, hidden_size)\n",
    "        # Output: (batch_size, seq_len, hidden_size)\n",
    "        return self.LayerNorm(hidden_states + input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a8b8bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 5: COMPLETE ATTENTION BLOCK - Combine attention + output projection\n",
    "# ============================================================================\n",
    "\n",
    "class BertAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete attention block combining multi-head attention and output projection\n",
    "    \"\"\"\n",
    "    def __init__(self, config: BertConfig):\n",
    "        super(BertAttention, self).__init__()\n",
    "        self.self_attention = MultiHeadSelfAttention(config=config)\n",
    "        self.output = SelfAttentionOutput(config=config)\n",
    "        \n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states: shape (batch_size, seq_len, hidden_size)\n",
    "            attention_mask: shape (batch_size, 1, 1, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            attention_output: shape (batch_size, seq_len, hidden_size)\n",
    "        \"\"\"\n",
    "        # Step 5.1: Apply multi-head self attention\n",
    "        # Inputs: (batch_size, seq_len, hidden_size), (batch_size, 1, 1, seq_len)\n",
    "        # Output: (batch_size, seq_len, hidden_size)\n",
    "        self_attention_output = self.self_attention(hidden_states, attention_mask)\n",
    "\n",
    "        # Step 5.2: Apply output projection with residual connection\n",
    "        # Inputs: (batch_size, seq_len, hidden_size), (batch_size, seq_len, hidden_size)\n",
    "        # Output: (batch_size, seq_len, hidden_size)\n",
    "        attention_output = self.output(self_attention_output, hidden_states)\n",
    "        return attention_output # Shape: (batch_size, seq_len, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9c5aaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 6: FEED-FORWARD NETWORK - Position-wise FFN after attention\n",
    "# ============================================================================\n",
    "\n",
    "class BertIntermediate(nn.Module):\n",
    "    \"\"\"\n",
    "    First part of feed-forward neural network - expands hidden_size to intermediate_size\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(BertIntermediate, self).__init__()\n",
    "\n",
    "        # Expands from hidden_size (768) to intermediate_size (3072)\n",
    "        # Input: (batch_size, seq_len, hidden_size)\n",
    "        # Output: (batch_size, seq_len, intermediate_size)\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "\n",
    "        # GELU activation function (Gaussian Error Linear Unit)\n",
    "        # Smoother than ReLU, used in original BERT\n",
    "        self.intermediate_act_fn = nn.GELU()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states: shape (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        Returns:\n",
    "            output: shape (batch_size, seq_len, intermediate_size)\n",
    "        \"\"\"\n",
    "        # Step 6.1: Apply linear transformation (expand dimension)\n",
    "        # Input: (batch_size, seq_len, hidden_size)\n",
    "        # Output: (batch_size, seq_len, intermediate_size)\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "\n",
    "        # Step 6.2: Apply GELU activation\n",
    "        # Input/output: (batch_size, seq_len, intermediate_size)\n",
    "        return self.intermediate_act_fn(hidden_states)\n",
    "    \n",
    "\n",
    "class BertOutput(nn.Module):\n",
    "    \"\"\"\n",
    "    Second part of feed-forward network - projects back to hidden_size\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(BertOutput, self).__init__()\n",
    "        # Project from intermediate_size (3072) to hidden_size (768)\n",
    "        # Input: (batch_size, seq_len, intermediate_size)\n",
    "        # Output: (batch_size, seq_len, hidden_size)\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states: FFN intermediate output, shape (batch_size, seq_len, intermediate_size)\n",
    "            input_tensor: Input to FFN block, shape (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            output: shape (batch_size, seq_len, hidden_size)\n",
    "        \"\"\"\n",
    "        # Step 6.3: Project back to hidden_size\n",
    "        # Input: (batch_size, seq_len, intermediate_size)\n",
    "        # Output: (batch_size, seq_len, hidden_size)\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "\n",
    "        # Step 6.4: Apply dropout\n",
    "        # Shape: (batch_size, seq_len, hidden_size)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "\n",
    "        # Step 6.5: Add residual connection adn layer norm\n",
    "        # Both inputs: (batch_size, seq_len, hidden_size)\n",
    "        # Output: (batch_size, seq_len, hidden_size)\n",
    "        return self.LayerNorm(hidden_states + input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebad0c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 7: ENCODER LAYER - Single transformer encoder block\n",
    "# ============================================================================\n",
    "\n",
    "class BertLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Single BERT encoder layer consisting of:\n",
    "    1. Multi-head self-attention\n",
    "    2. Feed-forward network\n",
    "    \n",
    "    Each sub-layer has residual connection + layer normalization\n",
    "    \"\"\"\n",
    "    def __init__(self, config: BertConfig):\n",
    "        super(BertLayer, self).__init__()\n",
    "        self.attention = BertAttention(config=config)\n",
    "        self.intermediate = BertIntermediate(config=config)\n",
    "        self.output = BertOutput(config=config)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states: shape (batch_size, seq_len, hidden_size)\n",
    "            attention_mask: shape (batch_size, 1, 1, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            layer_output: shape (batch_size, seq_len, hidden_size)\n",
    "        \"\"\"\n",
    "        # step 7.1: Apply attention block\n",
    "        # Input: (batch_size, seq_len, hidden_size)\n",
    "        # Output: (batch_size, seq_len, hidden_size)\n",
    "        attention_output = self.attention(hidden_states, attention_mask)\n",
    "\n",
    "        # Step 7.2: Apply feed-forward network - intermediate expansion\n",
    "        # Input: (batch_size, seq_len, hidden_size)\n",
    "        # Output: (batch_size, seq_len, intermediate_size)\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "\n",
    "        # Step 7.3: Apply feed-forward network - project back with residual\n",
    "        # Inputs: (batch_size, seq_len, intermediate_size), (batch_size, seq_len, hidden_size)\n",
    "        # Output: (batch_size, seq_len, hidden_size)\n",
    "        return self.output(intermediate_output, attention_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "176c3ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 8: ENCODER - Stack of encoder layers\n",
    "# ============================================================================\n",
    "\n",
    "class BertEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT Encoder consisting of multiple stacked encoder layers.\n",
    "    For BERT-base: 12 layers\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(BertEncoder, self).__init__()\n",
    "\n",
    "        # Create a list of 12 encoder layers\n",
    "        self.layer = nn.ModuleList([\n",
    "            BertLayer(config=config) for _ in range(config.num_hidden_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states: Embeddings, shape (batch_size, seq_len, hidden_size)\n",
    "            attention_mask: shape (batch_size, 1, 1, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            hidden_states: Final encoder output, shape (batch_size, seq_len, hidden_size)\n",
    "        \"\"\"\n",
    "        # Step 8.1: Pass through each encoder layer sequrentially\n",
    "        # Each layer : Input (batch_size, seq_len, hidden_size) -> Output (batch_size, seq_len, hidden_size)\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            # Pass through layer i (i=0 to 11 for BERT-base)\n",
    "            hidden_states = layer_module(hidden_states, attention_mask)\n",
    "            # After each layer shape remains (batch_size, seq_len, hidden_size)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64f0e824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 9: POOLER - Extract [CLS] token representation\n",
    "# ============================================================================\n",
    "\n",
    "class BertPooler(nn.Module):\n",
    "    \"\"\"\n",
    "    Pools the output by taking the hidden state of [CLS] token (first_token).\n",
    "    Used for classification tasks\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(BertPooler, self).__init__()\n",
    "\n",
    "        # Linear layer + tanh activation\n",
    "        # Input/Output: (batch_size, hidden_size)\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states: Encoder output, shape (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            pooled_output: [CLS] representation, shape (batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        # Step 9.1: Extract first token ([CLS]) hidden state.\n",
    "        # Input: (batch_size, seq_len, hidden_size)\n",
    "        # Output: (batch_size, hidden_size)\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        \n",
    "        # Step 9.2: Apply linear transformation\n",
    "        # Input: (batch_size, hidden_size)\n",
    "        # Output: (batch_size, hidden_size)\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "\n",
    "        # Step 9.3: Apply tanh activation\n",
    "        # Input/Output: (batch_size, hidden_size)\n",
    "        return self.activation(pooled_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e257a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 10: COMPLETE BERT MODEL - Putting it all together\n",
    "# ============================================================================\n",
    "\n",
    "class BertModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete BERT model combining all components\n",
    "    1. Embeddings (token + position + segment)\n",
    "    2. Encoder (12 transformer layers)\n",
    "    3. Pooler (for [CLS] token)\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(BertModel, self).__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Initialize all components\n",
    "        self.embeddings = BertEmbedding(config=config)\n",
    "        self.encoder = BertEncoder(config=config)\n",
    "        self.pooler = BertPooler(config=config)\n",
    "\n",
    "    def get_extended_attention_mask(self, attention_mask):\n",
    "        \"\"\"\n",
    "        Creates extended attention mask for multi-head attention.\n",
    "        Converts 1s (attend) and 0s (don't attend) to 0s and -10000s.\n",
    "\n",
    "        Args:\n",
    "            attention_mask: shape (batch_size, seq_len) with 1s and 0s.\n",
    "\n",
    "        Returns:\n",
    "            extended_attention_mask: shape (batch_size, 1, 1, seq_len)\n",
    "        \"\"\"\n",
    "        # Step 10.1: Add dimensions for broadcasting\n",
    "        # Input: (batch_size, seq_len)\n",
    "        # Output: (batch_size, 1, 1, seq_len)\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # Steop 10.2: Convert to float and create mask values\n",
    "        # 1 -> 0.0 (attend), 0 -> -10000.0 (don't attend)\n",
    "        # Shape: (batch_size, 1, 1, seq_len)\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=torch.float32)\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "        return extended_attention_mask\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Forward pass through BERT model.\n",
    "\n",
    "        Args:\n",
    "            input_ids: Token IDs shape (batch_size, seq_len)\n",
    "            attention_mask: Mask for padding, shape (batch_size, seq_len), Optional\n",
    "            token_type_ids: Segment IDs shape (batch_size, seq_len) Optional\n",
    "\n",
    "        Returns:\n",
    "            sequence_ouptput: All token respresentations, shape (batch_size, seq_len, hidden_size)\n",
    "            pooled_output: [CLS] token representation, shape (batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        # Step 10.3: Create attention mask if not provided\n",
    "        if attention_mask is None:\n",
    "            # Default: attend to all tokens\n",
    "            # Shape: (batch_size, seq_len)\n",
    "            attention_mask = torch.ones_like(input=input_ids)\n",
    "        \n",
    "        # Step 10.4: Extend attention mask for mult-head attention\n",
    "        # Input: (batch_size, seq_len)\n",
    "        # Output: (batch_size, 1, 1, seq_len)\n",
    "        extended_attention_mask = self.get_extended_attention_mask(attention_mask=attention_mask)\n",
    "\n",
    "        # Step 10.5: Get embeddings\n",
    "        # Input: input_ids (batch_size, seq_len), token_type_ids (batch_size, seq_len)\n",
    "        # Output: (batch_size, seq_len, hidden_size)\n",
    "        embedding_output = self.embeddings(input_ids, token_type_ids)\n",
    "\n",
    "        # Step 10.6: Pass through encoder layers\n",
    "        # Input: (batch_size, seq_len, hidden_size), attention_mask (batch_size, 1, 1, seq_len)\n",
    "        # Output: (batch_size, seq_len, hidden_size)\n",
    "        encoder_output = self.encoder(embedding_output, extended_attention_mask)\n",
    "\n",
    "        # Step 10.7: Pool [CLS] token representation\n",
    "        # Input: (batch_size, seq_len, hidden_size)\n",
    "        # Output: (batch_size, hidden_size)\n",
    "        pooled_output = self.pooler(encoder_output)\n",
    "\n",
    "        # encoder_output: (batch_size, seq_len, hidden_size) - all token representations\n",
    "        # pooled_output: (batch_size, hidden_size) - [CLS] token for classification\n",
    "        return encoder_output, pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "abdc86d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 11: BERT FOR MASKED LANGUAGE MODELING (Pre-training Task)\n",
    "# ============================================================================\n",
    "\n",
    "class BertForMaskedLM(nn.Module):\n",
    "    \"\"\"\n",
    "    Bert with masked language modelling head.\n",
    "    predicts masked tokens in the input sequence.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: BertConfig):\n",
    "        super(BertForMaskedLM, self).__init__()\n",
    "        \n",
    "        self.bert = BertModel(config=config)\n",
    "\n",
    "        # MLM prediction head\n",
    "        self.mlm_dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.mlm_activation = nn.GELU()\n",
    "        self.mlm_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "        # Final layer to predict vocabulary\n",
    "        # Input: (batch_size, seq_len, hidden_size)\n",
    "        # Output: (batch_size, seq_len, vocab_size)\n",
    "        self.mlm_classifier = nn.Linear(config.hidden_size, config.vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids: shape (batch_size, seq_len)\n",
    "            attention_mask: shape (batch_size, seq_len)\n",
    "            token_type_ids: shape (batch_size, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            prediction_scores: shape (batch_size, seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        # Step 11.1: Get BERT outputs\n",
    "        # sequence_output: (batch_size, seq_len, hidden_size)\n",
    "        # pooled_output: (batch_size, hidden_size)\n",
    "        sequence_output, pooled_output = self.bert(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "        # Step 11.2: Apply MLM head transformation\n",
    "        # Input: (batch_size, seq_len, hidden_size)\n",
    "        # Output: (batch_size, seq_len, hidden_size)\n",
    "        hidden_states = self.mlm_dense(sequence_output)\n",
    "\n",
    "        # Step 11.3: Apply GELU activation function\n",
    "        # Input: (batch_size, seq_len, hidden_size)\n",
    "        hidden_states = self.mlm_activation(hidden_states)\n",
    "\n",
    "        # Step 11.4: Apply layer normalization\n",
    "        # Input: (batch_size, seq_len, hidden_size)\n",
    "        hidden_states = self.mlm_layer_norm(hidden_states)\n",
    "\n",
    "        # Step 11.5: Project to vocabulary size\n",
    "        # Input: (batch_size, seq_len, hidden_size)\n",
    "        # Output: (batch_size, seq_len, vocab_size=30522)\n",
    "        prediction_scores = self.mlm_classifier(hidden_states)\n",
    "        return prediction_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "21a054d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 12: BERT FOR SEQUENCE CLASSIFICATION (Fine-tuning Task)\n",
    "# ============================================================================\n",
    "\n",
    "class BertForSequenceClassification(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT for classification tasks (e.g., sentiment analysis).\n",
    "    Uses [CLS] token representation for classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: BertConfig, num_labels):\n",
    "        super(BertForSequenceClassification, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel(config=config)\n",
    "\n",
    "        # Dropout for regularizaton\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "        # Classification head\n",
    "        # Input: (batch_size, hidden_size)\n",
    "        # Output: (batch_size, num_labels)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids: shape (batch_size, seq_len)\n",
    "            attention_mask: shape (batch_size, seq_len)\n",
    "            token_type_ids: shape (batch_size, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            logit: shape (batch-size, num_labels)\n",
    "        \"\"\"\n",
    "        # Step 12.1: Get BERT outputs\n",
    "        # sequence_output: (batch_size, seq_len, hidden_size)\n",
    "        # pooled_output: (batch_size, hidden_size) - [CLS] representation.\n",
    "        sequence_output, pooled_output = self.bert(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "        # Step 12.2: Apply dropout to [CLS] representation\n",
    "        # input/Output:  (batch_size, hidden_size)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "        # Step 12.3: Apply classification layer\n",
    "        # Input: (batch_size, hidden_size)\n",
    "        # Output: (batch_size, num_labels)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c60faf56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BERT MODEL FROM SCRATCH - EXAMPLE USAGE\n",
      "================================================================================\n",
      "\n",
      "1. BASIC BERT MODEL\n",
      "--------------------------------------------------------------------------------\n",
      "Input IDs shape: torch.Size([2, 128])\n",
      "Attention mask shape: torch.Size([2, 128])\n",
      "Token type IDs shape: torch.Size([2, 128])\n",
      "\n",
      "Sequence output shape: torch.Size([2, 128, 768])\n",
      "Pooled output shape: torch.Size([2, 768])\n",
      "\n",
      "2. BERT FOR MASKED LANGUAGE MODELING\n",
      "--------------------------------------------------------------------------------\n",
      "MLM prediction scores shape: torch.Size([2, 128, 30522])\n",
      "Predicted token ID at position 0: 14106\n",
      "\n",
      "3. BERT FOR SEQUENCE CLASSIFICATION (e.g., Sentiment Analysis)\n",
      "--------------------------------------------------------------------------------\n",
      "Classification logits shape: torch.Size([2, 2])\n",
      "Predictions: tensor([0, 0])\n",
      "Probabilities shape: torch.Size([2, 2])\n",
      "Sample probabilities: tensor([0.6052, 0.3948], grad_fn=<SelectBackward0>)\n",
      "\n",
      "4. MODEL STATISTICS\n",
      "--------------------------------------------------------------------------------\n",
      "Total parameters: 132,921,600\n",
      "Trainable parameters: 132,921,600\n",
      "\n",
      "================================================================================\n",
      "COMPLETE! All components working correctly.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 13: EXAMPLE USAGE\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*80)\n",
    "    print(\"BERT MODEL FROM SCRATCH - EXAMPLE USAGE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create configuration\n",
    "    config = BertConfig()\n",
    "\n",
    "    # Example 1: Basic BERT Model\n",
    "    print(\"\\n1. BASIC BERT MODEL\")\n",
    "    print(\"-\" * 80)\n",
    "    model = BertModel(config)\n",
    "\n",
    "    # Create dummy input\n",
    "    batch_size = 2\n",
    "    seq_len = 128\n",
    "\n",
    "    # Input IDs: (batch_size, seq_len) - random token IDs\n",
    "    input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))\n",
    "    print(f\"Input IDs shape: {input_ids.shape}\")  # (2, 128)\n",
    "\n",
    "    # Attention mask: (batch_size, seq_len) - 1 for real tokens, 0 for padding\n",
    "    attention_mask = torch.ones(batch_size, seq_len)\n",
    "    print(f\"Attention mask shape: {attention_mask.shape}\")  # (2, 128)\n",
    "\n",
    "    # Token type IDs: (batch_size, seq_len) - 0 for sentence A, 1 for sentence B\n",
    "    token_type_ids = torch.zeros(batch_size, seq_len, dtype=torch.long)\n",
    "    token_type_ids[:, seq_len//2:] = 1  # Second half is sentence B\n",
    "    print(f\"Token type IDs shape: {token_type_ids.shape}\")  # (2, 128)\n",
    "\n",
    "    # Forward pass\n",
    "    sequence_output, pooled_output = model(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "    print(f\"\\nSequence output shape: {sequence_output.shape}\")  # (2, 128, 768)\n",
    "    print(f\"Pooled output shape: {pooled_output.shape}\")  # (2, 768)\n",
    "\n",
    "    # Example 2: BERT for Masked Language Modeling\n",
    "    print(\"\\n2. BERT FOR MASKED LANGUAGE MODELING\")\n",
    "    print(\"-\" * 80)\n",
    "    mlm_model = BertForMaskedLM(config)\n",
    "\n",
    "    prediction_scores = mlm_model(input_ids, attention_mask, token_type_ids)\n",
    "    print(f\"MLM prediction scores shape: {prediction_scores.shape}\")  # (2, 128, 30522)\n",
    "\n",
    "    # Get predicted token for first masked position\n",
    "    predicted_token_id = torch.argmax(prediction_scores[0, 0, :])\n",
    "    print(f\"Predicted token ID at position 0: {predicted_token_id.item()}\")\n",
    "\n",
    "    # Example 3: BERT for Sequence Classification\n",
    "    print(\"\\n3. BERT FOR SEQUENCE CLASSIFICATION (e.g., Sentiment Analysis)\")\n",
    "    print(\"-\" * 80)\n",
    "    num_labels = 2  # Binary classification (positive/negative)\n",
    "    classification_model = BertForSequenceClassification(config, num_labels)\n",
    "\n",
    "    logits = classification_model(input_ids, attention_mask, token_type_ids)\n",
    "    print(f\"Classification logits shape: {logits.shape}\")  # (2, 2)\n",
    "\n",
    "    # Get predictions\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    print(f\"Predictions: {predictions}\")  # Tensor of class indices\n",
    "\n",
    "    # Calculate probabilities\n",
    "    probabilities = F.softmax(logits, dim=-1)\n",
    "    print(f\"Probabilities shape: {probabilities.shape}\")  # (2, 2)\n",
    "    print(f\"Sample probabilities: {probabilities[0]}\")\n",
    "\n",
    "    # Model statistics\n",
    "    print(\"\\n4. MODEL STATISTICS\")\n",
    "    print(\"-\" * 80)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPLETE! All components working correctly.\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43e7850b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original X:\n",
      " tensor([0, 1, 2, 3, 4])\n",
      "\n",
      "\n",
      "After (1, -1):\n",
      " tensor([[0, 1, 2, 3, 4]])\n",
      "\n",
      "\n",
      "After (2, -1):\n",
      " tensor([[0, 1, 2, 3, 4],\n",
      "        [0, 1, 2, 3, 4]])\n",
      "\n",
      "\n",
      "After (3, -1):\n",
      " tensor([[0, 1, 2, 3, 4],\n",
      "        [0, 1, 2, 3, 4],\n",
      "        [0, 1, 2, 3, 4]])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(5)\n",
    "\n",
    "print(f\"Original X:\\n {x}\\n\\n\")\n",
    "print(f\"After (1, -1):\\n {x.expand(1, -1)}\\n\\n\")\n",
    "print(f\"After (2, -1):\\n {x.expand(2, -1)}\\n\\n\")\n",
    "print(f\"After (3, -1):\\n {x.expand(3, -1)}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ca60a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5]), torch.Size([5]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size(), x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0e8d4c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9517, -0.9720,  0.1327]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "36325fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3]), torch.Size([1, 3]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size(), x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab14451e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.2346,  1.5581, -0.1645],\n",
       "         [-2.2346,  1.5581, -0.1645],\n",
       "         [-2.2346,  1.5581, -0.1645],\n",
       "         [-2.2346,  1.5581, -0.1645],\n",
       "         [-2.2346,  1.5581, -0.1645]],\n",
       "\n",
       "        [[-2.2346,  1.5581, -0.1645],\n",
       "         [-2.2346,  1.5581, -0.1645],\n",
       "         [-2.2346,  1.5581, -0.1645],\n",
       "         [-2.2346,  1.5581, -0.1645],\n",
       "         [-2.2346,  1.5581, -0.1645]],\n",
       "\n",
       "        [[-2.2346,  1.5581, -0.1645],\n",
       "         [-2.2346,  1.5581, -0.1645],\n",
       "         [-2.2346,  1.5581, -0.1645],\n",
       "         [-2.2346,  1.5581, -0.1645],\n",
       "         [-2.2346,  1.5581, -0.1645]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.expand(3, 5, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc392695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3]])     # shape: (1, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b5a80db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 3]],\n",
       "\n",
       "        [[1, 2, 3]],\n",
       "\n",
       "        [[1, 2, 3]],\n",
       "\n",
       "        [[1, 2, 3]],\n",
       "\n",
       "        [[1, 2, 3]],\n",
       "\n",
       "        [[1, 2, 3]],\n",
       "\n",
       "        [[1, 2, 3]],\n",
       "\n",
       "        [[1, 2, 3]],\n",
       "\n",
       "        [[1, 2, 3]],\n",
       "\n",
       "        [[1, 2, 3]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.expand(10, -1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0f062c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58618e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9807, -0.8164, -0.3877],\n",
       "        [-1.5797,  1.1740,  0.4903]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c07d3f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9807, -0.8164, -0.3877],\n",
       "         [-1.5797,  1.1740,  0.4903]],\n",
       "\n",
       "        [[ 0.9807, -0.8164, -0.3877],\n",
       "         [-1.5797,  1.1740,  0.4903]],\n",
       "\n",
       "        [[ 0.9807, -0.8164, -0.3877],\n",
       "         [-1.5797,  1.1740,  0.4903]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.expand(3, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c292f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   1,   2,  ..., 509, 510, 511],\n",
       "        [  0,   1,   2,  ..., 509, 510, 511]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(config.max_position_embeddings).expand(2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3166c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
