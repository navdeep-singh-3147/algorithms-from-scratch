{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcc18795",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "===================================================================\n",
    "LoRA (Low-Rank Adaptation) Implementation from Scratch in PyTorch\n",
    "===================================================================\n",
    "\n",
    "Mathematical Foundation:\n",
    "------------------------\n",
    "In standard finetuning: W_updated = W + ΔW\n",
    "In LoRA: W_updated = W + A·B (where ΔW ≈ A·B)\n",
    "\n",
    "- W: Original pretrained weight matrix (d × k)\n",
    "- A: Low-rank matrix (d × r)  \n",
    "- B: Low-rank matrix (r × k)\n",
    "- r: Rank (r << min(d, k))\n",
    "\n",
    "Instead of updating all parameters in W, we only train A and B.\n",
    "If W is 1000×1000, we need 1M parameters. With LoRA (r=16):\n",
    "- A: 1000×16 = 16,000 params\n",
    "- B: 16×1000 = 16,000 params\n",
    "- Total: 32,000 params (31x fewer!)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19081331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# STEP 1: LoRA Layer Implementation\n",
    "# ===================================================================\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Core LoRA layer that implements low-rank adaptation.\n",
    "    This layer computes: output = alpha * (input @ A @ B)\n",
    "    \n",
    "    Parameters:\n",
    "        in_dim (int): Input dimension (number of input features)\n",
    "        out_dim (int): Output dimension (number of output features)\n",
    "        rank (int): Rank for low-rank decomposition (bottleneck dimnesion)\n",
    "        alpha (float): Scaling factor for LoRA output\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Calculate standard devation for initialization\n",
    "        # Using 1/sqrt(rank) helps with gradient flow\n",
    "        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        \n",
    "        # Matrix A: Maps from input dimesnsion to low rank dimension\n",
    "        # Shape: (input_dim, rank)\n",
    "        # Initialized with samll random values from normal distribution\n",
    "        self.A = nn.Parameter(torch.randn(in_dim, rank) * std_dev)\n",
    "        \n",
    "        # Matrix B: Maps from low rank dimension to output dimension\n",
    "        # Shape: (rank, out_dim)\n",
    "        # Initialized with zeros so initial LoRA contribution is zero\n",
    "        # This ensures the model starts with pretrained behaviour\n",
    "        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        \n",
    "        # Alpha: Scaling hyperparameter\n",
    "        # Controls the magnitude of LoRA's contribution\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        print(f\"LoRALayer initialized:\")\n",
    "        print(f\"  Matrix A shape: {self.A.shape} (in_dim={in_dim}, rank={rank})\")\n",
    "        print(f\"  Matrix B shape: {self.B.shape} (rank={rank}, out_dim={out_dim})\")\n",
    "        print(f\"  Total LoRA params: {self.A.numel() + self.B.numel()}\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through LoRA layer.\n",
    "        \n",
    "        Input:\n",
    "            x: tensor of shape (batch_size, seq_len, in_dim) or (batch_size, in_dim)\n",
    "        \n",
    "        Output:\n",
    "            result: tensor of same shape as input but last dimension is out_dim\n",
    "            \n",
    "        Complete Flow:\n",
    "            1. x @ A: (batch_size, ..., in_dim) @ (in_dim, rank) = (batch_size, .., rank)\n",
    "            2. (..) @ B: (batch_size, .., rank) @ (rank, out_dim) = (batch_size, ..., out_dim)\n",
    "            3. alpha * (...): scale the result\n",
    "        \"\"\"\n",
    "        # Store original input shape for documentation\n",
    "        input_shape = x.shape\n",
    "        \n",
    "        # Step 1: Matrix multiply input with A\n",
    "        # Input shape: (batch_size, ..., in_dim)\n",
    "        # A shape: (in_dim, rank)\n",
    "        # Output shape: (batch_size, ..., rank)\n",
    "        x_A = x @ self.A\n",
    "        \n",
    "        # Step 2: Matrix multiply result with B\n",
    "        # Input shape: (batch_size, ..., rank)\n",
    "        # A shape: (rank, out_dim)\n",
    "        # Output shape: (batch_size, ..., out_dim)\n",
    "        x_A_B = x_A @ self.B\n",
    "        \n",
    "        # Step 3: Scale by alpha\n",
    "        # Shape: (batch_size, ..., out_dim)\n",
    "        result = self.alpha * x_A_B\n",
    "        \n",
    "        # Shape documentation\n",
    "        print(f\"\\nLoRALayer forward pass:\")\n",
    "        print(f\"  Input shape: {input_shape}\")\n",
    "        print(f\"  After x @ A: {x_A.shape}\")\n",
    "        print(f\"  After @ B: {x_A_B.shape}\")\n",
    "        print(f\"  Final output shape: {result.shape}\")\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5e359ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# STEP 2: Linear Layer with LoRA (Separate Computation)\n",
    "# ===================================================================\n",
    "\n",
    "class LinearWithLoRA(nn.Module):\n",
    "    \"\"\"\n",
    "    Replaces a standard linear layer with LoRA adaptation.\n",
    "    \n",
    "    Computes: output = Linear(x) + LoRA(x) = x @ W^T + alpha * (x @ A @ B)\n",
    "    \n",
    "    This keeps the original weights and LoRA weights separate.\n",
    "    Using distribution property: x(W + AB) = xW + xAB\n",
    "    \n",
    "    Parameters:\n",
    "        linear (nn.Linear): The original pretrained linear layer (will be frozen)\n",
    "        rank (int): Rank for LoRA decomposition\n",
    "        alpha (float): Scaling factor for LoRA\n",
    "    \"\"\"\n",
    "    def __init__(self, linear: nn.Linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Store the original linear layer\n",
    "        # This contains the pretrained weights W\n",
    "        # Shape of W: (out_features, in_features)\n",
    "        self.linear = linear\n",
    "        \n",
    "        # Create the LoRA layer\n",
    "        # Note: We use linear.in_features and linear.out_features to match original layer dimensions\n",
    "        self.lora = LoRALayer(\n",
    "            in_dim=linear.in_features,\n",
    "            out_dim=linear.out_features,\n",
    "            rank=rank,\n",
    "            alpha=alpha\n",
    "        )\n",
    "        print(f\"\\nLinearWithLoRA created:\")\n",
    "        print(f\"  Original Linear: in={linear.in_features}, out={linear.out_features}\")\n",
    "        print(f\"  Original params: {linear.weight.numel() + linear.bias.numel() if linear.bias is not None else 0}\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass: Combine original linear output with LoRA output.\n",
    "        \n",
    "        Input:\n",
    "            x: tensor of shape (batch_size, ..., in_features)\n",
    "        \n",
    "        Output:\n",
    "            result: tensor of shape (batch_size, ..., out_features)\n",
    "            \n",
    "        Computation:\n",
    "            1. Pass through original linear layer: x @ w^T + b\n",
    "            2. Pass through LoRA layer: alpha * (x @ A @ B)\n",
    "            3. Add both results together\n",
    "        \"\"\"\n",
    "        input_shape = x.shape\n",
    "        \n",
    "        # Step 1: Compute original lnear transformation\n",
    "        # Input: (batch_size, ..., in_features)\n",
    "        # Weight: (out_features, in_features) [transposed internally by Linear]\n",
    "        # Output: (batch_size, ..., out_features)\n",
    "        linear_output = self.linear(x)\n",
    "        \n",
    "        # Step 2: Compute LoRA transformation\n",
    "        # Input: (batch_size, ..., in_features)\n",
    "        # Ouptut: (batch_size, ..., out_features)\n",
    "        lora_output = self.lora(x)\n",
    "        \n",
    "        # Step 3: Combine both outputs\n",
    "        # Both shapes: (batch_size, ..., out_features)\n",
    "        # Result shape: (batch_size, ..., out_features)\n",
    "        result = linear_output + lora_output\n",
    "        \n",
    "        print(f\"\\nLinearWithLoRA forward pass:\")\n",
    "        print(f\"  Input shape: {input_shape}\")\n",
    "        print(f\"  Linear output shape: {linear_output.shape}\")\n",
    "        print(f\"  LoRA output shape: {lora_output.shape}\")\n",
    "        print(f\"  Combined output shape: {result.shape}\")\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c69078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# STEP 3: Linear Layer with LoRA (Merged Weights)\n",
    "# ===================================================================\n",
    "\n",
    "class LinearWithLoRAMerged(nn.Module):\n",
    "    \"\"\"\n",
    "    Alternative implementation that merge weights before computation.\n",
    "    \n",
    "    Computes: output = x @ (W + alpha * AB)^T\n",
    "    \n",
    "    This is mathematically equivalent to LinearWithLoRA but computes the\n",
    "    combined weight matrix first. Useful for inferernce optimization.\n",
    "    \n",
    "    Parameters:\n",
    "        linear (nn.Linear): The original pre-trained linear layer\n",
    "        rank (int): Rank for LoRA decomposition.\n",
    "        alpha (float): Scaling factor for LoRA\n",
    "    \"\"\"\n",
    "    def __init__(self, linear: nn.Linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Store original linear layer\n",
    "        self.linear = linear\n",
    "        \n",
    "        # Create LoRA layer\n",
    "        self.lora = LoRALayer(\n",
    "            in_dim=linear.in_features,\n",
    "            out_dim=linear.out_features,\n",
    "            rank=rank,\n",
    "            alpha=alpha\n",
    "        )\n",
    "        print(f\"\\nLinearWithLoRAMerged created\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass: Merge weights then compute\n",
    "        \n",
    "        Steps:\n",
    "            1. Compute LoRA weight update: ΔW = A @ B\n",
    "            2. Combine with original weights: W_new = W + alpha * ΔW^T\n",
    "            3. Apply combined weights: x @ W_new^T + b\n",
    "        \"\"\"\n",
    "        input_shape = x.shape\n",
    "        \n",
    "        # Step 1: Compute low-rank update matrix\n",
    "        # A shape: (in_fetaures, rank)\n",
    "        # B shape: (rank, out_features)\n",
    "        # lora shape: (in_features, out_features)\n",
    "        lora = self.lora.A @ self.lora.B\n",
    "        \n",
    "        # Step 2: Combine with original weights\n",
    "        # self.linear.weight shape: (out_features, in_features)\n",
    "        # lora.T shape: (out_features, in_features) [transpose to match]\n",
    "        # combined_weight shape: (out_features, in_features)\n",
    "        combined_weight = self.linear.weight + self.lora.alpha * lora.T\n",
    "        \n",
    "        # Step 3: Apply combined weights using F.linear\n",
    "        # x shape: (batch_size, ..., in_features)\n",
    "        # combined_weight shape: (out_features, in_features)\n",
    "        # Output shape: (batch_size, ..., out_features)\n",
    "        result = F.linear(\n",
    "            input=x,\n",
    "            weight=combined_weight,\n",
    "            bias=self.linear.bias\n",
    "        )\n",
    "        print(f\"\\nLinearWithLoRAMerged forward pass:\")\n",
    "        print(f\"  Input shape: {input_shape}\")\n",
    "        print(f\"  LoRA update (A@B) shape: {lora.shape}\")\n",
    "        print(f\"  Combined weight shape: {combined_weight.shape}\")\n",
    "        print(f\"  Output shape: {result.shape}\")\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "531f8aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# STEP 4: Example Neural Network with LoRA\n",
    "# ===================================================================\n",
    "\n",
    "class SimpleNeuralNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple 3-layer feedforward neural network\n",
    "    we'll apply LoRA to this network.\n",
    "    \n",
    "    Architecture:\n",
    "        Input (784) -> Linear (128) -> ReLU -> Linear(64) -> ReLU -> Linear(10)\n",
    "        \n",
    "    This is similar to a network for MNIST digit classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size=784,\n",
    "        hidden_size1=128,\n",
    "        hidden_size2=64,\n",
    "        output_size=10\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Layer 1: Input to first hidden layer\n",
    "        # Weight shape: (hidden_size1, input_size)\n",
    "        self.fc1 = nn.Linear(in_features=input_size, out_features=hidden_size1)\n",
    "        \n",
    "        # Layer 2: First hidden to second hidden layer\n",
    "        # Weight shape: (hidden_size2, hidden_size1)\n",
    "        self.fc2 = nn.Linear(in_features=hidden_size1, out_features=hidden_size2)\n",
    "        \n",
    "        # Layer 3: Second hiddent to output layer\n",
    "        # Weight shape: (output_size, hidden_size2)\n",
    "        self.fc3 = nn.Linear(in_features=hidden_size2, out_features=output_size)\n",
    "        \n",
    "        print(f\"\\nSimpleNeuralNetwork initialized:\")\n",
    "        print(f\"  Layer 1: {input_size} -> {hidden_size1}\")\n",
    "        print(f\"  Layer 2: {hidden_size1} -> {hidden_size2}\")\n",
    "        print(f\"  Layer 3: {hidden_size2} -> {output_size}\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Input:\n",
    "        ------\n",
    "        x : tensor of shape (batch_size, input_size)\n",
    "        \n",
    "        Output:\n",
    "        -------\n",
    "        output : tensor of shape (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        print(f\"\\nSimpleNeuralNetwork forward pass:\")\n",
    "        print(f\"  Input shape: {x.shape}\")  # (batch_size, 784)\n",
    "        \n",
    "        # Layer 1: Linear + ReLU\n",
    "        # Input: (batch_size, input_size)\n",
    "        # Output: (batch_size, hidden_size1)\n",
    "        x = self.fc1(x)\n",
    "        print(f\"  After fc1: {x.shape}\")\n",
    "        x = F.relu(x)\n",
    "        print(f\"  After ReLU: {x.shape}\")\n",
    "        \n",
    "        # Layer 2: Linear + ReLU\n",
    "        # Input: (batch_size, 128)\n",
    "        # Output: (batch_size, 64)\n",
    "        x = self.fc2(x)\n",
    "        print(f\"  After fc2: {x.shape}\")\n",
    "        x = F.relu(x)\n",
    "        print(f\"  After ReLU: {x.shape}\")\n",
    "        \n",
    "        # Layer 3: Linear (no activation)\n",
    "        # Input: (batch_size, 64)\n",
    "        # Output: (batch_size, 10)\n",
    "        x = self.fc3(x)\n",
    "        print(f\"  After fc3 (final): {x.shape}\")\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d30ef5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SimpleNeuralNetwork initialized:\n",
      "  Layer 1: 784 -> 128\n",
      "  Layer 2: 128 -> 64\n",
      "  Layer 3: 64 -> 10\n",
      "Linear(in_features=784, out_features=128, bias=True) ===> Parameter containing:\n",
      "tensor([[ 0.0158, -0.0069, -0.0318,  ..., -0.0109, -0.0332, -0.0254],\n",
      "        [ 0.0013,  0.0260,  0.0219,  ...,  0.0167, -0.0257, -0.0279],\n",
      "        [-0.0096,  0.0282,  0.0223,  ..., -0.0085, -0.0147, -0.0077],\n",
      "        ...,\n",
      "        [ 0.0153,  0.0317,  0.0131,  ...,  0.0356, -0.0032,  0.0280],\n",
      "        [-0.0044, -0.0101, -0.0131,  ..., -0.0279, -0.0301,  0.0119],\n",
      "        [ 0.0264,  0.0254,  0.0004,  ..., -0.0195, -0.0072, -0.0225]],\n",
      "       requires_grad=True)\n",
      "Linear(in_features=784, out_features=128, bias=True) ===> Parameter containing:\n",
      "tensor([ 0.0085, -0.0234, -0.0183,  0.0240, -0.0226,  0.0128, -0.0108,  0.0335,\n",
      "         0.0089,  0.0040,  0.0158, -0.0069,  0.0331,  0.0284, -0.0027,  0.0160,\n",
      "         0.0047,  0.0139, -0.0286, -0.0249,  0.0280,  0.0273,  0.0234,  0.0169,\n",
      "         0.0109, -0.0343,  0.0284, -0.0252,  0.0315, -0.0086, -0.0198, -0.0028,\n",
      "        -0.0085,  0.0034,  0.0096, -0.0201,  0.0191, -0.0090, -0.0107, -0.0151,\n",
      "         0.0072, -0.0039, -0.0236, -0.0351, -0.0102, -0.0324, -0.0298,  0.0173,\n",
      "        -0.0106, -0.0216,  0.0056,  0.0212, -0.0215,  0.0146, -0.0152,  0.0201,\n",
      "         0.0209,  0.0041,  0.0088, -0.0048,  0.0089, -0.0174, -0.0203, -0.0295,\n",
      "        -0.0293,  0.0277, -0.0029, -0.0009,  0.0274, -0.0002,  0.0237, -0.0055,\n",
      "         0.0051,  0.0151,  0.0247, -0.0091,  0.0155, -0.0078, -0.0057, -0.0253,\n",
      "        -0.0080, -0.0253, -0.0280, -0.0339, -0.0161, -0.0036,  0.0035,  0.0011,\n",
      "        -0.0014, -0.0087,  0.0231,  0.0053,  0.0276,  0.0029,  0.0068,  0.0102,\n",
      "        -0.0068,  0.0287, -0.0049,  0.0127, -0.0104,  0.0236, -0.0257,  0.0240,\n",
      "        -0.0262, -0.0196,  0.0273, -0.0237, -0.0148,  0.0241,  0.0107, -0.0230,\n",
      "        -0.0274,  0.0191, -0.0051, -0.0284,  0.0289,  0.0352,  0.0033, -0.0010,\n",
      "        -0.0139, -0.0200, -0.0149, -0.0244,  0.0192,  0.0214,  0.0117, -0.0042],\n",
      "       requires_grad=True)\n",
      "Linear(in_features=128, out_features=64, bias=True) ===> Parameter containing:\n",
      "tensor([[ 0.0086, -0.0308, -0.0255,  ...,  0.0478, -0.0441, -0.0815],\n",
      "        [-0.0322,  0.0650,  0.0361,  ..., -0.0352,  0.0720, -0.0281],\n",
      "        [ 0.0581, -0.0821,  0.0860,  ...,  0.0646, -0.0178,  0.0351],\n",
      "        ...,\n",
      "        [ 0.0480,  0.0175,  0.0293,  ...,  0.0717,  0.0672, -0.0403],\n",
      "        [ 0.0008, -0.0317,  0.0668,  ..., -0.0057, -0.0046,  0.0032],\n",
      "        [ 0.0427,  0.0161,  0.0279,  ..., -0.0625,  0.0580,  0.0681]],\n",
      "       requires_grad=True)\n",
      "Linear(in_features=128, out_features=64, bias=True) ===> Parameter containing:\n",
      "tensor([-0.0838, -0.0104,  0.0639, -0.0169,  0.0239, -0.0469, -0.0576,  0.0570,\n",
      "        -0.0200,  0.0051, -0.0573, -0.0407,  0.0278, -0.0055,  0.0411, -0.0598,\n",
      "         0.0064, -0.0474,  0.0544,  0.0333, -0.0759, -0.0359, -0.0355, -0.0503,\n",
      "         0.0627,  0.0283,  0.0791,  0.0811,  0.0729,  0.0648, -0.0220,  0.0440,\n",
      "        -0.0111, -0.0570,  0.0586,  0.0323,  0.0708, -0.0818, -0.0843,  0.0551,\n",
      "        -0.0063,  0.0145,  0.0616,  0.0733, -0.0044,  0.0474, -0.0370,  0.0826,\n",
      "         0.0305, -0.0490, -0.0512,  0.0588, -0.0142,  0.0594, -0.0046,  0.0833,\n",
      "         0.0048,  0.0279,  0.0415,  0.0356, -0.0729, -0.0577, -0.0018,  0.0592],\n",
      "       requires_grad=True)\n",
      "Linear(in_features=64, out_features=10, bias=True) ===> Parameter containing:\n",
      "tensor([[ 7.8727e-02,  7.6662e-02, -6.5558e-03, -8.7192e-02,  3.7678e-02,\n",
      "         -3.6093e-02, -3.3980e-02, -6.2006e-02, -2.1688e-02, -9.2377e-02,\n",
      "         -1.0340e-01, -7.5022e-02,  9.1159e-02, -8.8737e-02, -4.0795e-02,\n",
      "         -5.3292e-02, -7.5880e-02,  1.1122e-01, -1.6142e-03, -5.0520e-02,\n",
      "          4.5116e-03,  8.7370e-02,  8.9964e-02,  1.2329e-01,  8.5693e-02,\n",
      "         -7.6711e-02, -8.0496e-02, -3.3843e-02, -3.2898e-02, -4.7323e-02,\n",
      "         -1.1073e-01,  1.1969e-01, -3.3840e-02, -8.9492e-02,  1.2191e-01,\n",
      "          1.1356e-01,  1.2455e-01, -4.9100e-02, -1.0078e-01,  1.2011e-01,\n",
      "          5.8914e-02, -2.3557e-02, -4.4135e-02, -6.3333e-04,  1.2452e-01,\n",
      "         -2.0164e-02,  4.9690e-02, -7.5095e-02,  1.1055e-01, -2.4090e-02,\n",
      "          1.6638e-02, -2.2506e-02,  9.6757e-02,  1.1688e-01,  8.2388e-02,\n",
      "         -7.3402e-02,  4.9973e-02,  6.7335e-02, -5.0402e-02,  8.1521e-02,\n",
      "         -6.8478e-02, -1.1819e-01,  3.0186e-02,  1.2279e-01],\n",
      "        [ 1.2408e-02, -9.5002e-02, -1.0252e-01, -4.0781e-02,  4.5636e-02,\n",
      "         -4.5590e-02, -4.3943e-02,  1.0090e-02,  1.2468e-01, -3.7790e-02,\n",
      "          1.0475e-01,  3.4789e-02, -8.3874e-02,  1.0655e-01,  5.9622e-03,\n",
      "         -1.7483e-02,  7.7940e-02,  9.8170e-02,  1.1584e-02,  9.0906e-02,\n",
      "          2.7766e-02, -1.1146e-01,  1.0283e-01,  8.9370e-02, -1.0629e-01,\n",
      "          1.2232e-01, -8.8168e-02, -2.3863e-02, -3.4795e-02, -6.5393e-02,\n",
      "         -8.4242e-02,  9.0066e-02,  1.0324e-01, -1.9147e-03, -3.2619e-02,\n",
      "         -7.6708e-02, -7.7118e-04, -2.8121e-02, -4.4870e-02,  7.8084e-02,\n",
      "         -2.1546e-02,  4.2268e-02,  4.3683e-03, -4.8363e-02, -1.0618e-01,\n",
      "          6.7022e-02, -8.9166e-02, -1.1075e-01,  9.7032e-03, -7.8022e-02,\n",
      "          8.5548e-02, -7.6678e-02, -7.5433e-02, -2.7086e-02,  1.1170e-01,\n",
      "          7.1447e-02,  4.0201e-03, -3.8944e-02, -1.3915e-03, -4.7484e-02,\n",
      "         -2.2566e-02, -8.9173e-02,  7.7699e-03,  1.2038e-01],\n",
      "        [ 6.7781e-02,  8.2822e-02, -1.1777e-01,  3.6001e-02, -5.2333e-02,\n",
      "         -6.2003e-02,  7.2450e-02, -2.2039e-02, -2.4875e-02, -8.5281e-02,\n",
      "         -1.7399e-02,  1.2091e-02, -3.4320e-02, -4.9506e-02,  7.0679e-02,\n",
      "         -1.1147e-01,  1.1699e-01, -3.3432e-02,  3.9650e-02, -9.1485e-02,\n",
      "          4.2521e-02,  6.9422e-02, -1.0251e-01,  1.4692e-03,  7.0428e-02,\n",
      "         -1.9047e-02, -1.0620e-01, -6.8186e-02, -1.2300e-01, -8.9190e-02,\n",
      "          1.6819e-02, -7.7072e-02,  4.3767e-02,  8.7601e-02,  9.2256e-02,\n",
      "          7.7160e-02, -5.0664e-03, -3.6719e-02, -1.1215e-01, -8.8259e-03,\n",
      "         -5.1756e-02,  6.6997e-02, -3.1681e-03,  7.1066e-02, -2.3236e-02,\n",
      "          7.0770e-02, -1.0398e-01, -7.4835e-02,  9.9399e-02,  9.9099e-02,\n",
      "         -1.2301e-01,  6.2453e-02,  1.2187e-02,  9.9716e-02,  7.7703e-02,\n",
      "         -5.9434e-02,  6.6957e-02, -5.7680e-02, -1.1452e-01,  6.1290e-02,\n",
      "         -7.3921e-02,  8.1297e-03, -7.9305e-02,  9.1961e-02],\n",
      "        [-8.1215e-02,  9.7827e-02, -3.6062e-03, -7.9375e-02, -9.9174e-03,\n",
      "          1.8122e-02,  7.5628e-02, -4.9416e-02,  1.0604e-01,  5.6571e-03,\n",
      "         -2.9377e-02, -5.6422e-02,  1.0998e-01,  3.4793e-02,  3.4640e-02,\n",
      "          1.2411e-01,  1.9313e-02, -7.6069e-02, -1.2027e-01,  2.0395e-02,\n",
      "         -1.2382e-01, -6.5773e-02,  9.4176e-02,  6.9970e-02, -1.0299e-01,\n",
      "         -4.0744e-02,  2.7409e-02, -5.8851e-02,  1.1178e-01, -9.3258e-02,\n",
      "         -5.1295e-02, -4.4331e-03, -1.0438e-01,  1.2286e-01,  6.0584e-02,\n",
      "          1.1456e-01,  2.6180e-02, -3.9354e-02,  3.5086e-02, -1.1993e-01,\n",
      "         -5.2007e-02,  8.1390e-02, -4.6080e-02,  9.1767e-02,  8.6538e-02,\n",
      "          1.6749e-02,  7.3425e-02,  1.0980e-01, -2.4885e-02,  7.2768e-02,\n",
      "          1.1210e-01,  1.1271e-01,  3.6319e-02, -1.2153e-01,  8.9800e-02,\n",
      "          2.6865e-02,  5.1525e-02,  3.9121e-02,  6.2199e-02, -2.8508e-02,\n",
      "         -5.9932e-02, -1.3956e-03,  3.3826e-06, -3.2131e-02],\n",
      "        [ 1.1481e-01, -3.3915e-02,  5.1765e-02,  7.8189e-02, -6.8101e-02,\n",
      "         -5.3499e-03,  1.5406e-02, -6.2315e-02,  3.1590e-02,  5.0279e-02,\n",
      "          6.0142e-02,  3.3019e-02, -1.1109e-01, -7.9627e-03,  2.7631e-02,\n",
      "          9.0569e-02,  3.0404e-03,  1.3248e-02, -4.9924e-02,  8.8756e-02,\n",
      "          5.2325e-02, -1.0796e-01, -5.5392e-03, -4.4635e-03,  1.0013e-01,\n",
      "         -5.6024e-02, -1.6877e-02, -7.1961e-02, -1.9040e-02,  3.3094e-02,\n",
      "         -3.4841e-02, -4.1727e-02,  8.9530e-02,  9.2117e-02,  4.5542e-02,\n",
      "         -5.0442e-02,  6.6427e-03, -5.2588e-02, -1.9656e-02,  2.0525e-02,\n",
      "          6.9118e-02, -1.7920e-02, -5.2312e-02,  2.2174e-02, -1.2478e-01,\n",
      "         -7.3853e-02, -9.2846e-02,  4.1766e-03, -2.6775e-02,  5.5057e-02,\n",
      "          6.7292e-02, -1.3397e-02, -6.1375e-02, -1.2342e-01,  1.6964e-02,\n",
      "          6.2233e-02, -7.1881e-02,  1.7385e-02,  1.0438e-01, -1.1356e-02,\n",
      "          4.9610e-02, -2.6184e-02,  8.5158e-02, -1.0420e-01],\n",
      "        [-1.0027e-01, -1.2318e-01,  5.9734e-02, -3.4806e-02, -7.5476e-02,\n",
      "         -8.0104e-02, -9.4820e-02, -1.1553e-01,  1.0704e-01, -3.0268e-02,\n",
      "          2.8218e-02,  9.7864e-02, -1.0679e-01, -6.6946e-02, -1.1677e-01,\n",
      "          1.2484e-01,  7.8537e-02,  1.3509e-02, -1.0340e-01,  1.1177e-01,\n",
      "          8.6899e-02, -8.2395e-02,  1.0662e-01, -7.0387e-02,  2.2122e-02,\n",
      "         -6.1453e-02,  7.6415e-02,  5.1320e-03,  1.1000e-02, -6.5715e-02,\n",
      "          8.0814e-03,  2.9922e-02,  3.1342e-02,  5.0949e-02, -8.0074e-02,\n",
      "         -4.0092e-02, -1.1450e-01,  8.5213e-03,  1.8236e-02,  5.9105e-02,\n",
      "          1.0740e-03,  1.8691e-02,  8.8541e-02,  8.3061e-02,  3.5674e-02,\n",
      "          8.9869e-02,  1.0621e-01, -8.5265e-02, -1.1282e-02,  4.6474e-02,\n",
      "         -2.1058e-02,  3.6263e-02,  9.6223e-02,  4.9516e-02, -4.5608e-02,\n",
      "         -1.0697e-01, -1.0448e-01,  2.1246e-02, -7.9620e-02, -2.3634e-02,\n",
      "          5.3316e-03,  5.8651e-02, -1.0357e-01,  5.8523e-02],\n",
      "        [ 2.5859e-04,  9.9790e-02, -9.9953e-02,  4.7871e-03, -9.8741e-02,\n",
      "         -4.5522e-02, -9.5699e-02, -1.2501e-02,  7.9689e-02, -4.4904e-02,\n",
      "         -2.1252e-02,  7.1955e-03,  3.3594e-02,  1.0711e-01,  1.1379e-02,\n",
      "         -1.0193e-01,  7.2403e-02, -5.6928e-02, -4.8072e-02, -1.7888e-02,\n",
      "          8.7912e-03, -9.4652e-02,  9.0843e-02,  5.6631e-02, -1.2388e-01,\n",
      "         -4.1948e-02,  4.2946e-02, -4.5311e-02,  2.2817e-02,  1.1221e-01,\n",
      "         -2.2516e-03,  1.1988e-01, -6.0199e-02, -1.4063e-02,  1.1759e-01,\n",
      "         -1.1451e-01, -7.3162e-02, -1.2105e-01,  2.9005e-02, -3.4774e-02,\n",
      "         -6.7452e-02,  5.9892e-02,  5.4246e-02, -1.1555e-03, -8.3541e-02,\n",
      "         -9.0429e-02,  1.0660e-01,  3.7814e-02, -1.0430e-01, -9.5061e-02,\n",
      "          5.3309e-02, -2.0158e-02,  1.0169e-01,  7.8493e-02,  4.1550e-02,\n",
      "          6.2320e-02, -6.0564e-02,  4.5766e-02, -5.4984e-02, -1.0110e-01,\n",
      "         -9.7118e-02,  1.0812e-01, -1.2114e-01, -1.0485e-01],\n",
      "        [ 2.6406e-02, -4.6891e-02,  1.1400e-02,  1.8192e-02, -6.0296e-02,\n",
      "          5.5061e-02,  1.0166e-01, -1.2466e-01, -1.1650e-02, -1.7139e-02,\n",
      "         -7.5674e-02,  1.2240e-01, -1.0681e-01, -8.4397e-03,  6.3309e-02,\n",
      "          3.7031e-02, -9.1842e-02,  7.7027e-02,  1.7702e-02,  5.0741e-02,\n",
      "          1.0816e-01,  3.1409e-03, -8.4445e-02,  1.4310e-02, -8.4352e-02,\n",
      "          1.9581e-02, -1.8673e-02,  6.6238e-02,  8.6620e-02, -1.1328e-01,\n",
      "         -1.0188e-01,  5.5736e-02,  1.0747e-01, -1.1653e-01,  8.4172e-02,\n",
      "         -6.9658e-02,  1.1026e-01, -8.7303e-02,  2.4174e-02, -1.0122e-01,\n",
      "          9.8247e-02, -1.1044e-01,  4.8618e-02,  9.0583e-02,  2.3359e-02,\n",
      "          8.2971e-02,  4.5514e-02, -1.6129e-02,  3.0540e-02, -1.1718e-01,\n",
      "         -1.1183e-01, -2.3103e-03,  2.6916e-02, -5.9935e-02, -5.7495e-02,\n",
      "          3.6501e-02,  9.2983e-02,  9.8603e-02, -1.1868e-01, -1.0700e-01,\n",
      "         -1.0092e-01,  4.6934e-02, -8.6455e-02, -1.1830e-01],\n",
      "        [ 7.8404e-02, -1.0076e-01,  4.4137e-02,  9.7399e-02, -1.0556e-01,\n",
      "          1.5244e-02, -3.3546e-03, -1.0345e-01,  9.4711e-02,  3.0979e-02,\n",
      "         -2.0536e-02, -1.1223e-01,  8.6242e-03, -7.7027e-02, -1.6173e-02,\n",
      "          9.6982e-02,  1.0455e-01, -3.1166e-02, -6.9962e-02,  6.8523e-02,\n",
      "         -4.9997e-03, -1.1496e-01,  3.3127e-02, -1.2328e-01,  8.7216e-02,\n",
      "          8.0568e-02, -1.0433e-01, -7.4530e-02,  3.9415e-02, -4.9524e-02,\n",
      "         -6.7904e-02, -7.4051e-03,  6.0785e-02, -9.9799e-02, -1.5888e-02,\n",
      "          1.4810e-02,  7.6816e-02, -2.9597e-02,  1.0939e-01,  7.7281e-02,\n",
      "          8.1544e-02, -1.0529e-02, -3.6222e-03, -1.5567e-02,  7.7414e-02,\n",
      "         -7.1086e-03, -3.7130e-02,  5.9187e-02, -7.6348e-03,  2.8858e-02,\n",
      "          6.0452e-02,  1.1884e-01,  1.1611e-01, -3.5782e-02,  4.5151e-02,\n",
      "         -6.6702e-02, -5.0066e-02, -7.1330e-02, -5.8503e-02, -8.1959e-02,\n",
      "         -1.3391e-02,  2.4355e-02, -5.5239e-02,  1.1877e-01],\n",
      "        [-1.4500e-02, -9.0318e-02,  6.0728e-03,  6.7426e-02,  9.4545e-02,\n",
      "          2.2613e-03, -9.6986e-02, -1.0964e-01, -1.7204e-02,  7.8383e-02,\n",
      "          5.0628e-02,  9.2759e-02,  8.3819e-02,  9.6072e-02, -8.2919e-02,\n",
      "          5.1511e-02,  1.4675e-02, -1.1176e-02, -1.2257e-01,  5.7524e-02,\n",
      "         -5.0114e-02, -8.6026e-03, -6.3406e-02,  5.1646e-02,  3.7154e-02,\n",
      "         -1.1125e-01,  1.0358e-02, -8.6144e-02,  3.8004e-03,  8.6149e-02,\n",
      "          1.2330e-01, -4.3757e-02, -4.9173e-02,  1.0899e-01, -2.0653e-02,\n",
      "         -1.2132e-01, -3.8671e-02, -2.2413e-02,  5.6758e-03,  1.3587e-02,\n",
      "          4.7338e-03,  2.2530e-02,  2.1427e-02, -1.1593e-01,  3.6646e-02,\n",
      "          1.8676e-02,  5.6698e-02,  5.7793e-02, -1.1887e-01, -4.0565e-02,\n",
      "         -1.0993e-01,  9.3663e-02, -7.4181e-02,  1.6963e-02, -2.7130e-02,\n",
      "          1.0675e-01,  1.0219e-01, -1.2069e-01, -7.7618e-02, -9.4260e-02,\n",
      "         -1.2454e-01, -4.6079e-02, -3.0470e-02,  6.1772e-02]],\n",
      "       requires_grad=True)\n",
      "Linear(in_features=64, out_features=10, bias=True) ===> Parameter containing:\n",
      "tensor([ 0.0961, -0.1132,  0.0702, -0.0275,  0.1209, -0.1154, -0.0519,  0.0071,\n",
      "        -0.0564, -0.0353], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = SimpleNeuralNetwork()\n",
    "\n",
    "for child in model.children():\n",
    "    for param in child.parameters():\n",
    "        print(f\"{child} ===> {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3eb7e026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# STEP 5: Helper Functions for LoRA Integration\n",
    "# ===================================================================\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"\n",
    "    Count total and trainable parameters in a model.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    total_params : int\n",
    "        Total number of parameters\n",
    "    trainable_params : int\n",
    "        Number of trainable parameters\n",
    "    \"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params\n",
    "\n",
    "def freeze_linear_layers(model):\n",
    "    \"\"\"\n",
    "    Freeze all parameters in standard Linear layers.\n",
    "    This keeps pretrained weights fixed while allowing LoRA weights to train.\n",
    "    \n",
    "    How it works:\n",
    "    -------------\n",
    "    1. Iterate through all modules in the model\n",
    "    2. If module is nn.Linear, set requires_grad=False for all params\n",
    "    3. LoRA parameters (A and B matrices) remain trainable\n",
    "    \"\"\"\n",
    "    for child in model.children():\n",
    "        if isinstance(child, nn.Linear):\n",
    "            # Freeze this linear layer\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(f\"  Froze linear layers: {child}\")\n",
    "        else:\n",
    "            # Recursively process child modules\n",
    "            freeze_linear_layers(child)\n",
    "            \n",
    "            \n",
    "def apply_lora_to_model(model, rank=8, alpha=16, layer_indices=None):\n",
    "    \"\"\"\n",
    "    Apply LoRA to specified layers in a neural network.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The neural network to modify\n",
    "    rank : int\n",
    "        Rank for LoRA (lower = fewer parameters)\n",
    "    alpha : float\n",
    "        Scaling factor (typically 2 * rank)\n",
    "    layer_indices : list or None\n",
    "        Which fc layers to apply LoRA to. If None, apply to all.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    model : nn.Module\n",
    "        Modified model with LoRA layers\n",
    "    \"\"\"\n",
    "    print(f\"\\nApplying LoRA (rank={rank}, alpha={alpha}):\")\n",
    "    \n",
    "    # Get all linear layers from the model\n",
    "    layers_to_modify = []\n",
    "    \n",
    "    if hasattr(model, \"fc1\"):\n",
    "        layers_to_modify.append((\"fc1\", model.fc1))\n",
    "    if hasattr(model, \"fc2\"):\n",
    "        layers_to_modify.append((\"fc2\", model.fc2))\n",
    "    if hasattr(model, \"fc1\"):\n",
    "        layers_to_modify.append((\"fc3\", model.fc3))\n",
    "\n",
    "    # Apply LoRA to each layer\n",
    "    for name, layer in layers_to_modify:\n",
    "        if layer_indices is None or name in layer_indices:\n",
    "            # Replace with LoRA version\n",
    "            lora_layer = LinearWithLoRA(layer, rank=rank, alpha=alpha)\n",
    "            setattr(model, name, lora_layer)\n",
    "            print(f\"  Applied LoRA to {name}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "297c3ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# STEP 6: Complete Example with Training\n",
    "# ===================================================================\n",
    "\n",
    "def demonstrate_lora():\n",
    "    \"\"\"\n",
    "    Complete demonstration of LoRA from scratch.\n",
    "    Shows initialization, forward pass, and parameter comparison.\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"LoRA (Low-Rank Adaptation) - Complete Demonstration\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # -------------------------------------------------------------------\n",
    "    # Part A: Create Original Model\n",
    "    # -------------------------------------------------------------------\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PART A: Creating Original Model\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    model = SimpleNeuralNetwork(\n",
    "        input_size=784,                 # 28x28 MNIST images flattened\n",
    "        hidden_size1=128,\n",
    "        hidden_size2=64,\n",
    "        output_size=10                  # 10 digit classes\n",
    "    )\n",
    "    # Count original parameters\n",
    "    total_orig, trainable_orig = count_parameters(model)\n",
    "    print(f\"\\nOriginal model parameters:\")\n",
    "    print(f\"  Total: {total_orig:,}\")\n",
    "    print(f\"  Trainable: {trainable_orig:,}\")\n",
    "    \n",
    "    # -------------------------------------------------------------------\n",
    "    # Part B: Apply LoRA to Model\n",
    "    # -------------------------------------------------------------------\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PART B: Applying LoRA to Model\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Apply LoRA with rank=8\n",
    "    # This adds 8 * (in_dim + out_dim) parameters per layer\n",
    "    model = apply_lora_to_model(model, rank=8, alpha=16)\n",
    "    \n",
    "    # -------------------------------------------------------------------\n",
    "    # Part C: Freeze Original Weights\n",
    "    # -------------------------------------------------------------------\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PART C: Freezing Original Weights\")\n",
    "    print(\"=\"*70)\n",
    "    freeze_linear_layers(model=model)\n",
    "    \n",
    "    # Count parameters after LoRA\n",
    "    total_lora, trainable_lora = count_parameters(model=model)\n",
    "    print(f\"\\nAfter applying LoRA:\")\n",
    "    print(f\"  Total parameters: {total_lora:,}\")\n",
    "    print(f\"  Trainable parameters: {trainable_lora:,}\")\n",
    "    print(f\"  Reduction: {(1 - trainable_lora/trainable_orig)*100:.1f}%\")\n",
    "    print(f\"  Compression ratio: {trainable_orig/trainable_lora:.1f}x\")\n",
    "    \n",
    "    # -------------------------------------------------------------------\n",
    "    # Part D: Test Forward Pass\n",
    "    # -------------------------------------------------------------------\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PART D: Testing Forward Pass with Sample Data\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Create dummy batch of data\n",
    "    # Shape: (batch_size=4, input_size=784)\n",
    "    batch_size = 4\n",
    "    input_size = 784\n",
    "    dummy_input = torch.randn(batch_size, input_size)\n",
    "    print(f\"\\nInput batch shape: {dummy_input.shape}\")\n",
    "    \n",
    "    # Disable gradient computation for testing\n",
    "    with torch.no_grad():\n",
    "        # Forward pass (this will print shapes at each step)\n",
    "        output = model(dummy_input)\n",
    "        \n",
    "    print(f\"\\nFinal output shape: {output.shape}\")\n",
    "    print(f\"Expected shape: (batch_size={batch_size}, num_classes=10)\")\n",
    "    \n",
    "    # -------------------------------------------------------------------\n",
    "    # Part E: Show Trainable Parameters\n",
    "    # -------------------------------------------------------------------\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PART E: Trainable Parameters\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nTrainable parameters (LoRA only):\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"  {name:30s}: shape {str(list(param.shape)):20s} ({param.numel():6,} params)\")\n",
    "    \n",
    "    print(\"\\nFrozen parameters (original weights):\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            print(f\"  {name:30s}: shape {str(list(param.shape)):20s} ({param.numel():6,} params)\")\n",
    "            \n",
    "    # -------------------------------------------------------------------\n",
    "    # Part F: Compare Computation Approaches\n",
    "    # -------------------------------------------------------------------\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PART F: Comparing LinearWithLoRA vs LinearWithLoRAMerged\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Create a simple test case\n",
    "    torch.manual_seed(42)\n",
    "    test_linear = nn.Linear(10, 5)\n",
    "    test_input = torch.randn(3, 10) # (batch_size, in_features)\n",
    "    \n",
    "    print(\"\\nTest Linear layer:\")\n",
    "    print(f\"  Input shape: {test_input.shape}\")\n",
    "    print(f\"  Weight shape: {test_linear.weight.shape}\")\n",
    "    print(f\"  Output shape: (3, 5)\")\n",
    "    \n",
    "    # Create both versions\n",
    "    lora_separate = LinearWithLoRA(test_linear, rank=2, alpha=4)\n",
    "    lora_merged = LinearWithLoRAMerged(test_linear, rank=2, alpha=4)\n",
    "    \n",
    "    # Copy weights to make them identical\n",
    "    lora_merged.lora.A.data = lora_separate.lora.A.data.clone()\n",
    "    lora_merged.lora.B.data = lora_separate.lora.B.data.clone()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_separate = lora_separate(test_input)\n",
    "        output_merged = lora_merged(test_input)\n",
    "        \n",
    "    # Check if outputs are identical\n",
    "    difference = torch.max(torch.abs(output_separate - output_merged)).item()\n",
    "    print(f\"\\nMax difference between methods: {difference:.10f}\")\n",
    "    print(f\"Are outputs identical? {difference < 1e-6}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Demonstration Complete!\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72aa047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# STEP 7: Training Example (Optional - Shows How to Actually Train)\n",
    "# ===================================================================\n",
    "def train_lora_model(num_epochs=2, batch_size=64):\n",
    "    \"\"\"\n",
    "    Complete training example using LoRA on MNIST dataset.\n",
    "    \n",
    "    This shows how to:\n",
    "    1. Load data\n",
    "    2. Setup model with LoRA\n",
    "    3. Train only LoRA parameters\n",
    "    4. Evaluate performance\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Training Example with LoRA on MNIST\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Setup device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\nUsing device: {device}\")\n",
    "    \n",
    "    # -------------------------------------------------------------------\n",
    "    # Step 1: Prepare Data\n",
    "    # -------------------------------------------------------------------\n",
    "    print(\"\\nStep 1: Loading MNIST dataset...\")\n",
    "    \n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Load training data\n",
    "    train_dataset = datasets.MNIST(\n",
    "        root=\"./data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    print(f\"  Training samples: {len(train_dataset)}\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    print(f\"  Number of batches: {len(train_loader)}\")\n",
    "    \n",
    "    # -------------------------------------------------------------------\n",
    "    # Step 2: Create Model with LoRA\n",
    "    # -------------------------------------------------------------------\n",
    "    print(\"\\nStep 2: Creating model with LoRA...\")\n",
    "    \n",
    "    model = SimpleNeuralNetwork().to(device=device)\n",
    "    model = apply_lora_to_model(model, rank=8, alpha=16)\n",
    "    freeze_linear_layers(model=model)\n",
    "    \n",
    "    total_params, trainable_params = count_parameters(model)\n",
    "    print(f\"  Total parameters: {total_params:,}\")\n",
    "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # -------------------------------------------------------------------\n",
    "    # Step 3: Setup Training\n",
    "    # -------------------------------------------------------------------\n",
    "    print(\"\\nStep 3: Setting up training...\")\n",
    "    \n",
    "    # Only optimizer LoRA parameters\n",
    "    optimizer = optim.Adam(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=0.001\n",
    "    )\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # -------------------------------------------------------------------\n",
    "    # Step 4: Training Loop\n",
    "    # -------------------------------------------------------------------\n",
    "    print(f\"\\nStep 4: Training for {num_epochs} epoch(s)...\\n\")\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # Move data to device\n",
    "            # data shape: (batch_size, 1, 28, 28)\n",
    "            # target shape: (batch_size, )\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Flatten images: (batch_size, 1, 28, 28) -> (batch_size, 784)\n",
    "            data = data.view(data.size(0), -1)\n",
    "            \n",
    "            # zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pas\n",
    "            # Input shape: (batch_size, 784)\n",
    "            # Output_shape: (batch_size, 10)\n",
    "            output = model(data)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update only LoRA parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "            \n",
    "            # Print progress\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "                    f\"Batch {batch_idx}/{len(train_loader)} | \"\n",
    "                    f\"Loss: {loss.item():.4f} | \"\n",
    "                    f\"Acc: {100.*correct/total:.2f}%\")\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        accuracy = 100.0 * correct / total\n",
    "        print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "        print(f\"  Average Loss: {avg_loss:.4f}\")\n",
    "        print(f\"  Accuracy: {accuracy:.2f}%\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55500c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "               LoRA Implementation from Scratch\n",
      "======================================================================\n",
      "======================================================================\n",
      "LoRA (Low-Rank Adaptation) - Complete Demonstration\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "PART A: Creating Original Model\n",
      "======================================================================\n",
      "\n",
      "SimpleNeuralNetwork initialized:\n",
      "  Layer 1: 784 -> 128\n",
      "  Layer 2: 128 -> 64\n",
      "  Layer 3: 64 -> 10\n",
      "\n",
      "Original model parameters:\n",
      "  Total: 109,386\n",
      "  Trainable: 109,386\n",
      "\n",
      "======================================================================\n",
      "PART B: Applying LoRA to Model\n",
      "======================================================================\n",
      "\n",
      "Applying LoRA (rank=8, alpha=16):\n",
      "LoRALayer initialized:\n",
      "  Matrix A shape: torch.Size([784, 8]) (in_dim=784, rank=8)\n",
      "  Matrix B shape: torch.Size([8, 128]) (rank=8, out_dim=128)\n",
      "  Total LoRA params: 7296\n",
      "\n",
      "LinearWithLoRA created:\n",
      "  Original Linear: in=784, out=128\n",
      "  Original params: 100480\n",
      "  Applied LoRA to fc1\n",
      "LoRALayer initialized:\n",
      "  Matrix A shape: torch.Size([128, 8]) (in_dim=128, rank=8)\n",
      "  Matrix B shape: torch.Size([8, 64]) (rank=8, out_dim=64)\n",
      "  Total LoRA params: 1536\n",
      "\n",
      "LinearWithLoRA created:\n",
      "  Original Linear: in=128, out=64\n",
      "  Original params: 8256\n",
      "  Applied LoRA to fc2\n",
      "LoRALayer initialized:\n",
      "  Matrix A shape: torch.Size([64, 8]) (in_dim=64, rank=8)\n",
      "  Matrix B shape: torch.Size([8, 10]) (rank=8, out_dim=10)\n",
      "  Total LoRA params: 592\n",
      "\n",
      "LinearWithLoRA created:\n",
      "  Original Linear: in=64, out=10\n",
      "  Original params: 650\n",
      "  Applied LoRA to fc3\n",
      "\n",
      "======================================================================\n",
      "PART C: Freezing Original Weights\n",
      "======================================================================\n",
      "  Froze linear layers: Linear(in_features=784, out_features=128, bias=True)\n",
      "  Froze linear layers: Linear(in_features=128, out_features=64, bias=True)\n",
      "  Froze linear layers: Linear(in_features=64, out_features=10, bias=True)\n",
      "\n",
      "After applying LoRA:\n",
      "  Total parameters: 118,810\n",
      "  Trainable parameters: 9,424\n",
      "  Reduction: 91.4%\n",
      "  Compression ratio: 11.6x\n",
      "\n",
      "======================================================================\n",
      "PART D: Testing Forward Pass with Sample Data\n",
      "======================================================================\n",
      "\n",
      "Input batch shape: torch.Size([4, 784])\n",
      "\n",
      "SimpleNeuralNetwork forward pass:\n",
      "  Input shape: torch.Size([4, 784])\n",
      "\n",
      "LoRALayer forward pass:\n",
      "  Input shape: torch.Size([4, 784])\n",
      "  After x @ A: torch.Size([4, 8])\n",
      "  After @ B: torch.Size([4, 128])\n",
      "  Final output shape: torch.Size([4, 128])\n",
      "\n",
      "LinearWithLoRA forward pass:\n",
      "  Input shape: torch.Size([4, 784])\n",
      "  Linear output shape: torch.Size([4, 128])\n",
      "  LoRA output shape: torch.Size([4, 128])\n",
      "  Combined output shape: torch.Size([4, 128])\n",
      "  After fc1: torch.Size([4, 128])\n",
      "  After ReLU: torch.Size([4, 128])\n",
      "\n",
      "LoRALayer forward pass:\n",
      "  Input shape: torch.Size([4, 128])\n",
      "  After x @ A: torch.Size([4, 8])\n",
      "  After @ B: torch.Size([4, 64])\n",
      "  Final output shape: torch.Size([4, 64])\n",
      "\n",
      "LinearWithLoRA forward pass:\n",
      "  Input shape: torch.Size([4, 128])\n",
      "  Linear output shape: torch.Size([4, 64])\n",
      "  LoRA output shape: torch.Size([4, 64])\n",
      "  Combined output shape: torch.Size([4, 64])\n",
      "  After fc2: torch.Size([4, 64])\n",
      "  After ReLU: torch.Size([4, 64])\n",
      "\n",
      "LoRALayer forward pass:\n",
      "  Input shape: torch.Size([4, 64])\n",
      "  After x @ A: torch.Size([4, 8])\n",
      "  After @ B: torch.Size([4, 10])\n",
      "  Final output shape: torch.Size([4, 10])\n",
      "\n",
      "LinearWithLoRA forward pass:\n",
      "  Input shape: torch.Size([4, 64])\n",
      "  Linear output shape: torch.Size([4, 10])\n",
      "  LoRA output shape: torch.Size([4, 10])\n",
      "  Combined output shape: torch.Size([4, 10])\n",
      "  After fc3 (final): torch.Size([4, 10])\n",
      "\n",
      "Final output shape: torch.Size([4, 10])\n",
      "Expected shape: (batch_size=4, num_classes=10)\n",
      "\n",
      "======================================================================\n",
      "PART E: Trainable Parameters\n",
      "======================================================================\n",
      "\n",
      "Trainable parameters (LoRA only):\n",
      "  fc1.lora.A                    : shape [784, 8]             ( 6,272 params)\n",
      "  fc1.lora.B                    : shape [8, 128]             ( 1,024 params)\n",
      "  fc2.lora.A                    : shape [128, 8]             ( 1,024 params)\n",
      "  fc2.lora.B                    : shape [8, 64]              (   512 params)\n",
      "  fc3.lora.A                    : shape [64, 8]              (   512 params)\n",
      "  fc3.lora.B                    : shape [8, 10]              (    80 params)\n",
      "\n",
      "Frozen parameters (original weights):\n",
      "  fc1.linear.weight             : shape [128, 784]           (100,352 params)\n",
      "  fc1.linear.bias               : shape [128]                (   128 params)\n",
      "  fc2.linear.weight             : shape [64, 128]            ( 8,192 params)\n",
      "  fc2.linear.bias               : shape [64]                 (    64 params)\n",
      "  fc3.linear.weight             : shape [10, 64]             (   640 params)\n",
      "  fc3.linear.bias               : shape [10]                 (    10 params)\n",
      "\n",
      "======================================================================\n",
      "PART F: Comparing LinearWithLoRA vs LinearWithLoRAMerged\n",
      "======================================================================\n",
      "\n",
      "Test Linear layer:\n",
      "  Input shape: torch.Size([3, 10])\n",
      "  Weight shape: torch.Size([5, 10])\n",
      "  Output shape: (3, 5)\n",
      "LoRALayer initialized:\n",
      "  Matrix A shape: torch.Size([10, 2]) (in_dim=10, rank=2)\n",
      "  Matrix B shape: torch.Size([2, 5]) (rank=2, out_dim=5)\n",
      "  Total LoRA params: 30\n",
      "\n",
      "LinearWithLoRA created:\n",
      "  Original Linear: in=10, out=5\n",
      "  Original params: 55\n",
      "LoRALayer initialized:\n",
      "  Matrix A shape: torch.Size([10, 2]) (in_dim=10, rank=2)\n",
      "  Matrix B shape: torch.Size([2, 5]) (rank=2, out_dim=5)\n",
      "  Total LoRA params: 30\n",
      "\n",
      "LinearWithLoRAMerged created\n",
      "\n",
      "LoRALayer forward pass:\n",
      "  Input shape: torch.Size([3, 10])\n",
      "  After x @ A: torch.Size([3, 2])\n",
      "  After @ B: torch.Size([3, 5])\n",
      "  Final output shape: torch.Size([3, 5])\n",
      "\n",
      "LinearWithLoRA forward pass:\n",
      "  Input shape: torch.Size([3, 10])\n",
      "  Linear output shape: torch.Size([3, 5])\n",
      "  LoRA output shape: torch.Size([3, 5])\n",
      "  Combined output shape: torch.Size([3, 5])\n",
      "\n",
      "LinearWithLoRAMerged forward pass:\n",
      "  Input shape: torch.Size([3, 10])\n",
      "  LoRA update (A@B) shape: torch.Size([10, 5])\n",
      "  Combined weight shape: torch.Size([5, 10])\n",
      "  Output shape: torch.Size([3, 5])\n",
      "\n",
      "Max difference between methods: 0.0000000000\n",
      "Are outputs identical? True\n",
      "\n",
      "======================================================================\n",
      "Demonstration Complete!\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "All demonstrations complete!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# STEP 8: Main Execution\n",
    "# ===================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" \"*15 + \"LoRA Implementation from Scratch\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Run the demonstration\n",
    "    demonstrate_lora()\n",
    "    \n",
    "    # Uncomment the following to run actual training (takes time)\n",
    "    # print(\"\\n\\nWould you like to train the model? (requires downloading MNIST)\")\n",
    "    # train_lora_model(num_epochs=1, batch_size=64)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"All demonstrations complete!\")\n",
    "    print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
